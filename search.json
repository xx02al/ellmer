[{"path":"https://ellmer.tidyverse.org/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ellmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"vocabulary","dir":"Articles","previous_headings":"","what":"Vocabulary","title":"Getting started with ellmer","text":"’ll start laying key vocab ’ll need understand LLMs. Unfortunately vocab little entangled: understand one term ’ll often know little others. ’ll start simple definitions important terms iteratively go little deeper. starts prompt, text (typically question request) send LLM. starts conversation, sequence turns alternate user prompts model responses. Inside model, prompt response represented sequence tokens, represent either individual words subcomponents word. tokens used compute cost using model measure size context, combination current prompt previous prompts responses used generate next response. ’s useful make distinction providers models. provider web API gives access one models. distinction bit subtle providers often synonymous model, like OpenAI GPT, Anthropic Claude, Google Gemini. providers, like Ollama, can host many different models, typically open source models like LLaMa Mistral. Still providers support open closed models, typically partnering company provides popular closed model. example, Azure OpenAI offers open source models OpenAI’s GPT, AWS Bedrock offers open source models Anthropic’s Claude.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-token","dir":"Articles","previous_headings":"Vocabulary","what":"What is a token?","title":"Getting started with ellmer","text":"LLM model, like models needs way represent inputs numerically. LLMs, means need way convert words numbers. goal tokenizer. example, using GPT 4o tokenizer, string “R created?” converted 5 tokens: 5958 (“”), 673 (” ”), 460 (” R”), 5371 (” created”), 30 (“?”). can see, many simple strings can represented single token. complex strings require multiple tokens. example, string “counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”), 9477 (“volution”), 815 (“ary”). (can see various strings tokenized http://tiktokenizer.vercel.app/). ’s important rough sense text converted tokens tokens used determine cost model much context can used predict next response. average English word needs ~1.5 tokens page might require 375-400 tokens complete book might require 75,000 150,000 tokens. languages typically require tokens, (brief) LLMs trained data internet, primarily English. LLMs priced per million tokens. State art models (like GPT-4o Claude 3.5 sonnet) cost $2-3 per million input tokens, $10-15 per million output tokens. Cheaper models can cost much less, e.g. GPT-4o mini costs $0.15 per million input tokens $0.60 per million output tokens. Even $10 API credit give lot room experimentation, particularly cheaper models, prices likely decline model performance improves. Tokens also used measure context window, much text LLM can use generate next response. ’ll discuss shortly, context length includes full state conversation far (prompts model’s responses), means cost grow rapidly number conversational turns. ellmer, can see many tokens conversations used printing , can see total usage session token_usage(). want learn tokens tokenizers, ’d recommend watching first 20-30 minutes Let’s build GPT Tokenizer Andrej Karpathy. certainly don’t need learn build tokenizer, intro give bunch useful background knowledge help improve undersstanding LLM’s work.","code":"chat <- chat_openai(model = \"gpt-4o\") . <- chat$chat(\"Who created R?\", echo = FALSE) chat #> <Chat OpenAI/gpt-4o turns=2 tokens=11/54 $0.00> #> ── user [11] ────────────────────────────────────────────────────────── #> Who created R? #> ── assistant [54] ───────────────────────────────────────────────────── #> R was created by Ross Ihaka and Robert Gentleman in the mid-1990s.  #> They were statisticians at the University of Auckland, New Zealand. R  #> is an open-source implementation of the S programming language and has #> become widely used for statistical computing and graphics.  token_usage() #>   provider  model input output price #> 1   OpenAI gpt-4o    11     54 $0.00"},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-conversation","dir":"Articles","previous_headings":"Vocabulary","what":"What is a conversation?","title":"Getting started with ellmer","text":"conversation LLM takes place series HTTP requests responses: send question LLM HTTP request, sends back reply HTTP response. words, conversation consists sequence paired turns: sent prompt returned response. ’s important note request includes current user prompt, every previous user prompt model response. means : cost conversation grows quadratically number turns: want save money, keep conversations short. response affected previous prompts responses. can make converstion get stuck local optimum, ’s generally better iterate starting new conversation better prompt rather long back--forth. ellmer full control conversational history. ’s ellmer’s responsibility send previous turns conversation, ’s possible start conversation one model finish another.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"what-is-a-prompt","dir":"Articles","previous_headings":"Vocabulary","what":"What is a prompt?","title":"Getting started with ellmer","text":"user prompt question send model. two important prompts underlie user prompt: platform prompt, unchangeable, set model provider, affects every conversation. can see look like Anthropic, publishes core system prompts. system prompt (aka developer prompt), set create new conversation, affects every response. ’s used provide additional instructions model, shaping responses needs. example, might use system prompt ask model always respond Spanish write dependency-free base R code. can also use system prompt provide model information wouldn’t otherwise know, like details database schema, preferred ggplot2 theme color palette. OpenAI calls chain command: conflicts inconsistencies prompts, platform prompt overrides system prompt, turn overrides user prompt. use chat app like ChatGPT claude.ai can iterate user prompt. ’re programming LLMs, ’ll primarily iterate system prompt. example, ’re developing app helps user write tidyverse code, ’d work system prompt ensure user gets style code want. Writing good prompt, called prompt design, key effective use LLMs. discussed detail vignette(\"prompt-design\").","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with ellmer","text":"Now ’ve got basic vocab belt, ’m going fire bunch interesting potential use cases . special purpose tools might solve cases faster /cheaper, LLM allows rapidly prototype solution. can extremely valuable even end using specialised tools final product. general, recommend avoiding LLMs accuracy critical. said, still many cases use. example, even though always require manual fiddling, might save bunch time ever 80% correct solution. fact, even --good solution can still useful makes easier get started: ’s easier react something rather start scratch blank page.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"chatbots","dir":"Articles","previous_headings":"Example uses","what":"Chatbots","title":"Getting started with ellmer","text":"great place start ellmer LLMs build chatbot custom prompt. Chatbots familiar interface LLMs easy create R shinychat. ’s surprising amount value creating custom chatbot prompt stuffed useful knowledge. example: Help people use new package. , need custom prompt LLMs trained data prior package’s existence. can create surprisingly useful tool just preloading prompt README vignettes. ellmer assistant works. Build language specific prompts R /python. Shiny assistant helps build shiny apps (either R python) combining prompt gives general advice building apps prompt R python. python prompt detailed ’s much less information Shiny Python existing LLM knowledgebases. Help people find answers questions. Even ’ve written bunch documentation something, might find still get questions folks can’t easily find exactly ’re looking . can reduce need answer questions creating chatbot prompt contains documentation. example, ’re teacher, create chatbot includes syllabus prompt. eliminates common class question data necessary answer question available, hard find. Another direction give chatbot additional context current environment. example, aidea allows user interactively explore dataset help LLM. adds summary statistics dataset prompt LLM knows something data. Along lines, imagine writing chatbot help data import prompt include files current directory along first lines.","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"structured-data-extraction","dir":"Articles","previous_headings":"Example uses","what":"Structured data extraction","title":"Getting started with ellmer","text":"LLMs often good extracting structured data unstructured text. can give traction analyse data previously unaccessible. example: Customer tickets GitHub issues: can use LLMs quick dirty sentiment analysis extracting specifically mentioned products summarising discussion bullet points. Geocoding: LLMs surprisingly good job geocoding, especially extracting addresses finding latitute/longitude cities. specialised tools better, using LLM makes easy get started. Recipes: ’ve extracted structured data baking cocktail recipes. data structured form can use R skills better understand recipes vary within cookbook look recipes use ingredients currently kitchen. even use shiny assistant help make techniques available anyone, just R users. Structured data extraction also works well images. ’s fastest cheapest way extract data makes really easy prototype ideas. example, maybe bunch scanned documents want index. can convert PDFs images (e.g. using {imagemagick}) use structured data extraction pull key details. Learn structured data extraction vignette(\"structure-data\").","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"programming","dir":"Articles","previous_headings":"Example uses","what":"Programming","title":"Getting started with ellmer","text":"LLMs can also useful solve general programming problems. example: Write detailed prompt explains update code use new version package. combine rstudioapi package allow user select code, transform , replace existing text. comprehensive example sort app chores, includes prompts automatically generating roxygen documentation blocks, updating testthat code 3rd edition, converting stop() abort() use cli::cli_abort(). automatically look documentation R function, include prompt make easier figure use specific function. can use LLMs explain code, even ask generate diagram. can ask LLM analyse code potential code smells security issues. can function time, explore entire source code package script prompt. use gh find unlabelled issues, extract text, ask LLM figure labels might appropriate. maybe LLM might able help people create better reprexes, simplify reprexes complicated? find useful LLM document function , even knowing ’s likely mostly incorrect. something react make much easier get started. ’re working code data another programming language, can ask LLM convert R code . Even ’s perfect, ’s still typically much faster everything .","code":""},{"path":"https://ellmer.tidyverse.org/articles/ellmer.html","id":"miscellaneous","dir":"Articles","previous_headings":"","what":"Miscellaneous","title":"Getting started with ellmer","text":"finish ideas seem cool didn’t seem fit categories: Automatically generate alt text plots, using content_image_plot(). Analyse text statistical report look flaws statistical reasoning (e.g. misinterpreting p-values assuming causation correlation exists). Use existing company style guide generate brand.yaml specification automatically style reports, apps, dashboards plots match corporate style guide.","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best practices","title":"Prompt design","text":"’s highly likely ’ll end writing long, possibly multi-page prompts. ensure success task, two recommendations. First, put prompt , separate file. Second, write prompts using markdown. reason use markdown ’s quite readable LLMs (humans), allows things like use headers divide prompt sections itemised lists enumerate multiple options. can see examples style prompt : https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md https://github.com/jcheng5/py-sidebot/blob/main/prompt.md https://github.com/simonpcouch/pal/tree/main/inst/prompts https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md terms file names, one prompt project, call prompt.md. multiple prompts, give informative names like prompt-extract-metadata.md prompt-summarize-text.md. ’re writing package, put prompt(s) inst/prompts, otherwise ’s fine put project’s root directory. prompts going change time, ’d highly recommend commiting git repo. ensure can easily see changed, accidentally make mistake can easily roll back known good verison. prompt includes dynamic data, use ellmer::interpolate_file() intergrate prompt. interpolate_file() works like glue uses {{ }} instead { } make easier work JSON. iterate prompt, ’s good idea build small set challenging examples can regularly re-check latest version prompt. Currently ’ll need hand, hope eventually provide tools ’ll help little formally. Unfortunately, won’t see best practices action vignette since ’re keeping prompts short inline make easier grok ’s going .","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"code-generation","dir":"Articles","previous_headings":"","what":"Code generation","title":"Prompt design","text":"Let’s explore prompt design simple code generation task: ’ll use chat_anthropic() problem experience best job generating code.","code":"question <- \"   How can I compute the mean and median of variables a, b, c, and so on,   all the way up to z, grouped by age and sex. \""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"basic-flavour","dir":"Articles","previous_headings":"Code generation","what":"Basic flavour","title":"Prompt design","text":"don’t provide system prompt, sometimes get answers different languages different styles R code: can ensure always get R code specific style providing system prompt: Note ’m using system prompt (defines general behaviour) user prompt (asks specific question). put content user prompt get similar results, think ’s helpful use cleanly divide general framing response specific questions ask. Since ’m mostly interested code, ask drop explanation sample data: course, want different style R code, just ask :","code":"chat <- chat_anthropic() #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> # Computing Mean and Median by Age and Sex #>  #> To compute the mean and median of variables a through z grouped by age #> and sex, you'll need to use a statistical software package. Here's how #> to do it in several common platforms: #>  #> ## In R #> ```r #> # Assuming your data is in a dataframe called 'df' #> library(dplyr) #>  #> summary_stats <- df %>% #>   group_by(age, sex) %>% #>   summarize(across(a:z, list( #>     mean = ~mean(., na.rm = TRUE), #>     median = ~median(., na.rm = TRUE) #>   ))) #> ``` #>  #> ## In Python with pandas #> ```python #> import pandas as pd #>  #> # Assuming your data is in a dataframe called 'df' #> # For mean #> means = df.groupby(['age',  #> 'sex'])[list('abcdefghijklmnopqrstuvwxyz')].mean() #>  #> # For median #> medians = df.groupby(['age',  #> 'sex'])[list('abcdefghijklmnopqrstuvwxyz')].median() #>  #> # Or combined #> summary = df.groupby(['age',  #> 'sex'])[list('abcdefghijklmnopqrstuvwxyz')].agg(['mean', 'median']) #> ``` #>  #> ## In SPSS #> ``` #> SORT CASES BY age sex. #> AGGREGATE #>   /OUTFILE=* MODE=ADDVARIABLES #>   /BREAK=age sex #>   /a_mean=MEAN(a) /a_median=MEDIAN(a) #>   /b_mean=MEAN(b) /b_median=MEDIAN(b) #>   /* Continue for all variables through z */ #>   /z_mean=MEAN(z) /z_median=MEDIAN(z). #> ``` #>  #> ## In Stata #> ```stata #> collapse (mean) a_mean=a b_mean=b ... z_mean=z /// #>          (median) a_median=a b_median=b ... z_median=z, /// #>          by(age sex) #> ``` #>  #> ## In SAS #> ```sas #> PROC MEANS DATA=mydata MEAN MEDIAN; #>   CLASS age sex; #>   VAR a b c d e f g h i j k l m n o p q r s t u v w x y z; #>   OUTPUT OUT=summary_stats MEAN= MEDIAN= / AUTONAME; #> RUN; #> ``` #>  #> Make sure your variables are properly formatted as numeric before  #> computing these statistics, and consider how you want to handle  #> missing values in your analysis. chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> # Computing Mean and Median by Group in R #>  #> To compute the mean and median of variables a through z, grouped by  #> age and sex, I'll use the tidyverse approach. Here's how to do it  #> efficiently: #>  #> ```r #> library(tidyverse) #>  #> # Assuming your data is in a dataframe called 'df' #> result <- df %>% #>   # Group by age and sex #>   group_by(age, sex) %>% #>   # Calculate mean and median for variables a through z #>   summarize(across( #>     .cols = a:z, #>     .fns = list(mean = mean, median = median), #>     .names = \"{.col}_{.fn}\", #>     na.rm = TRUE #>   )) %>% #>   # Optional: Convert back to regular dataframe #>   ungroup() #> ``` #>  #> This code: #> 1. Groups your data by age and sex #> 2. Uses `across()` to apply both mean and median functions to all  #> variables from a to z #> 3. Names the resulting columns as \"variable_function\" (e.g., \"a_mean\", #> \"a_median\") #> 4. Handles missing values with `na.rm = TRUE` #>  #> If your variables aren't conveniently named a through z in sequence,  #> you can use a character vector instead: #>  #> ```r #> variables <- letters[1:26]  # Creates c(\"a\", \"b\", ..., \"z\") #>  #> result <- df %>% #>   group_by(age, sex) %>% #>   summarize(across( #>     .cols = all_of(variables), #>     .fns = list(mean = mean, median = median), #>     .names = \"{.col}_{.fn}\", #>     na.rm = TRUE #>   )) %>% #>   ungroup() #> ``` #>  #> The result will be a dataframe with age and sex columns, followed by  #> mean and median columns for each variable. chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> library(tidyverse) #>  #> data %>% #>   group_by(age, sex) %>% #>   summarize(across(a:z, list(mean = mean, median = median), na.rm =  #> TRUE), #>             .groups = \"drop\") #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers data.table.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> library(data.table) #>  #> dt[, lapply(.SD, function(x) list(mean = mean(x, na.rm = TRUE),  #>                                   median = median(x, na.rm = TRUE))),  #>     by = .(age, sex),  #>     .SDcols = letters] #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers base R.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> # Assuming data frame 'df' with variables a through z, age, and sex #> vars <- letters[1:26] #> result <- lapply(split(df, list(df$age, df$sex)), function(subset) { #>   means <- sapply(subset[, vars], mean, na.rm = TRUE) #>   medians <- sapply(subset[, vars], median, na.rm = TRUE) #>   data.frame( #>     age = unique(subset$age), #>     sex = unique(subset$sex), #>     variable = vars, #>     mean = means, #>     median = medians #>   ) #> }) #>  #> # Combine results into a single data frame #> final_result <- do.call(rbind, result) #> rownames(final_result) <- NULL #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"be-explicit","dir":"Articles","previous_headings":"Code generation","what":"Be explicit","title":"Prompt design","text":"’s something output don’t like, try explicit. example, code isn’t styled quite ’d like , provide details want: still doesn’t yield exactly code ’d write, ’s pretty close. provide different prompt looking explanation code:","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data.    Follow the tidyverse style guide:   * Spread long function calls across multiple lines.   * Where needed, always indent function calls with two spaces.   * Only name arguments that are less commonly used.   * Always use double quotes for strings.   * Use the base pipe, `|>`, not the magrittr pipe `%>%`. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> library(tidyverse) #>  #> data |> #>   group_by(age, sex) |> #>   summarize( #>     across( #>       a:z, #>       list( #>         mean = mean, #>         median = median #>       ), #>       na.rm = TRUE #>     ), #>     .groups = \"drop\" #>   ) #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R teacher.   I am a new R user who wants to improve my programming skills.   Help me understand the code you produce by explaining each function call with   a brief comment. For more complicated calls, add documentation to each   argument. Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> # Load dplyr for data manipulation #> library(dplyr) #>  #> # Create a vector of variable names from a to z #> vars <- letters #>  #> # Compute mean and median for each variable, grouped by age and sex #> result <- data %>% #>   group_by(age, sex) %>% #>   summarise(across( #>     .cols = all_of(vars),  # Select all variables from a to z #>     .fns = list( #>       mean = ~mean(., na.rm = TRUE),  # Calculate mean, removing NA  #> values #>       median = ~median(., na.rm = TRUE)  # Calculate median, removing  #> NA values #>     ) #>   )) #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"teach-it-about-new-features","dir":"Articles","previous_headings":"Code generation","what":"Teach it about new features","title":"Prompt design","text":"can imagine LLMs sort average internet given point time. means provide popular answers, tend reflect older coding styles (either new features aren’t index, older features much popular). want code use specific newer language features, might need provide examples :","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer.   Just give me the code; no explanation in text.   Use the `.by` argument rather than `group_by()`.   dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.   e.g., instead of:    transactions |>     group_by(company, year) |>     mutate(total = sum(revenue))    write this:   transactions |>     mutate(       total = sum(revenue),       .by = c(company, year)     ) \" ) #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(question) #> ```r #> data |> #>   summarize( #>     across(a:z, list(mean = mean, median = median), na.rm = TRUE), #>     .by = c(age, sex) #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"structured-data","dir":"Articles","previous_headings":"","what":"Structured data","title":"Prompt design","text":"Providing rich set examples great way encourage output produce exactly want. known multi-shot prompting. ’ll work prompt designed extract structured data recipes, ideas apply many situations.","code":""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"getting-started","dir":"Articles","previous_headings":"Structured data","what":"Getting started","title":"Prompt design","text":"overall goal turn list ingredients, like following, nicely structured JSON can analyse R (e.g. compute total weight, scale recipe , convert units volumes weights). (isn’t ingredient list real recipe includes sampling styles encountered project.) don’t strong feelings data structure look like, can start loose prompt see get back. find useful pattern underspecified problems heavy lifting lies precisely defining problem want solve. Seeing LLM’s attempt create data structure gives something react , rather start blank page. (don’t know additional colour, “’re expert baker also loves JSON”, anything, like think helps LLM get right mindset nerdy baker.)","code":"ingredients <- \"   ¾ cup (150g) dark brown sugar   2 large eggs   ¾ cup (165g) sour cream   ½ cup (113g) unsalted butter, melted   1 teaspoon vanilla extract   ¾ teaspoon kosher salt   ⅓ cup (80ml) neutral oil   1½ cups (190g) all-purpose flour   150g plus 1½ teaspoons sugar \" instruct_json <- \"   You're an expert baker who also loves JSON. I am going to give you a list of   ingredients and your job is to return nicely structured JSON. Just return the   JSON and no other commentary. \"  chat <- chat_openai(instruct_json) #> Using model = \"gpt-4o\". chat$chat(ingredients) #> ```json #> { #>   \"ingredients\": [ #>     { #>       \"quantity\": \"¾ cup\", #>       \"weight\": \"150g\", #>       \"ingredient\": \"dark brown sugar\" #>     }, #>     { #>       \"quantity\": \"2\", #>       \"unit\": \"large\", #>       \"ingredient\": \"eggs\" #>     }, #>     { #>       \"quantity\": \"¾ cup\", #>       \"weight\": \"165g\", #>       \"ingredient\": \"sour cream\" #>     }, #>     { #>       \"quantity\": \"½ cup\", #>       \"weight\": \"113g\", #>       \"ingredient\": \"unsalted butter\", #>       \"preparation\": \"melted\" #>     }, #>     { #>       \"quantity\": \"1 teaspoon\", #>       \"ingredient\": \"vanilla extract\" #>     }, #>     { #>       \"quantity\": \"¾ teaspoon\", #>       \"ingredient\": \"kosher salt\" #>     }, #>     { #>       \"quantity\": \"⅓ cup\", #>       \"volume\": \"80ml\", #>       \"ingredient\": \"neutral oil\" #>     }, #>     { #>       \"quantity\": \"1½ cups\", #>       \"weight\": \"190g\", #>       \"ingredient\": \"all-purpose flour\" #>     }, #>     { #>       \"quantity\": \"150g plus 1½ teaspoons\", #>       \"ingredient\": \"sugar\" #>     } #>   ] #> } #> ```"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"provide-examples","dir":"Articles","previous_headings":"Structured data","what":"Provide examples","title":"Prompt design","text":"isn’t bad start, prefer cook weight want see volumes weight isn’t available provide couple examples ’m looking . pleasantly suprised can provide input output examples loose format. Just providing examples seems work remarkably well. found useful also include description examples trying accomplish. ’m sure helps LLM , certainly makes easier understand organisation whole prompt check ’ve covered key pieces ’m interested . structure also allows give LLMs hint want multiple ingredients stored, .e. JSON array. iterated prompt, looking results different recipes get sense LLM getting wrong. Much felt like waws iterating understanding problem didn’t start knowing exactly wanted data. example, started didn’t really think various ways ingredients specified. later analysis, always want quantities number, even originally fractions, units aren’t precise (like pinch). made realise ingredients unitless. might want take look full prompt see ended .","code":"instruct_weight <- r\"(   Here are some examples of the sort of output I'm looking for:    ¾ cup (150g) dark brown sugar   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}    ⅓ cup (80ml) neutral oil   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}    2 t ground cinnamon   {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"} )\"  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4o\". chat$chat(ingredients) #> ```json #> [ #>   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"large eggs\", \"quantity\": 2, \"unit\": \"piece\"}, #>   {\"name\": \"sour cream\", \"quantity\": 165, \"unit\": \"g\"}, #>   {\"name\": \"unsalted butter, melted\", \"quantity\": 113, \"unit\": \"g\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"}, #>   {\"name\": \"kosher salt\", \"quantity\": 0.75, \"unit\": \"teaspoon\"}, #>   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 190, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 1.5, \"unit\": \"teaspoon\"} #> ] #> ``` instruct_weight <- r\"(   * If an ingredient has both weight and volume, extract only the weight:    ¾ cup (150g) dark brown sugar   [     {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}   ]  * If an ingredient only lists a volume, extract that.    2 t ground cinnamon   ⅓ cup (80ml) neutral oil   [     {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"},     {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}   ] )\" instruct_unit <- r\"( * If the unit uses a fraction, convert it to a decimal.    ⅓ cup sugar   ½ teaspoon salt   [     {\"name\": \"dark brown sugar\", \"quantity\": 0.33, \"unit\": \"cup\"},     {\"name\": \"salt\", \"quantity\": 0.5, \"unit\": \"teaspoon\"}   ]  * Quantities are always numbers    pinch of kosher salt   [     {\"name\": \"kosher salt\", \"quantity\": 1, \"unit\": \"pinch\"}   ]  * Some ingredients don't have a unit.   2 eggs   1 lime   1 apple   [     {\"name\": \"egg\", \"quantity\": 2},     {\"name\": \"lime\", \"quantity\": 1},     {\"name\", \"apple\", \"quantity\": 1}   ] )\""},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"structured-data-1","dir":"Articles","previous_headings":"Structured data","what":"Structured data","title":"Prompt design","text":"Now ’ve iterated get data structure like, seems useful formalise tell LLM exactly ’m looking dealing structured data. guarantees LLM return JSON, JSON fields expect, ellmer convert R data structure.","code":"type_ingredient <- type_object(   name = type_string(\"Ingredient name\"),   quantity = type_number(),   unit = type_string(\"Unit of measurement\") )  type_ingredients <- type_array(items = type_ingredient)  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4o\". chat$extract_data(ingredients, type = type_ingredients) #>                 name quantity     unit #> 1   dark brown sugar   150.00        g #> 2               eggs     2.00    large #> 3         sour cream   165.00        g #> 4    unsalted butter   113.00        g #> 5    vanilla extract     1.00 teaspoon #> 6        kosher salt     0.75 teaspoon #> 7        neutral oil    80.00       ml #> 8  all-purpose flour   190.00        g #> 9              sugar   150.00        g #> 10             sugar     1.50 teaspoon"},{"path":"https://ellmer.tidyverse.org/articles/prompt-design.html","id":"capturing-raw-input","dir":"Articles","previous_headings":"Structured data","what":"Capturing raw input","title":"Prompt design","text":"One thing ’d next time also include raw ingredient names output. doesn’t make much difference simple example makes much easier align input output start developing automated measures well prompt . think particularly important ’re working even less structured text. example, imagine text: Including input text output makes easier see ’s good job: ran writing vignette, seemed working weight ingredients specified volume, even though prompt specifically asks . may suggest need broaden examples.","code":"instruct_weight_input <- r\"(   * If an ingredient has both weight and volume, extract only the weight:      ¾ cup (150g) dark brown sugar     [       {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\", \"input\": \"¾ cup (150g) dark brown sugar\"}     ]    * If an ingredient only lists a volume, extract that.      2 t ground cinnamon     ⅓ cup (80ml) neutral oil     [       {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\", \"input\": \"2 t ground cinnamon\"},       {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\", \"input\": \"⅓ cup (80ml) neutral oil\"}     ] )\" recipe <- r\"(   In a large bowl, cream together one cup of softened unsalted butter and a   quarter cup of white sugar until smooth. Beat in an egg and 1 teaspoon of   vanilla extract. Gradually stir in 2 cups of all-purpose flour until the   dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop   spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C)   for 10-12 minutes, or until the edges are lightly browned. Let the cookies   cool on the baking sheet for a few minutes before transferring to a wire   rack to cool completely. Enjoy! )\" chat <- chat_openai(c(instruct_json, instruct_weight_input)) #> Using model = \"gpt-4o\". chat$chat(recipe) #> ```json #> [ #>   {\"name\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\", \"input\":  #> \"one cup of softened unsalted butter\"}, #>   {\"name\": \"white sugar\", \"quantity\": 0.25, \"unit\": \"cup\", \"input\": \"a #> quarter cup of white sugar\"}, #>   {\"name\": \"egg\", \"quantity\": 1, \"unit\": \"whole\", \"input\": \"an egg\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\",  #> \"input\": \"1 teaspoon of vanilla extract\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cup\", \"input\": #> \"2 cups of all-purpose flour\"}, #>   {\"name\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\",  #> \"input\": \"1 cup of semisweet chocolate chips\"} #> ] #> ```"},{"path":[]},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console returns result response complete.) want process response arrives, can use stream() method. useful want send response, realtime, somewhere R console (e.g., file, HTTP response, Shiny chat window), want manipulate response displaying without giving immediacy streaming. stream() method, returns coro generator, can process response looping arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"ellmer also supports async usage. useful want run multiple, concurrent chat sessions. particularly important Shiny applications using methods described block Shiny app users duration response. use async chat, call chat_async()/stream_async() instead chat()/stream(). _async variants take arguments construction return promise instead actual response. Remember chat objects stateful; preserve conversation history interact . means doesn’t make sense issue multiple, concurrent chat/stream operations chat object conversation history can become corrupted interleaved conversation fragments. need run concurrent chat sessions, create multiple chat objects.","code":""},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, ’d use chat() method , handle result promise instead string. TODO: Shiny example","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://ellmer.tidyverse.org/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, ’d use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, ’ll simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"structured-data-basics","dir":"Articles","previous_headings":"","what":"Structured data basics","title":"Structured data","text":"extract structured data call $extract_data() method instead $chat() method. ’ll also need define type specification describes structure data want (shortly). ’s simple example extracts two specific values string: basic idea works images :","code":"chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(   \"My name is Susan and I'm 13 years old\",   type = type_object(     age = type_number(),     name = type_string()   ) ) #> $age #> [1] 13 #>  #> $name #> [1] \"Susan\" chat$extract_data(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   type = type_object(     primary_shape = type_string(),     primary_colour = type_string()   ) ) #> $primary_shape #> [1] \"oval\" #>  #> $primary_colour #> [1] \"gray and blue\""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"data-types-basics","dir":"Articles","previous_headings":"","what":"Data types basics","title":"Structured data","text":"define desired type specification (also known schema), use type_() functions. (might already familiar ’ve done function calling, discussed vignette(\"function-calling\")). type functions can divided three main groups: Scalars represent five types single values, type_boolean(), type_integer(), type_number(), type_string(), type_enum(), represent single logical, integer, double, string, factor value respectively. Arrays represent number values type. created type_array(). must always supply item argument specifies type individual element. Arrays scalars similar R’s atomic vectors: can also arrays arrays arrays objects, closely resemble lists well defined structures: Objects represent collection named values. created type_object(). Objects can contain number scalars, arrays, objects. similar named lists R. Using type specifications ensures LLM return JSON. ellmer goes one step convert results closest R analog. Currently, converts arrays boolean, integers, numbers, strings logical, integer, numeric, character vectors. Arrays objects converted data frames. can opt-get plain lists setting convert = FALSE $extract_data(). addition defining types, need provide LLM information actually want. purpose first argument, description, string describes data want. good place ask nicely attributes ’ll like value (e.g. minimum maximum values, date formats, …). ’s guarantee requests honoured, LLM usually make best effort . Now ’ll dive examples coming back talk details data types.","code":"type_logical_vector <- type_array(items = type_boolean()) type_integer_vector <- type_array(items = type_integer()) type_double_vector <- type_array(items = type_number()) type_character_vector <- type_array(items = type_string()) list_of_integers <- type_array(items = type_integer_vector) type_person <- type_object(   name = type_string(),   age = type_integer(),   hobbies = type_array(items = type_string()) ) type_type_person <- type_object(   \"A person\",   name = type_string(\"Name\"),   age = type_integer(\"Age, in years.\"),   hobbies = type_array(     \"List of hobbies. Should be exclusive and brief.\",     items = type_string()   ) )"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Structured data","text":"following examples, closely inspired Claude documentation, hint ways can use structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-1-article-summarisation","dir":"Articles","previous_headings":"Examples","what":"Example 1: Article summarisation","title":"Structured data","text":"","code":"text <- readLines(system.file(\"examples/third-party-testing.txt\", package = \"ellmer\")) # url <- \"https://www.anthropic.com/news/third-party-testing\" # html <- rvest::read_html(url) # text <- rvest::html_text2(rvest::html_element(html, \"article\"))  type_summary <- type_object(   \"Summary of the article.\",   author = type_string(\"Name of the article author\"),   topics = type_array(     'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.',     type_string(),   ),   summary = type_string(\"Summary of the article. One or two paragraphs max\"),   coherence = type_integer(\"Coherence of the article's key points, 0-100 (inclusive)\"),   persuasion = type_number(\"Article's persuasion score, 0.0-1.0 (inclusive)\") )  chat <- chat_openai() #> Using model = \"gpt-4o\". data <- chat$extract_data(text, type = type_summary) cat(data$summary) #> The article discusses the necessity of implementing effective third-party testing for advanced AI systems as a part of critical AI policy. It underlines how the complexity and capabilities of AI models, such as large-scale generative AI systems, necessitate a regulatory environment that effectively assesses and mitigates potential societal harms, both accidental and intentional. It outlines the key aspects of a robust testing regime and argues for its importance in areas ranging from national security risks to ensuring public trust and safety. Moreover, it suggests a multi-actor approach, involving industry, government, and academia, for both developing the testing protocols and executing them. Anthropic's future efforts to support these initiatives are also highlighted to emphasize the collaborative efforts needed to address the evolving challenges and ensure the ethical deployment of AI technologies.  str(data) #> List of 5 #>  $ author    : chr \"Anonymous\" #>  $ topics    : chr [1:5] \"AI policy\" \"AI safety\" \"Third-party testing\" \"Regulatory practices\" ... #>  $ summary   : chr \"The article discusses the necessity of implementing effective third-party testing for advanced AI systems as a \"| __truncated__ #>  $ coherence : int 85 #>  $ persuasion: num 0.85"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-2-named-entity-recognition","dir":"Articles","previous_headings":"Examples","what":"Example 2: Named entity recognition","title":"Structured data","text":"","code":"text <- \"   John works at Google in New York. He met with Sarah, the CEO of   Acme Inc., last week in San Francisco. \"  type_named_entity <- type_object(   name = type_string(\"The extracted entity name.\"),   type = type_enum(\"The entity type\", c(\"person\", \"location\", \"organization\")),   context = type_string(\"The context in which the entity appears in the text.\") ) type_named_entities <- type_array(items = type_named_entity)  chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(text, type = type_named_entities) #>            name         type                             context #> 1          John       person         Works at Google in New York #> 2        Google organization                    John's workplace #> 3      New York     location                John's work location #> 4         Sarah       person                     CEO of Acme Inc #> 5     Acme Inc. organization             The company Sarah leads #> 6 San Francisco     location Where John met with Sarah last week"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-3-sentiment-analysis","dir":"Articles","previous_headings":"Examples","what":"Example 3: Sentiment analysis","title":"Structured data","text":"Note ’ve asked nicely scores sum 1, example (least ran code), guaranteed.","code":"text <- \"   The product was okay, but the customer service was terrible. I probably   won't buy from them again. \"  type_sentiment <- type_object(   \"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\",   positive_score = type_number(\"Positive sentiment score, ranging from 0.0 to 1.0.\"),   negative_score = type_number(\"Negative sentiment score, ranging from 0.0 to 1.0.\"),   neutral_score = type_number(\"Neutral sentiment score, ranging from 0.0 to 1.0.\") )  chat <- chat_openai() #> Using model = \"gpt-4o\". str(chat$extract_data(text, type = type_sentiment)) #> List of 3 #>  $ positive_score: num 0.1 #>  $ negative_score: num 0.7 #>  $ neutral_score : num 0.2"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-4-text-classification","dir":"Articles","previous_headings":"Examples","what":"Example 4: Text classification","title":"Structured data","text":"","code":"text <- \"The new quantum computing breakthrough could revolutionize the tech industry.\"  type_classification <- type_array(   \"Array of classification results. The scores should sum to 1.\",   type_object(     name = type_enum(       \"The category name\",       values = c(         \"Politics\",         \"Sports\",         \"Technology\",         \"Entertainment\",         \"Business\",         \"Other\"       )     ),     score = type_number(       \"The classification score for the category, ranging from 0.0 to 1.0.\"     )   ) )  chat <- chat_openai() #> Using model = \"gpt-4o\". data <- chat$extract_data(text, type = type_classification) data #>         name score #> 1 Technology  0.95 #> 2   Business  0.04 #> 3      Other  0.01"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-5-working-with-unknown-keys","dir":"Articles","previous_headings":"Examples","what":"Example 5: Working with unknown keys","title":"Structured data","text":"example works Claude, GPT Gemini, Claude supports adding additional, arbitrary properties.","code":"type_characteristics <- type_object(   \"All characteristics\",   .additional_properties = TRUE )  prompt <- \"   Given a description of a character, your task is to extract all the characteristics of that character.    <description>   The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.   <\/description> \"  chat <- chat_anthropic() #> Using model = \"claude-3-7-sonnet-latest\". str(chat$extract_data(prompt, type = type_characteristics)) #>  list()"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"example-6-extracting-data-from-an-image","dir":"Articles","previous_headings":"Examples","what":"Example 6: Extracting data from an image","title":"Structured data","text":"final example comes Dan Nguyen (can see interesting applications link). goal extract structured data screenshot: Even without descriptions, ChatGPT pretty well:","code":"type_asset <- type_object(   assert_name = type_string(),   owner = type_string(),   location = type_string(),   asset_value_low = type_integer(),   asset_value_high = type_integer(),   income_type = type_string(),   income_low = type_integer(),   income_high = type_integer(),   tx_gt_1000 = type_boolean() ) type_assets <- type_array(items = type_asset)  chat <- chat_openai() #> Using model = \"gpt-4o\". image <- content_image_file(\"congressional-assets.png\") data <- chat$extract_data(image, type = type_assets) data #>                                 assert_name owner #> 1  11 Zinfandel Lane - Home & Vineyard [RP]    JT #> 2 25 Point Lobos - Commercial Property [RP]    SP #>                              location asset_value_low asset_value_high #> 1             St. Helena/Napa, CA, US         5000001         25000000 #> 2 San Francisco/San Francisco, CA, US         5000001         25000000 #>   income_type income_low income_high tx_gt_1000 #> 1 Grape Sales     100001     1000000       TRUE #> 2        Rent     100001     1000000       TRUE"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"advanced-data-types","dir":"Articles","previous_headings":"","what":"Advanced data types","title":"Structured data","text":"Now ’ve seen examples, ’s time get specifics data type declarations.","code":""},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"required-vs-optional","dir":"Articles","previous_headings":"Advanced data types","what":"Required vs optional","title":"Structured data","text":"default, components object required. want make optional, set required = FALSE. good idea don’t think text always contain required fields LLMs may hallucinate data order fulfill spec. example, LLM hallucinates date even though isn’t one text: Note ’ve used explict prompt . example, found generated better results ’s useful place put additional instructions. let LLM know fields optional, ’ll return NULL missing fields:","code":"type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\"),   author = type_string(\"Name of the author\"),   date = type_string(\"Date written in YYYY-MM-DD format.\") )  prompt <- \"   Extract data from the following text:    <text>   # Structured Data   By Hadley Wickham    When using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like.   <\/text> \"  chat <- chat_openai() #> Using model = \"gpt-4o\". chat$extract_data(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> [1] \"2023-10-10\" str(data) #> 'data.frame':    2 obs. of  9 variables: #>  $ assert_name     : chr  \"11 Zinfandel Lane - Home & Vineyard [RP]\" \"25 Point Lobos - Commercial Property [RP]\" #>  $ owner           : chr  \"JT\" \"SP\" #>  $ location        : chr  \"St. Helena/Napa, CA, US\" \"San Francisco/San Francisco, CA, US\" #>  $ asset_value_low : int  5000001 5000001 #>  $ asset_value_high: int  25000000 25000000 #>  $ income_type     : chr  \"Grape Sales\" \"Rent\" #>  $ income_low      : int  100001 100001 #>  $ income_high     : int  1000000 1000000 #>  $ tx_gt_1000      : logi  TRUE TRUE type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\", required = FALSE),   author = type_string(\"Name of the author\", required = FALSE),   date = type_string(\"Date written in YYYY-MM-DD format.\", required = FALSE) ) chat$extract_data(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> NULL"},{"path":"https://ellmer.tidyverse.org/articles/structured-data.html","id":"data-frames","dir":"Articles","previous_headings":"Advanced data types","what":"Data frames","title":"Structured data","text":"want define data frame like object, might tempted create definition similar R uses: object (.e., named list) containing multiple vectors (.e., array): , however, quite right becuase ’s way specify array length. Instead, ’ll need turn data structure “inside ” create array objects: ’re familiar terms row-oriented column-oriented data frames, idea. Since languages don’t possess vectorisation like R, row-oriented structures tend much common wild.","code":"type_my_df <- type_object(   name = type_array(items = type_string()),   age = type_array(items = type_integer()),   height = type_array(items = type_number()),   weight = type_array(items = type_number()) ) type_my_df <- type_array(   items = type_object(     name = type_string(),     age = type_integer(),     height = type_number(),     weight = type_number()   ) )"},{"path":[]},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool/function calling","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . ’s easy think tool calling might work like : fact works like : value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(ellmer)"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool/function calling","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Unfortunately, example run September 18, 2024. Let’s give chat model ability determine current time try .","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that #> was from the current year of 2023, we can calculate the difference in years, months, and days. #> #> From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you #> would add the additional time since then. If it is before, you would consider slightly less than 54 #> years. #> #> As of right now, can you confirm the current date so we can calculate the precise duration?"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool/function calling","text":"first thing ’ll define R function returns current time. tool. Note ’ve gone trouble creating roxygen2 comments. important step help model use tool correctly! Let’s test :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time() #> [1] \"2024-09-18 17:47:14 UTC\""},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"registering-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering tools","title":"Tool/function calling","text":"Now need tell chat object get_current_time function. creating registering tool: fair amount code write, even simple function get_current_time. Fortunately, don’t write hand! generated register_tool call calling create_tool_def(get_current_time), printed code console. create_tool_def() works passing function’s signature documentation GPT-4o, asking generate register_tool call . Note create_tool_def() may create perfect results, must review generated code using . huge time-saver nonetheless, removes tedious boilerplate generation ’d otherwise.","code":"chat <- chat_openai(model = \"gpt-4o\")  chat$register_tool(tool(   get_current_time,   \"Gets the current time in the given time zone.\",   tz = type_string(     \"The time zone to get the current time in. Defaults to `\\\"UTC\\\"`.\",     required = FALSE   ) ))"},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"using-the-tool","dir":"Articles","previous_headings":"Introduction","what":"Using the tool","title":"Tool/function calling","text":"’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using ellmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. #> #> To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19 #> UTC), we need to break it down. #> #> 1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years. #> 2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down: #> #>    - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is: #>      - 1 full month (August) #>      - 30 – 20 = 10 days of July #>      - 18 days of September until 17:47:19 UTC #> #> So, in detail: #>    - 55 years #>    - 1 month #>    - 28 days #>    - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds #> #> Time Total: #> - 55 years #> - 1 month #> - 28 days #> - 23 hours #> - 30 minutes #> - 19 seconds #> #> This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon."},{"path":"https://ellmer.tidyverse.org/articles/tool-calling.html","id":"tool-limitations","dir":"Articles","previous_headings":"Introduction","what":"Tool limitations","title":"Tool/function calling","text":"Remember tool arguments come chat model, tool results returned chat model. means simple, {jsonlite} compatible data types can used inputs outputs. ’s highly recommended stick strings/character, numbers, booleans/logical, null, named unnamed lists types. can forget using functions, environments, external pointers, R6 classes, complex R objects arguments return values. Returning data frames seems work OK, although careful return much data, counts tokens (.e., count context window limit also cost money).","code":""},{"path":"https://ellmer.tidyverse.org/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Joe Cheng. Author. Aaron Jacobs. Author. . Copyright holder, funder.           03wc8by49","code":""},{"path":"https://ellmer.tidyverse.org/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Cheng J, Jacobs (2025). ellmer: Chat Large Language Models. R package version 0.1.1, https://github.com/tidyverse/ellmer, https://ellmer.tidyverse.org.","code":"@Manual{,   title = {ellmer: Chat with Large Language Models},   author = {Hadley Wickham and Joe Cheng and Aaron Jacobs},   year = {2025},   note = {R package version 0.1.1, https://github.com/tidyverse/ellmer},   url = {https://ellmer.tidyverse.org}, }"},{"path":"https://ellmer.tidyverse.org/index.html","id":"ellmer-","dir":"","previous_headings":"","what":"Chat with Large Language Models","title":"Chat with Large Language Models","text":"ellmer makes easy use large language models (LLM) R. supports wide variety LLM providers implements rich set features including streaming outputs, tool/function calling, structured data extraction, . (Looking something similar ellmer python? Check chatlas!)","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Chat with Large Language Models","text":"can install ellmer CRAN :","code":"install.packages(\"ellmer\")"},{"path":"https://ellmer.tidyverse.org/index.html","id":"providers","dir":"","previous_headings":"","what":"Providers","title":"Chat with Large Language Models","text":"ellmer supports wide variety model providers: Anthropic’s Claude: chat_anthropic(). AWS Bedrock: chat_aws_bedrock(). Azure OpenAI: chat_azure_openai(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini: chat_google_gemini(). Groq: chat_groq(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() chat_cortex_analyst(). VLLM: chat_vllm().","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"providermodel-choice","dir":"","previous_headings":"Providers","what":"Provider/model choice","title":"Chat with Large Language Models","text":"’re using ellmer inside organisation, may internal policies limit models big cloud providers, e.g. chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake(). ’re using ellmer exploration, ’ll lot freedom, recommendations help get started: chat_openai() chat_anthropic() good places start. chat_openai() defaults GPT-4o, can use model = \"gpt-4o-mini\" cheaper, lower-quality model, model = \"o1-mini\" complex reasoning. chat_anthropic() also good; defaults Claude 3.5 Sonnet, found particularly good writing code. chat_google_gemini() great large prompts much larger context window models. allows 1 million tokens, compared Claude 3.5 Sonnet’s 200k GPT-4o’s 128k. also comes generous free tier (downside data used improve model). chat_ollama(), uses Ollama, allows run models computer. biggest models can run locally aren’t good state art hosted models, don’t share data effectively free.","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"authentication","dir":"","previous_headings":"Providers","what":"Authentication","title":"Chat with Large Language Models","text":"Authentication works little differently depending provider. popular ones (including OpenAI Anthropic) require obtain API key. recommend save environment variable rather using directly code, deploy app report uses ellmer another system, ’ll need ensure environment variable available , . ellmer also automatically detects many OAuth IAM-based credentials used big cloud providers (currently chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake()). includes credentials platforms managed Posit Workbench Posit Connect. find cases ellmer detect credentials one cloud providers, feel free open issue; ’re happy add auth mechanisms needed.","code":""},{"path":"https://ellmer.tidyverse.org/index.html","id":"using-ellmer","dir":"","previous_headings":"","what":"Using ellmer","title":"Chat with Large Language Models","text":"can work ellmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful R6 objects: retain context conversation, new query builds previous ones. call methods $.","code":"library(ellmer)  chat <- chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\", )"},{"path":"https://ellmer.tidyverse.org/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using ellmer","what":"Interactive chat console","title":"Chat with Large Language Models","text":"interactive least programmatic way using ellmer chat directly R console browser live_console(chat) live_browser(): Keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist exit back R prompt. true regardless chat function use.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://ellmer.tidyverse.org/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using ellmer","what":"Interactive method call","title":"Chat with Large Language Models","text":"second interactive way chat call chat() method: initialize chat object global environment, chat method stream response console. entire response received, ’s also (invisibly) returned character vector. useful want see response arrives, don’t want enter chat console. want ask question image, can pass one additional input arguments using content_image_file() /content_image_url():","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by the S programming language, particularly S-PLUS. #> Other languages that had an impact include Scheme and various data analysis #> languages. chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo of R features a stylized letter \"R\" in blue, enclosed in an oval #> shape that resembles the letter \"O,\" signifying the programming language's #> name. The design conveys a modern and professional look, reflecting its use #> in statistical computing and data analysis. The blue color often represents #> trust and reliability, which aligns with R's role in data science."},{"path":"https://ellmer.tidyverse.org/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using ellmer","what":"Programmatic chat","title":"Chat with Large Language Models","text":"programmatic way chat create chat object inside function. , live streaming automatically suppressed $chat() returns result string: needed, can manually control behaviour echo argument. useful programming ellmer result either intended human consumption want process response displaying .","code":"my_function <- function() {   chat <- chat_openai(     model = \"gpt-4o-mini\",     system_prompt = \"You are a friendly but terse assistant.\",   )   chat$chat(\"Is R a functional programming language?\") } my_function() #> [1] \"Yes, R supports functional programming concepts. It allows functions to #> be first-class objects, supports higher-order functions, and encourages the #> use of functions as core components of code. However, it also supports #> procedural and object-oriented programming styles.\""},{"path":"https://ellmer.tidyverse.org/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Chat with Large Language Models","text":"ellmer comes bunch vignettes help learn : Learn key vocabulary see example use cases vignette(\"ellmer\"). Learn design prompt vignette(\"prompt-design\"). Learn tool/function calling vignette(\"tool-calling\"). Learn extract structured data vignette(\"structured-data\"). Learn streaming async APIs vignette(\"streaming-async\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"A chat — Chat","title":"A chat — Chat","text":"Chat sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chat — Chat","text":"Chat object","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"A chat — Chat","text":"Chat$new() Chat$get_turns() Chat$set_turns() Chat$add_turn() Chat$get_system_prompt() Chat$get_model() Chat$set_system_prompt() Chat$tokens() Chat$get_cost() Chat$last_turn() Chat$chat() Chat$chat_parallel() Chat$extract_data() Chat$extract_data_parallel() Chat$extract_data_async() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$get_provider() Chat$get_tools() Chat$set_tools() Chat$clone()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$new(provider, turns, seed = NULL, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"provider provider object. turns unnamed list turns start chat (.e., continuing previous conversation). NULL zero-length list, conversation begins scratch. seed Optional integer seed ChatGPT uses try make output reproducible. echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-turns-","dir":"Reference","previous_headings":"","what":"Method get_turns()","title":"A chat — Chat","text":"Retrieve turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_turns(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-set-turns-","dir":"Reference","previous_headings":"","what":"Method set_turns()","title":"A chat — Chat","text":"Replace existing turns new list.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_turns(value)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value list Turns.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-add-turn-","dir":"Reference","previous_headings":"","what":"Method add_turn()","title":"A chat — Chat","text":"Add pair turns chat.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$add_turn(user, system)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"user user Turn. system system Turn.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-system-prompt-","dir":"Reference","previous_headings":"","what":"Method get_system_prompt()","title":"A chat — Chat","text":"set, system prompt, , NULL.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_system_prompt()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-model-","dir":"Reference","previous_headings":"","what":"Method get_model()","title":"A chat — Chat","text":"Retrieve model name","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_model()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-set-system-prompt-","dir":"Reference","previous_headings":"","what":"Method set_system_prompt()","title":"A chat — Chat","text":"Update system prompt","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_system_prompt(value)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value string giving new system prompt","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-tokens-","dir":"Reference","previous_headings":"","what":"Method tokens()","title":"A chat — Chat","text":"data frame tokens column proides number input tokens used user turns number output tokens used assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$tokens(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"A chat — Chat","text":"cost chat","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_cost(include = c(\"all\", \"last\"))"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include default, \"\", gives total cumulative cost chat. Alternatively, use \"last\" get cost just recent turn.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"A chat — Chat","text":"last turn returned assistant.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"A chat — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-chat-parallel-","dir":"Reference","previous_headings":"","what":"Method chat_parallel()","title":"A chat — Chat","text":"Submit multiple prompts parallel. Returns list Chat objects, one prompt.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_parallel(prompts, max_active = 10, rpm = 500)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"prompts list user prompts. max_active maximum number simultaenous requests send. rpm Maximum number requests per minute.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-extract-data-","dir":"Reference","previous_headings":"","what":"Method extract_data()","title":"A chat — Chat","text":"Extract structured data","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-extract-data-parallel-","dir":"Reference","previous_headings":"","what":"Method extract_data_parallel()","title":"A chat — Chat","text":"Submit multiple prompts parallel. Returns list extracted data, one prompt.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data_parallel(   prompts,   type,   convert = TRUE,   max_active = 10,   rpm = 500 )"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"prompts list user prompts. type type specification extracted data. created type_() function. convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector. max_active maximum number simultaenous requests send. rpm Maximum number requests per minute.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-extract-data-async-","dir":"Reference","previous_headings":"","what":"Method extract_data_async()","title":"A chat — Chat","text":"Extract structured data, asynchronously. Returns promise resolves object matching type specification.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data_async(..., type, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"A chat — Chat","text":"Submit input chatbot, receive promise resolves response . Returns promise resolves string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_async(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"A chat — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"A chat — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream_async(...)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"A chat — Chat","text":"Register tool (R function) chatbot can use. chatbot decides use function, ellmer automatically call submit results back. return value function. Generally, either string, JSON-serializable value. must direct control structure JSON returned, can return JSON-serializable value wrapped base::(), ellmer leave alone entire request JSON-serialized.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$register_tool(tool_def)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tool_def Tool definition created tool().","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-provider-","dir":"Reference","previous_headings":"","what":"Method get_provider()","title":"A chat — Chat","text":"Get underlying provider object. expert use .","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_provider()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-get-tools-","dir":"Reference","previous_headings":"","what":"Method get_tools()","title":"A chat — Chat","text":"Retrieve list registered tools.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_tools()"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-set-tools-","dir":"Reference","previous_headings":"","what":"Method set_tools()","title":"A chat — Chat","text":"Sets available tools. expert use ; users use register_tool().","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_tools(tools)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"A chat — Chat","text":"objects class cloneable method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chat — Chat","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(\"Tell me a funny joke\") #> Sure, here's one for you: #>  #> Why don’t skeletons fight each other? #>  #> They don’t have the guts!"},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"Use functions writing package extends ellmer need customise methods various types content. normal use, see content_image_url() friends. ellmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents types content can either sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user). object automatically created value returned calling tool() function. Alternatively, expert users can return ContentToolResult tool() function include additional data customize display result.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = stop(\"Required\"))  ContentImage()  ContentImageRemote(url = stop(\"Required\"), detail = \"\")  ContentImageInline(type = stop(\"Required\"), data = NULL)  ContentToolRequest(   id = stop(\"Required\"),   name = stop(\"Required\"),   arguments = list(),   tool = NULL )  ContentToolResult(value = NULL, error = NULL, extra = list(), request = NULL)  ContentThinking(thinking = stop(\"Required\"), extra = list())  ContentPDF(type = stop(\"Required\"), data = stop(\"Required\"))"},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result). Automatically managed ellmer. name Function name arguments Named list arguments call function . tool ellmer automatically matches tool request tools defined chatbot. NULL, request match defined tool. value results calling tool function, succeeded. error error message, string, error condition thrown result failure calling tool function. Must NULL tool call successful. extra Additional data. request ContentToolRequest associated tool result, automatically added ellmer evaluating tool call. thinking text thinking output.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content types received from and sent to a chatbot — Content","text":"S7 objects inherit Content","code":""},{"path":"https://ellmer.tidyverse.org/reference/Content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content() #> <ellmer::Content> ContentText(\"Tell me a joke\") #> <ellmer::ContentText> #>  @ text: chr \"Tell me a joke\" ContentImageRemote(\"https://www.r-project.org/Rlogo.png\") #> <ellmer::ContentImageRemote> #>  @ url   : chr \"https://www.r-project.org/Rlogo.png\" #>  @ detail: chr \"\" ContentToolRequest(id = \"abc\", name = \"mean\", arguments = list(x = 1:5)) #> <ellmer::ContentToolRequest> #>  @ id       : chr \"abc\" #>  @ name     : chr \"mean\" #>  @ arguments:List of 1 #>  .. $ x: int [1:5] 1 2 3 4 5 #>  @ tool     : NULL"},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = stop(\"Required\"),   model = stop(\"Required\"),   base_url = stop(\"Required\"),   params = list(),   extra_args = list() )"},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"name Name provider. model Name model. base_url base URL API. params list standard parameters created params(). extra_args Arbitrary extra arguments included request body.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chatbot provider — Provider","text":"S7 Provider object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Provider.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = \"CoolModels\",   model = \"my_model\",   base_url = \"https://cool-models.com\" ) #> <ellmer::Provider> #>  @ name      : chr \"CoolModels\" #>  @ model     : chr \"my_model\" #>  @ base_url  : chr \"https://cool-models.com\" #>  @ params    : list() #>  @ extra_args: list()"},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user or assistant turn — Turn","title":"A user or assistant turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, ellmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user or assistant turn — Turn","text":"","code":"Turn(   role,   contents = list(),   json = list(),   tokens = c(0, 0),   completed = Sys.time() )"},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user or assistant turn — Turn","text":"role Either \"user\", \"assistant\", \"system\". contents list Content objects. json serialized JSON corresponding underlying data turns. Currently provided assistant. useful information returned provider ellmer otherwise expose. tokens numeric vector length 2 representing number input output tokens (respectively) used turn. Currently recorded assistant turns. completed POSIXct timestamp indicating turn completed.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A user or assistant turn — Turn","text":"S7 Turn object","code":""},{"path":"https://ellmer.tidyverse.org/reference/Turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A user or assistant turn — Turn","text":"","code":"Turn(role = \"user\", contents = list(ContentText(\"Hello, world!\"))) #> <ellmer::Turn> #>  @ role     : chr \"user\" #>  @ contents :List of 1 #>  .. $ : <ellmer::ContentText> #>  ..  ..@ text: chr \"Hello, world!\" #>  @ json     : list() #>  @ tokens   : num [1:2] 0 0 #>  @ text     : chr \"Hello, world!\" #>  @ completed: POSIXct[1:1], format: \"2025-04-04 14:52:22\""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":null,"dir":"Reference","previous_headings":"","what":"Type definitions for function calling and structured data extraction. — Type","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 classes provided use package devlopers extending ellmer. every day use, use type_boolean() friends.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(description = NULL, required = TRUE, type = stop(\"Required\"))  TypeEnum(description = NULL, required = TRUE, values = character(0))  TypeArray(description = NULL, required = TRUE, items = Type())  TypeObject(   description = NULL,   required = TRUE,   properties = list(),   additional_properties = TRUE )"},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type definitions for function calling and structured data extraction. — Type","text":"description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required component required? FALSE, component exist data, LLM may hallucinate value. applies element nested inside type_object(). type Basic type name. Must one boolean, integer, number, string. values Character vector permitted values. items type array items. Can created type_ function. properties Named list properties stored inside object. element S7 Type object.` additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 objects inheriting Type","code":""},{"path":"https://ellmer.tidyverse.org/reference/Type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(type = \"boolean\") #> <ellmer::TypeBasic> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ type       : chr \"boolean\" TypeArray(items = TypeBasic(type = \"boolean\")) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"boolean\""},{"path":"https://ellmer.tidyverse.org/reference/chat_anthropic.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_anthropic","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Anthropic provides number chat based models Claude moniker. Note Claude Pro membership give ability call models via API; instead, need sign (pay ) developer account.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_anthropic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat_anthropic(   system_prompt = NULL,   turns = NULL,   params = NULL,   max_tokens = deprecated(),   model = NULL,   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   beta_headers = character(),   api_key = anthropic_key(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_anthropic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. params Common model parameters, usually created params(). max_tokens Maximum number tokens generate stopping. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). base_url base URL endpoint; default uses OpenAI. beta_headers Optionally, character vector beta headers opt-claude features still beta. api_key API key use authentication. generally supply directly, instead set ANTHROPIC_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_anthropic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_anthropic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat <- chat_anthropic() #> Using model = \"claude-3-7-sonnet-latest\". chat$chat(\"Tell me three jokes about statisticians\") #> # Three Jokes About Statisticians #>  #> 1. A statistician returned from lunch and announced, \"I just took part #> in a poll and now I'm 90% confident that 70% of the restaurant prefers #> their chicken 80% cooked.\" #>  #> 2. How many statisticians does it take to change a light bulb? #>    \"Well... just one, but he needs to do it 30 times to obtain a  #> significant result.\" #>  #> 3. A statistician's wife gave birth to twins. He was delighted. He  #> rang the minister who asked, \"Are they boys or girls?\" #>    \"Yes,\" replied the statistician, \"2 out of 2 of them are.\""},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an AWS bedrock model — chat_aws_bedrock","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"AWS Bedrock provides number language models, including Anthropic's Claude, using Bedrock Converse API.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Authentication handled {paws.common}, authentication work automatically, need follow advice https://www.paws-r-sdk.com/#credentials. particular, org uses AWS SSO, need run aws sso login terminal.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"chat_aws_bedrock(   system_prompt = NULL,   turns = NULL,   model = NULL,   profile = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model ellmer provides default model, typically need specify model actually access . using cross-region inference, need use inference profile ID, e.g. model=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\". profile AWS profile use. api_args Named list arbitrary extra arguments appended body every chat API call. useful arguments include:   echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":"api_args = list(   inferenceConfig = list(     maxTokens = 100,     temperature = 0.7,     topP = 0.9,     topK = 20   ) )"},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_aws_bedrock.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage chat <- chat_aws_bedrock() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"chat_azure_openai() supports API keys credentials parameter, also makes use : Azure service principals (AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET environment variables set). Interactive Entra ID authentication, like Azure CLI. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"chat_azure_openai(   endpoint = azure_endpoint(),   deployment_id,   params = NULL,   api_version = NULL,   system_prompt = NULL,   turns = NULL,   api_key = NULL,   token = deprecated(),   credentials = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT envinronment variable. deployment_id Deployment id model want use. params Common model parameters, usually created params(). api_version API version use. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. api_key API key use authentication. generally supply directly, instead set AZURE_OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). token literal Azure token use authentication. Deprecated favour ambient Azure credentials explicit credentials argument. credentials list authentication headers pass httr2::req_headers(), function returns , NULL use token api_key generate headers instead. escape hatch allows users incorporate Azure credentials generated packages ellmer, manage lifetime credentials need refreshed. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_azure_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_azure_openai(deployment_id = \"gpt-4o-mini\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Chat LLM-powered Snowflake Cortex Analyst.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"chat_cortex_analyst() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Unlike comparable model APIs, Cortex take system prompt. Instead, caller must provide \"semantic model\" describing available tables, meaning, verified queries can run starting point. semantic model can passed YAML string via reference existing file Snowflake Stage. Note Cortex support multi-turn, remember previous messages. support registering tools, attempting result error. See chat_snowflake() chat general-purpose models hosted Snowflake.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"","code":"chat_cortex_analyst(   account = snowflake_account(),   credentials = NULL,   model_spec = NULL,   model_file = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model_spec semantic model specification, NULL using model_file instead. model_file Path semantic model file stored Snowflake Stage, NULL using model_spec instead. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_cortex_analyst.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"","code":"if (FALSE) { # has_credentials(\"cortex\") chat <- chat_cortex_analyst(   model_file = \"@my_db.my_schema.my_stage/model.yaml\" ) chat$chat(\"What questions can I ask?\") }"},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Databricks — chat_databricks","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks provides ---box access number foundation models can also serve gateway external models hosted third party.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Databricks — chat_databricks","text":"chat_databricks() picks ambient Databricks credentials subset Databricks client unified authentication model. Specifically, supports: Personal access tokens Service principals via OAuth (OAuth M2M) User account via OAuth (OAuth U2M) Authentication via Databricks CLI Posit Workbench-managed credentials Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks models support images, support structured outputs. Tool calling support also limited present currently supported ellmer.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"chat_databricks(   workspace = databricks_workspace(),   system_prompt = NULL,   turns = NULL,   model = NULL,   token = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Databricks — chat_databricks","text":"workspace URL Databricks workspace, e.g. \"https://example.cloud.databricks.com\". use value environment variable DATABRICKS_HOST, set. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. Available foundational models include: databricks-dbrx-instruct (default) databricks-mixtral-8x7b-instruct databricks-meta-llama-3-1-70b-instruct databricks-meta-llama-3-1-405b-instruct token authentication token Databricks workspace, NULL use ambient credentials. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_databricks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_databricks() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on DeepSeek — chat_deepseek","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Sign https://platform.deepseek.com.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Structured data extraction supported. Images supported.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"chat_deepseek(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.deepseek.com\",   api_key = deepseek_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses DeepSeek. api_key API key use authentication. generally supply directly, instead set DEEPSEEK_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_deepseek.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_deepseek() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub (via Azure) hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub model marketplace.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   turns = NULL,   base_url = \"https://models.inference.ai.azure.com/\",   api_key = github_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead manage GitHub credentials described https://usethis.r-lib.org/articles/git-credentials.html. headless environments, also look GITHUB_PAT env var. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_github.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_github() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini model — chat_google_gemini","title":"Chat with a Google Gemini model — chat_google_gemini","text":"See gemini_upload() upload files (PDFs, images, video, audio, etc.)","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a Google Gemini model — chat_google_gemini","text":"default, chat_google_gemini() use Google's default application credentials API key provided. requires gargle package. can also pick viewer-based credentials Posit Connect. turn requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini model — chat_google_gemini","text":"","code":"chat_google_gemini(   system_prompt = NULL,   turns = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini model — chat_google_gemini","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini model — chat_google_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_google_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Google Gemini model — chat_google_gemini","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. function lightweight wrapper around chat_openai() defaults tweaked groq.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Groq — chat_groq","text":"groq currently support structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GROQ_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Groq — chat_groq","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_groq() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local Ollama model — chat_ollama","title":"Chat with a local Ollama model — chat_ollama","text":"use chat_ollama() first download install Ollama. install models either command line (e.g. ollama pull llama3.1) within R using ollamar (e.g. ollamar::pull(\"llama3.1\")). function lightweight wrapper around chat_openai() defaults tweaked ollama.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a local Ollama model — chat_ollama","text":"Tool calling supported streaming (.e. echo \"text\" \"\") Models can use 2048 input tokens, way get use , except creating custom model different default. Tool calling generally seems quite weak, least models tried .","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   turns = NULL,   base_url = \"http://localhost:11434\",   model,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local Ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a local Ollama model — chat_ollama","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_ollama(model = \"llama3.2\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"OpenAI provides number chat-based models, mostly ChatGPT brand. Note ChatGPT Plus membership grant access API. need sign developer account (pay ) developer platform.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   params = NULL,   seed = deprecated(),   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4o\". chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") #> - **Printing:**   #>   - Tibbles: When printed, tibbles display a cleaner and more concise  #> output by showing only the first 10 rows and only the columns that fit #> on the screen. This helps in easily scanning the data without  #> overwhelming the output. #>   - Data Frames: Printing a data frame outputs the entire content,  #> which can be unwieldy for large datasets. #>  #> - **Column Data Types:**   #>   - Tibbles: Preserve the data type of columns more reliably and will  #> not automatically convert strings to factors, providing more  #> consistent data handling. #>   - Data Frames: By default, data frames convert character strings to  #> factors which can lead to unexpected behavior without explicit control #> over data types. #>  #> - **Subsetting:** #>   - Tibbles: Subsetting with `[` always returns a tibble, regardless  #> of the number of rows or columns. They also support non-standard  #> evaluation, which allows for more intuitive subsetting using column  #> names directly. #>   - Data Frames: Subsetting a single column with `[` returns a vector  #> instead of a data frame unless explicitly set otherwise. #>  #> - **Support for Non-Syntactic Names:**   #>   - Tibbles: Can have column names that are not syntactically valid R  #> names (e.g., names with spaces or special characters), and these names #> can still be easily accessed with backticks. #>   - Data Frames: Typically require syntactically valid names or  #> special handling to access such names. #>  #> - **Package Dependency:** #>   - Tibbles: Provided by the `tibble` package and are part of the  #> tidyverse suite of packages, which promote modern data science  #> practices. #>   - Data Frames: Built-in R data structure that does not require  #> additional packages to use, though may benefit from complementary  #> packages for enhanced functionality. #>  #> - **Performance:** #>   - Tibbles: Often optimized for performance in large-scale data  #> manipulation through integration with other tidyverse packages like  #> `dplyr`. #>   - Data Frames: Basic data handling, without built-in optimizations  #> for larger datasets beyond what base R functions provide.  chat$chat(\"Tell me three funny jokes about statisticians\") #> Certainly! Here are three light-hearted jokes about statisticians: #>  #> 1. **Statistically Speaking:** #>    - Why did the statistician bring a ladder to the bar? #>    - Because they heard the drinks were on the house, and they never  #> miss a high point! #>  #> 2. **Sampling Error:** #>    - Why don't statisticians play hide and seek? #>    - Because good luck hiding when you're constantly being sampled! #>  #> 3. **Copycat:** #>    - A statistician and a researcher were asked who would likely live  #> longer. The statistician replied, \"That's easy, the person who uses  #> statistical models!\" #>    - The researcher asked, \"Why's that?\" #>    - The statistician grinned and said, \"Because statistics show they  #> always have more good data!\""},{"path":"https://ellmer.tidyverse.org/reference/chat_openrouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Sign https://openrouter.ai. Support features depends underlying model use; see https://openrouter.ai/models details.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openrouter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"chat_openrouter(   system_prompt = NULL,   turns = NULL,   api_key = openrouter_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_openrouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. api_key API key use authentication. generally supply directly, instead set OPENROUTER_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_openrouter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_openrouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openrouter() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. Perplexity AI platform running LLMs capable searching web real-time help answer questions information may available model trained. function lightweight wrapper around chat_openai() defaults tweaked Perplexity AI.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   turns = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PERPLEXITY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/chat_perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_perplexity() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Snowflake — chat_snowflake","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Snowflake provider allows interact LLM models available Cortex LLM REST API.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"chat_snowflake() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Note Snowflake-hosted models support images, tool calling, structured outputs. See chat_cortex_analyst() chat Snowflake Cortex Analyst rather general-purpose model.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"chat_snowflake(   system_prompt = NULL,   turns = NULL,   account = snowflake_account(),   credentials = NULL,   model = NULL,   api_args = list(),   echo = c(\"none\", \"text\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_snowflake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"if (FALSE) { # has_credentials(\"cortex\") chat <- chat_snowflake() chat$chat(\"Tell me a joke in the form of a SQL query.\") }"},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted by vLLM — chat_vllm","title":"Chat with a model hosted by vLLM — chat_vllm","text":"vLLM open source library provides efficient convenient LLMs model server. can use chat_vllm() connect endpoints powered vLLM.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"chat_vllm(   base_url,   system_prompt = NULL,   turns = NULL,   model,   seed = NULL,   api_args = list(),   api_key = vllm_key(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted by vLLM — chat_vllm","text":"base_url base URL endpoint; default uses OpenAI. system_prompt system prompt set behavior assistant. turns list Turns start chat (.e., continuing previous conversation). provided, conversation begins scratch. model model use chat. default, NULL, pick reasonable default, tell . strongly recommend explicitly choosing model casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_key API key use authentication. generally supply directly, instead set VLLM_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). echo One following options: none: emit output (default running function). text: echo text output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted by vLLM — chat_vllm","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/chat_vllm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_vllm(\"http://my-vllm.com\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode images for chat input — content_image_url","title":"Encode images for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode images for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode images for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode images for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode images for chat input — content_image_url","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"ellmer\")) ) #> The first image is the logo for R, which is a programming language and #> environment used for statistical computing and graphics. #>  #> The second image is a hex sticker design featuring the text \"httr2\"  #> with an illustration of a baseball player swinging a bat. This likely  #> represents an R package related to HTTP communications, as httr2 is a  #> package for working with HTTP in R.  plot(waiting ~ eruptions, data = faithful)  chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4o\". chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) #> The plot is a line chart depicting four lines with varying patterns.  #> The x-axis is labeled \"Number of Steps,\" and the y-axis is labeled  #> \"Loss.\" Each line represents a different data set: \"Training loss\" in  #> solid blue, \"Val loss\" in solid orange, \"Training accuracy\" in dashed  #> blue, and \"Val accuracy\" in dashed orange. The \"Training loss\" line  #> shows a sharp decrease initially before leveling out, while the \"Val  #> loss\" line decreases at a slower rate and appears more erratic. The  #> \"Training accuracy\" line increases steadily after a gradual start,  #> reaching a high point, whereas the \"Val accuracy\" line also rises but  #> with more fluctuations. Overall, the primary patterns indicate an  #> improvement in both loss and accuracy over time, albeit with more  #> variability in the validation data."},{"path":"https://ellmer.tidyverse.org/reference/content_pdf_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode PDFs content for chat input — content_pdf_file","title":"Encode PDFs content for chat input — content_pdf_file","text":"functions used prepare PDFs input chatbot. content_pdf_url() function used provide URL PDF file, content_pdf_file() used local PDF files. providers support PDF input, check documentation provider using.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_pdf_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode PDFs content for chat input — content_pdf_file","text":"","code":"content_pdf_file(path)  content_pdf_url(url)"},{"path":"https://ellmer.tidyverse.org/reference/content_pdf_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode PDFs content for chat input — content_pdf_file","text":"path, url Path URL PDF file.","code":""},{"path":"https://ellmer.tidyverse.org/reference/content_pdf_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode PDFs content for chat input — content_pdf_file","text":"ContentPDF object","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Format contents into a textual representation — contents_text","title":"Format contents into a textual representation — contents_text","text":"generic functions can use convert Turn contents Content objects textual representations. contents_text() minimal includes ContentText objects output. contents_markdown() returns text content (assumes markdown convert ) plus markdown representations images content types. contents_html() returns text content, converted markdown HTML commonmark::markdown_html(), plus HTML representations images content types.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format contents into a textual representation — contents_text","text":"","code":"contents_text(content, ...)  contents_html(content, ...)  contents_markdown(content, ...)"},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format contents into a textual representation — contents_text","text":"content Turn Content object converted text. contents_markdown() also accepts Chat instances turn entire conversation history markdown text. ... Additional arguments passed methods.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format contents into a textual representation — contents_text","text":"string text, markdown HTML.","code":""},{"path":"https://ellmer.tidyverse.org/reference/contents_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format contents into a textual representation — contents_text","text":"","code":"turns <- list(   Turn(\"user\", contents = list(     ContentText(\"What's this image?\"),     content_image_url(\"https://placehold.co/200x200\")   )),   Turn(\"assistant\", \"It's a placeholder image.\") )  lapply(turns, contents_text) #> [[1]] #> [1] \"What's this image?\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  lapply(turns, contents_markdown) #> [[1]] #> [1] \"What's this image?\\n\\n![](https://placehold.co/200x200)\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  if (rlang::is_installed(\"commonmark\")) {   contents_html(turns[[1]]) } #> [1] \"<p>What's this image?<\/p>\\n\\n<img src=\\\"https://placehold.co/200x200\\\">\""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right call tool(). function helps documented functions extracting function's R documentation creating tool() call , using LLM. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful Roxygen comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(   topic,   chat = NULL,   model = deprecated(),   echo = interactive(),   verbose = FALSE )"},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. chat Chat object used generate output. NULL (default) uses chat_openai(). model lifecycle::badge(\"deprecated\") Formally used definining model used chat. Now supply chat instead. echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://ellmer.tidyverse.org/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\")   create_tool_def(\"rnorm\", chat = chat_azure_openai()) } # }"},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/deprecated.html","id":"deprecated-in-v-","dir":"Reference","previous_headings":"","what":"Deprecated in v0.2.0","title":"Deprecated functions — deprecated","text":"chat_azure() renamed chat_azure_openai(). chat_bedrock() renamed chat_aws_bedrock(). chat_claude() renamed chat_anthropic(). chat_gemini() renamed chat_google_gemini().","code":""},{"path":"https://ellmer.tidyverse.org/reference/deprecated.html","id":"deprecated-in-v--1","dir":"Reference","previous_headings":"","what":"Deprecated in v0.1.1","title":"Deprecated functions — deprecated","text":"chat_cortex() renamed v0.1.1 chat_cortex_analyst() distinguish general-purpose Snowflake Cortex chat function, chat_snowflake().","code":""},{"path":"https://ellmer.tidyverse.org/reference/deprecated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated functions — deprecated","text":"","code":"chat_cortex(...)  chat_azure(...)  chat_bedrock(...)  chat_claude(...)  chat_gemini(...)"},{"path":"https://ellmer.tidyverse.org/reference/deprecated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated functions — deprecated","text":"... Additional arguments passed deprecated function replacement.","code":""},{"path":"https://ellmer.tidyverse.org/reference/ellmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ellmer: Chat with Large Language Models — ellmer-package","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Chat large language models range providers including 'Claude' https://claude.ai, 'OpenAI' https://chatgpt.com, . Supports streaming, asynchronous calls, tool calling, structured data extraction.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/reference/ellmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co (ORCID) Authors: Joe Cheng Aaron Jacobs contributors: Posit Software, PBC (03wc8by49) [copyright holder, funder]","code":""},{"path":"https://ellmer.tidyverse.org/reference/gemini_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file to gemini — gemini_upload","title":"Upload a file to gemini — gemini_upload","text":"function uploads file waits Gemini finish processing can immediately use prompt. experimental currently Gemini specific, expect providers evolve similar feature future. Uploaded files automatically deleted 2 days. file must less 2 GB can upload total 20 GB. ellmer currently provide way delete files early; please file issue useful .","code":""},{"path":"https://ellmer.tidyverse.org/reference/gemini_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file to gemini — gemini_upload","text":"","code":"gemini_upload(   path,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   mime_type = NULL )"},{"path":"https://ellmer.tidyverse.org/reference/gemini_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file to gemini — gemini_upload","text":"path Path file upload. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). mime_type Optionally, specify mime type file. specified, guesses file extension.","code":""},{"path":"https://ellmer.tidyverse.org/reference/gemini_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a file to gemini — gemini_upload","text":"<ContentUploaded> object can passed $chat().","code":""},{"path":"https://ellmer.tidyverse.org/reference/gemini_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a file to gemini — gemini_upload","text":"","code":"if (FALSE) { # \\dontrun{ file <- gemini_upload(\"path/to/file.pdf\")  chat <- chat_openai() chat$chat(file, \"Give me a three paragraph summary of this PDF\") } # }"},{"path":"https://ellmer.tidyverse.org/reference/has_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Are credentials avaiable? — has_credentials","title":"Are credentials avaiable? — has_credentials","text":"Used examples/testing.","code":""},{"path":"https://ellmer.tidyverse.org/reference/has_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Are credentials avaiable? — has_credentials","text":"","code":"has_credentials(provider)"},{"path":"https://ellmer.tidyverse.org/reference/has_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Are credentials avaiable? — has_credentials","text":"provider Provider name.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt: interpolate() works string. interpolate_file() works file. interpolate_package() works file insts/prompt directory package. Compared glue, dynamic values wrapped {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())  interpolate_package(package, path, ..., .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md). package Package name.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helpers for interpolating data into prompts — interpolate","text":"{glue} string.","code":""},{"path":"https://ellmer.tidyverse.org/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://ellmer.tidyverse.org/reference/live_console.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a live chat application — live_console","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_anthropic() live_console(chat) live_browser(chat) } # }"},{"path":"https://ellmer.tidyverse.org/reference/params.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard model parameters — params","title":"Standard model parameters — params","text":"helper function makes easier create list parameters used across many models. parameter names automatically standardised included correctly place API call. Note parameters supported given provider generate warning, error. allows use set parameters across multiple providers.","code":""},{"path":"https://ellmer.tidyverse.org/reference/params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard model parameters — params","text":"","code":"params(   temperature = NULL,   top_p = NULL,   top_k = NULL,   frequency_penalty = NULL,   presence_penalty = NULL,   seed = NULL,   max_tokens = NULL,   log_probs = NULL,   stop_sequences = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/reference/params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard model parameters — params","text":"temperature Temperature sampling distribution. top_p cumulative probability token selection. top_k number highest probability vocabulary tokens keep. frequency_penalty Frequency penalty generated tokens. presence_penalty Presence penalty generated tokens. seed Seed random number generator. max_tokens Maximum number tokens generate. log_probs Include log probabilities output? stop_sequences character vector tokens stop generation . ... Additional named parameters send provider.","code":""},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session. price shown known.","code":""},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Report on token usage in the current session — token_usage","text":"data frame","code":""},{"path":"https://ellmer.tidyverse.org/reference/token_usage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage() #>    provider             model input output price #> 1    OpenAI            gpt-4o  1840    837 $0.01 #> 2 Anthropic claude-3-7-sonnet    14    151    NA"},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — tool","title":"Define a tool — tool","text":"Define R function use chatbot. function always run current R instance. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — tool","text":"","code":"tool(.fun, .description, ..., .name = NULL, .annotations = list())"},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — tool","text":".fun function invoked tool called. return value function sent back chatbot. Expert users can customize tool result returning ContentToolResult object. .description detailed description function . Generally, information can provide , better. ... Name-type pairs define arguments accepted function. element created type_*() function. .name name function. .annotations Additional properties describe tool behavior. Usually created tool_annotations(), can find description annotation properties recommended Model Context Protocol.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a tool — tool","text":"S7 ToolDef object.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — tool","text":"","code":"# First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- tool(   rnorm,   \"Drawn numbers from a random normal distribution\",   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\") ) chat <- chat_openai() #> Using model = \"gpt-4o\". # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"   Give me five numbers from a random normal distribution. \") #> To provide numbers from a random normal distribution, I'll need the  #> mean and standard deviation you would like to use for the  #> distribution. Could you please specify these values?  # Look at the chat history to see how tool calling works: # Assistant sends a tool request which is evaluated locally and # results are send back in a tool result."},{"path":"https://ellmer.tidyverse.org/reference/tool_annotations.html","id":null,"dir":"Reference","previous_headings":"","what":"Tool annotations — tool_annotations","title":"Tool annotations — tool_annotations","text":"Tool annotations additional properties can used describe tool clients, example Shiny app another user interface. annotations tool_annotations() drawn Model Context Protocol considered hints. Tool authors use annotations communicate tool properties, users note annotations guaranteed.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool_annotations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tool annotations — tool_annotations","text":"","code":"tool_annotations(   title = NULL,   read_only_hint = NULL,   open_world_hint = NULL,   idempotent_hint = NULL,   destructive_hint = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/reference/tool_annotations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tool annotations — tool_annotations","text":"title human-readable title tool. read_only_hint TRUE, tool modify environment. open_world_hint TRUE, tool may interact \"open world\" external entities. FALSE, tool's domain interaction closed. example, world web search tool open, world memory tool . idempotent_hint TRUE, calling tool repeatedly arguments additional effect environment. (meaningful read_only_hint FALSE.) destructive_hint TRUE, tool may perform destructive updates environment, otherwise performs additive updates. (meaningful read_only_hint FALSE.) ... Additional named parameters include tool annotations.","code":""},{"path":"https://ellmer.tidyverse.org/reference/tool_annotations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tool annotations — tool_annotations","text":"list tool annotations.","code":""},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Type specifications — type_boolean","title":"Type specifications — type_boolean","text":"functions specify object types way chatbots understand used tool calling structured data extraction. names based JSON schema, APIs expect behind scenes. translation R concepts types fairly straightforward. type_boolean(), type_integer(), type_number(), type_string() represent scalars. equivalent length-1 logical, integer, double, character vectors (respectively). type_enum() equivalent length-1 factor; string can take specified values. type_array() equivalent vector R. can use represent atomic vector: e.g. type_array(items = type_boolean()) equivalent logical vector type_array(items = type_string()) equivalent character vector). can also use represent list complicated types every element type (R base equivalent ), e.g. type_array(items = type_array(items = type_string())) represents list character vectors. type_object() equivalent named list R, every element must specified type. example, type_object(= type_string(), b = type_array(type_integer())) equivalent list element called string element called b integer vector.","code":""},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type specifications — type_boolean","text":"","code":"type_boolean(description = NULL, required = TRUE)  type_integer(description = NULL, required = TRUE)  type_number(description = NULL, required = TRUE)  type_string(description = NULL, required = TRUE)  type_enum(description = NULL, values, required = TRUE)  type_array(description = NULL, items, required = TRUE)  type_object(   .description = NULL,   ...,   .required = TRUE,   .additional_properties = FALSE )"},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type specifications — type_boolean","text":"description, .description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required, .required component required? FALSE, component exist data, LLM may hallucinate value. applies element nested inside type_object(). values Character vector permitted values. items type array items. Can created type_ function. ... Name-type pairs defineing components object must possess. .additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/reference/type_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type specifications — type_boolean","text":"","code":"# An integer vector type_array(items = type_integer()) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"integer\"  # The closest equivalent to a data frame is an array of objects type_array(items = type_object(    x = type_boolean(),    y = type_string(),    z = type_number() )) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeObject> #>  .. @ description          : NULL #>  .. @ required             : logi TRUE #>  .. @ properties           :List of 3 #>  .. .. $ x: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"boolean\" #>  .. .. $ y: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"string\" #>  .. .. $ z: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"number\" #>  .. @ additional_properties: logi FALSE  # There's no specific type for dates, but you use a string with the # requested format in the description (it's not gauranteed that you'll # get this format back, but you should most of the time) type_string(\"The creation date, in YYYY-MM-DD format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The creation date, in YYYY-MM-DD format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\" type_string(\"The update date, in dd/mm/yyyy format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The update date, in dd/mm/yyyy format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"ellmer-development-version","dir":"Changelog","previous_headings":"","what":"ellmer (development version)","title":"ellmer (development version)","text":"ContentToolResult objects can now returned directly tool() function now includes additional information (#398 #399, @gadenbuie): extra: list additional data associated tool result shown chatbot. request: ContentToolRequest triggered tool call. ContentToolResult longer id property, instead tool call ID can retrieved request@id. ContentToolRequest gains tool property includes tool() definition request matched tool ellmer (#423, @gadenbuie). ellmer now tracks cost input output tokens. cost displayed print Chat object, tokens_usage(), Chat$get_cost(). best effort computing cost, treat estimate rather exact price. Unfortunately LLM APIs currently make hard figure exactly much queries costing (#203). ContentToolResult objects now include error condition error property tool call fails (#421, @gadenbuie). Several chat functions renamed better align companies providing API (#382, @gadenbuie): chat_azure_openai() replaces chat_azure() chat_aws_bedrock() replaces chat_bedrock() chat_anthropic() replaces chat_claude() chat_google_gemini() replaces chat_gemini() chat_claude() now supports thinking content type (#396). tool() gains .annotations argument can created tool_annotations() helper. Tool annotations described Model Context Protocol can used describe tool clients. (#402, @gadenbuie) Provider gains name model fields (#406). now reported print chat object used token_usage(). New interpolate_package() make easier interpolate prompts stored inst/prompts inside package (#164). chat_azure(), chat_claude(), chat_openai(), chat_gemini() now params argument allows specify common model paramaters (like seed temperature). Support models grow request (#280). chat_claude(max_tokens =) now deprecated favour chat_claude(params = ) (#280). chat_openai(seed =) now deprecated favour chat_openai(params = ) (#280). Chat$get_provider() lets access underlying provider object, needed (#202). $extract_data() now works better arrays required = FALSE (#384). chat_claude() chat_bedrock() longer choke receiving output consists whitespace (#376). live_browser() now initializes shinychat::chat_ui() messages chat turns, rather replaying turns server-side (#381). Chat$tokens() now returns data frame tokens, correctly aligned individual turn. print method now uses show many input/output tokens turn used (#354). requests now set custom User-Agent identifies requests comes ellmer (#341). provider_claude() now supports content_image_url() (#347). chat_claude() gains beta_header argument opt-beta features (#339). chat_claude() now supports content_image_url() (#347). chat_claude() now defaults Sonnet 3.7 displays default model (#336). Turn objects now include POSIXct timestamp completed slot records turn completed (#337, @simonpcouch). create_tool_def() can now use Chat instance (#118, @pedrobtz). New experimental $chat_parallel() $extract_data_parallel() make easier perform multiple actions parallel (#143). experimental ’m 100% sure shape user interface correct, particularly pertains handling errors. Claude, note number active connections limited primarily output tokens per limit (OTPM) estimated max_tokens parameter, defaults 4096. means ’re limited 16,000 OPTM, use 16,000 / 4096 = ~4 active connections (decrease max_tokens). Parallel calls OpenAI Gemini much simpler experience. gemini_upload() lets upload files Gemini (#310). chat_gemini() can now authenticate Google default application credentials (including service accounts, etc). requires gargle package (#317, @atheriel). chat_gemini() now detects viewer-based credentials running Posit Connect (#320, @atheriel). chat_ollama() now works tool() definitions optional arguments empty properties (#342, #348, @gadenbuie).","code":""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"ellmer-011","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.1","title":"ellmer 0.1.1","text":"CRAN release: 2025-02-06","code":""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"lifecycle-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"ellmer 0.1.1","text":"option(ellmer_verbosity) longer supported; instead use standard httr2 verbosity functions, httr2::with_verbosity(); now support streaming data. chat_cortex() renamed chat_cortex_analyst() better disambiguate chat_snowflake() (also uses “Cortex”) (#275, @atheriel).","code":""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.1.1","text":"providers now wait 60s get complete response. can increase , e.g., option(ellmer_timeout_s = 120) (#213, #300). chat_azure(), chat_databricks(), chat_snowflake(), chat_cortex_analyst() now detect viewer-based credentials running Posit Connect (#285, @atheriel). chat_deepseek() provides support DeepSeek models (#242). chat_openrouter() provides support models hosted OpenRouter (#212). chat_snowflake() allows chatting models hosted Snowflake’s Cortex LLM REST API (#258, @atheriel). content_pdf_file() content_pdf_url() allow upload PDFs supported models. Models currently support PDFs Google Gemini Claude Anthropic. help @walkerke @andrie (#265).","code":""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.1.1","text":"Chat$get_model() returns model name (#299). chat_azure() greatly improved support Azure Entra ID. API keys now optional can pick ambient credentials Azure service principals attempt use interactive Entra ID authentication possible. broken--design token argument deprecated (handle refreshing tokens properly), new credentials argument can used custom Entra ID support needed instead (instance, ’re trying use tokens generated AzureAuth package) (#248, #263, #273, #257, @atheriel). chat_azure() now reports better error messages underlying HTTP requests fail (#269, @atheriel). now also defaults api_version = \"2024-10-21\" includes data structured data extraction (#271). chat_bedrock() now handles temporary IAM credentials better (#261, @atheriel) chat_bedrock() gains api_args argument (@billsanto, #295). chat_databricks() now handles DATABRICKS_HOST environment variable correctly whether includes HTTPS prefix (#252, @atheriel). also respects SPARK_CONNECT_USER_AGENT environment variable making requests (#254, @atheriel). chat_gemini() now defaults using gemini-2.0-flash model. print(Chat) longer wraps long lines, making easier read code bulleted lists (#246).","code":""},{"path":"https://ellmer.tidyverse.org/news/index.html","id":"ellmer-010","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.0","title":"ellmer 0.1.0","text":"CRAN release: 2025-01-09 New chat_vllm() chat models served vLLM (#140). default chat_openai() model now GPT-4o. New Chat$set_turns() set turns. Chat$turns() now Chat$get_turns(). Chat$system_prompt() replaced Chat$set_system_prompt() Chat$get_system_prompt(). Async streaming async chat now event-driven use later::later_fd() wait efficiently curl socket activity (#157). New chat_bedrock() chat AWS bedrock models (#50). New chat$extract_data() uses structured data API available (tool calling otherwise) extract data structured according known type specification. can create specs functions type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object() (#31). general ToolArg() replaced specific type_*() functions. ToolDef() renamed tool. content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprobs (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. can now log low-level JSON request/response info setting options(ellmer_verbosity = 2). chat$register_tool() now takes object created Tool(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56). Databricks now supported via chat_databricks() (#152).","code":""}]
