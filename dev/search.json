[{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"package-overview","dir":"","previous_headings":"","what":"Package Overview","title":"CLAUDE.md","text":"ellmer R package provides unified interface multiple Large Language Model (LLM) providers. supports features like streaming outputs, tool/function calling, structured data extraction, asynchronous processing.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"testing","dir":"","previous_headings":"Development Commands","what":"Testing","title":"CLAUDE.md","text":"R CMD check - Full package check (used CI) testthat::test_check(\"ellmer\") - Run tests via testthat devtools::test() - Run tests interactively development Tests use VCR cassettes HTTP mocking (located tests/testthat/_vcr/) Test configuration includes parallel execution specific test ordering","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"building-and-documentation","dir":"","previous_headings":"Development Commands","what":"Building and Documentation","title":"CLAUDE.md","text":"devtools::document() - Generate documentation roxygen2 comments pkgdown::build_site() - Build package website devtools::build() - Build package tarball devtools::install() - Install package locally development","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"package-structure","dir":"","previous_headings":"Development Commands","what":"Package Structure","title":"CLAUDE.md","text":"Uses standard R package structure DESCRIPTION, NAMESPACE, man/ directories Source code organized R/ directory provider-specific files Vignettes vignettes/ directory demonstrate key features Tests tests/testthat/ snapshot testing enabled","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"core-components","dir":"","previous_headings":"Architecture","what":"Core Components","title":"CLAUDE.md","text":"Chat Objects: Central abstraction using R6 classes maintain conversation state - Chat - Main chat interface provider-agnostic methods - Provider - Abstract base class different LLM providers - Turn - Represents conversation turns user/assistant messages - Content - Handles different content types (text, images, PDFs, tool calls) Provider System: Modular architecture supporting 15+ LLM providers - provider separate R file (provider-*.R) - Common interface abstracts provider differences - Authentication handled per-provider (API keys, OAuth, IAM) Tool System: Function calling capabilities - tools-def.R - Tool definition framework - tools-def-auto.R - Automatic tool definition generation - chat-tools.R - Tool execution management Content Types: Rich content support - content-image.R - Image handling (files, URLs, plots) - content-pdf.R - PDF document processing - content-replay.R - Conversation replay functionality Parallel Processing: Asynchronous batch operations - parallel-chat.R - Parallel chat execution - batch-chat.R - Batch processing capabilities - Uses coro package async operations","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"key-design-patterns","dir":"","previous_headings":"Architecture","what":"Key Design Patterns","title":"CLAUDE.md","text":"S7 Type System: Uses S7 structured data types types.R - Type definitions tool parameters structured outputs - Runtime type checking validation Standalone Imports: Self-contained utility functions - import-standalone-*.R files reduce dependencies - Imported tidyverse packages Provider Plugin Architecture: provider implements common interface - provider.R defines base Provider class - Provider-specific files extend base functionality - Authentication request handling per provider","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"core-implementation","dir":"","previous_headings":"Key Files","what":"Core Implementation","title":"CLAUDE.md","text":"R/chat.R - Main Chat class implementation R/provider.R - Base Provider class interface R/types.R - S7 type definitions structured data R/content.R - Content handling framework","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"provider-implementations","dir":"","previous_headings":"Key Files","what":"Provider Implementations","title":"CLAUDE.md","text":"R/provider-openai.R - OpenAI/GPT integration R/provider-anthropic.R - Anthropic/Claude integration R/provider-google.R - Google Gemini integration Additional providers AWS, Azure, Ollama, etc.","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"features","dir":"","previous_headings":"Key Files","what":"Features","title":"CLAUDE.md","text":"R/chat-structured.R - Structured data extraction R/chat-tools.R - Tool/function calling R/live.R - Interactive chat interfaces R/interpolate.R - Template/prompt interpolation","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"testing-and-quality","dir":"","previous_headings":"Key Files","what":"Testing and Quality","title":"CLAUDE.md","text":"tests/testthat/ - Test suite VCR cassettes vignettes/ - Documentation examples .github/workflows/ - CI/CD R CMD check","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"testing-strategy","dir":"","previous_headings":"Development Notes","what":"Testing Strategy","title":"CLAUDE.md","text":"Uses VCR HTTP request mocking avoid live API calls Parallel test execution configured DESCRIPTION Snapshot testing output validation Separate test files major component","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"code-organization","dir":"","previous_headings":"Development Notes","what":"Code Organization","title":"CLAUDE.md","text":"Collate field DESCRIPTION defines file loading order Provider files follow consistent naming pattern Utility functions grouped purpose (utils-*.R) Standalone imports minimize external dependencies","code":""},{"path":"https://ellmer.tidyverse.org/dev/CLAUDE.html","id":"documentation","dir":"","previous_headings":"Development Notes","what":"Documentation","title":"CLAUDE.md","text":"Roxygen2 comments exported functions Vignettes demonstrate key use cases pkgdown site provides comprehensive documentation Examples use realistic safe API interactions","code":""},{"path":"https://ellmer.tidyverse.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ellmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"vocabulary","dir":"Articles","previous_headings":"","what":"Vocabulary","title":"Getting started with ellmer","text":"’ll start laying key vocab ’ll need understand LLMs. Unfortunately vocab little entangled: understand one term ’ll often know little others. ’ll start simple definitions important terms iteratively go little deeper. starts prompt, text (typically question request) send LLM. starts conversation, sequence turns alternate user prompts model responses. Inside model, prompt response represented sequence tokens, represent either individual words subcomponents word. tokens used compute cost using model measure size context, combination current prompt previous prompts responses used generate next response. ’s useful make distinction providers models. provider web API gives access one models. distinction bit subtle providers often synonymous model, like OpenAI GPT, Anthropic Claude, Google Gemini. providers, like Ollama, can host many different models, typically open source models like LLaMa Mistral. Still providers support open closed models, typically partnering company provides popular closed model. example, Azure OpenAI offers open source models OpenAI’s GPT, AWS Bedrock offers open source models Anthropic’s Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-token","dir":"Articles","previous_headings":"Vocabulary","what":"What is a token?","title":"Getting started with ellmer","text":"LLM model, like models needs way represent inputs numerically. LLMs, means need way convert words numbers. goal tokenizer. example, using GPT 4o tokenizer, string “R created?” converted 5 tokens: 5958 (“”), 673 (” ”), 460 (” R”), 5371 (” created”), 30 (“?”). can see, many simple strings can represented single token. complex strings require multiple tokens. example, string “counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”), 9477 (“volution”), 815 (“ary”). (can see various strings tokenized http://tiktokenizer.vercel.app/). ’s important rough sense text converted tokens tokens used determine cost model much context can used predict next response. average English word needs ~1.5 tokens page might require 375-400 tokens complete book might require 75,000 150,000 tokens. languages typically require tokens, (brief) LLMs trained data internet, primarily English. LLMs priced per million tokens. State art models (like GPT-4.1 Claude 3.5 sonnet) cost $2-3 per million input tokens, $10-15 per million output tokens. Cheaper models can cost much less, e.g. GPT-4.1 nano costs $0.10 per million input tokens $0.40 per million output tokens. Even $10 API credit give lot room experimentation, particularly cheaper models, prices likely decline model performance improves. Tokens also used measure context window, much text LLM can use generate next response. ’ll discuss shortly, context length includes full state conversation far (prompts model’s responses), means cost grow rapidly number conversational turns. ellmer, can see many tokens conversations used printing , can see total usage session token_usage(). want learn tokens tokenizers, ’d recommend watching first 20-30 minutes Let’s build GPT Tokenizer Andrej Karpathy. certainly don’t need learn build tokenizer, intro give bunch useful background knowledge help improve undersstanding LLM’s work.","code":"chat <- chat_openai(model = \"gpt-4.1\") . <- chat$chat(\"Who created R?\", echo = FALSE) chat #> <Chat OpenAI/gpt-4.1 turns=2 tokens=11/77 $0.00> #> ── user [11] ────────────────────────────────────────────────────────── #> Who created R? #> ── assistant [77] ───────────────────────────────────────────────────── #> R was originally created by **Ross Ihaka** and **Robert Gentleman** at the University of Auckland, New Zealand, in the early 1990s. They began developing R as a free, open-source implementation of the S programming language for statistical computing and graphics. The project quickly grew, and today it is maintained by the R Core Team and the contributions of many users worldwide.  token_usage() #>   provider   model input output cached_input price #> 1   OpenAI gpt-4.1    11     77            0 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-conversation","dir":"Articles","previous_headings":"Vocabulary","what":"What is a conversation?","title":"Getting started with ellmer","text":"conversation LLM takes place series HTTP requests responses: send question LLM HTTP request, sends back reply HTTP response. words, conversation consists sequence paired turns: sent prompt returned response. ’s important note request includes current user prompt, every previous user prompt model response. means : cost conversation grows quadratically number turns: want save money, keep conversations short. response affected previous prompts responses. can make conversation get stuck local optimum, ’s generally better iterate starting new conversation better prompt rather long back--forth. ellmer full control conversational history. ’s ellmer’s responsibility send previous turns conversation, ’s possible start conversation one model finish another.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-prompt","dir":"Articles","previous_headings":"Vocabulary","what":"What is a prompt?","title":"Getting started with ellmer","text":"user prompt question send model. two important prompts underlie user prompt: platform prompt, unchangeable, set model provider, affects every conversation. can see look like Anthropic, publishes core system prompts. system prompt (aka developer prompt), set create new conversation, affects every response. ’s used provide additional instructions model, shaping responses needs. example, might use system prompt ask model always respond Spanish write dependency-free base R code. can also use system prompt provide model information wouldn’t otherwise know, like details database schema, preferred ggplot2 theme color palette. OpenAI calls chain command: conflicts inconsistencies prompts, platform prompt overrides system prompt, turn overrides user prompt. use chat app like ChatGPT Claude.ai can iterate user prompt. ’re programming LLMs, ’ll primarily iterate system prompt. example, ’re developing app helps user write tidyverse code, ’d work system prompt ensure user gets style code want. Writing good prompt, called prompt design, key effective use LLMs. discussed detail vignette(\"prompt-design\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with ellmer","text":"Now ’ve got basic vocab belt, ’m going fire bunch interesting potential use cases . special purpose tools might solve cases faster /cheaper, LLM allows rapidly prototype solution. can extremely valuable even end using specialised tools final product. general, recommend avoiding LLMs accuracy critical. said, still many cases use. example, even though always require manual fiddling, might save bunch time ever 80% correct solution. fact, even --good solution can still useful makes easier get started: ’s easier react something rather start scratch blank page.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"chatbots","dir":"Articles","previous_headings":"Example uses","what":"Chatbots","title":"Getting started with ellmer","text":"great place start ellmer LLMs build chatbot custom prompt. Chatbots familiar interface LLMs easy create R shinychat. ’s surprising amount value creating custom chatbot prompt stuffed useful knowledge. example: Help people use new package. , need custom prompt LLMs trained data prior package’s existence. can create surprisingly useful tool just preloading prompt README vignettes. ellmer assistant works. Build language specific prompts R /Python. Shiny Assistant helps build shiny apps (either R Python) combining prompt gives general advice building apps prompt R python. Python prompt detailed ’s much less information Shiny Python existing LLM knowledgebases. Help people find answers questions. Even ’ve written bunch documentation something, might find still get questions folks can’t easily find exactly ’re looking . can reduce need answer questions creating chatbot prompt contains documentation. example, ’re teacher, create chatbot includes syllabus prompt. eliminates common class question data necessary answer question available, hard find. Another direction give chatbot additional context current environment. example, aidea allows user interactively explore dataset help LLM. adds summary statistics dataset prompt LLM knows something data. Along lines, imagine writing chatbot help data import prompt include files current directory along first lines.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"structured-data-extraction","dir":"Articles","previous_headings":"Example uses","what":"Structured data extraction","title":"Getting started with ellmer","text":"LLMs often good extracting structured data unstructured text. can give traction analyse data previously unaccessible. example: Customer tickets GitHub issues: can use LLMs quick dirty sentiment analysis extracting specifically mentioned products summarising discussion bullet points. Geocoding: LLMs surprisingly good job geocoding, especially extracting addresses finding latitute/longitude cities. specialised tools better, using LLM makes easy get started. Recipes: ’ve extracted structured data baking cocktail recipes. data structured form can use R skills better understand recipes vary within cookbook look recipes use ingredients currently kitchen. even use shiny assistant help make techniques available anyone, just R users. Structured data extraction also works well images. ’s fastest cheapest way extract data makes really easy prototype ideas. example, maybe bunch scanned documents want index. can convert PDFs images (e.g. using {imagemagick}) use structured data extraction pull key details. Learn structured data extraction vignette(\"structure-data\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"programming","dir":"Articles","previous_headings":"Example uses","what":"Programming","title":"Getting started with ellmer","text":"LLMs can also useful solve general programming problems. example: Write detailed prompt explains update code use new version package. combine rstudioapi package allow user select code, transform , replace existing text. comprehensive example sort app chores, includes prompts automatically generating roxygen documentation blocks, updating testthat code 3rd edition, converting stop() abort() use cli::cli_abort(). automatically look documentation R function, include prompt make easier figure use specific function. can use LLMs explain code, even ask generate diagram. can ask LLM analyse code potential code smells security issues. can function time, explore entire source code package script prompt. use gh find unlabelled issues, extract text, ask LLM figure labels might appropriate. maybe LLM might able help people create better reprexes, simplify reprexes complicated? find useful LLM document function , even knowing ’s likely mostly incorrect. something react make much easier get started. ’re working code data another programming language, can ask LLM convert R code . Even ’s perfect, ’s still typically much faster everything .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"miscellaneous","dir":"Articles","previous_headings":"","what":"Miscellaneous","title":"Getting started with ellmer","text":"finish ideas seem cool didn’t seem fit categories: Automatically generate alt text plots, using content_image_plot(). Analyse text statistical report look flaws statistical reasoning (e.g. misinterpreting p-values assuming causation correlation exists). Use existing company style guide generate brand.yaml specification automatically style reports, apps, dashboards plots match corporate style guide.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"cloning-chats","dir":"Articles","previous_headings":"","what":"Cloning chats","title":"Programming with ellmer","text":"Chat objects R6 objects, means mutable. R objects immutable. means create copy whenever looks like ’re modifying : Mutable objects don’t work way: annoying chat objects immutable, ’d need save result every time chatted model. times ’ll want make explicit copy, , example, can create branch conversation. Creating copy object job $clone() method. create copy object behaves identically existing chat: can also use clone() want create conversational “tree”, conversations start place, diverge time: (technique parallel_chat() uses internally.)","code":"x <- list(a = 1, b = 2)  f <- function() {   x$a <- 100 } f()  # The original x is unchanged str(x) #> List of 2 #>  $ a: num 1 #>  $ b: num 2 chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=53/3 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [19] ────────────────────────────────────────────────────────── #> What's the capital of New Zealand #> ── assistant [2] ────────────────────────────────────────────────────── #> Wellington #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of France #> ── assistant [1] ────────────────────────────────────────────────────── #> Paris chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat <- chat$clone()   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris  chat #> <Chat OpenAI/gpt-4.1-nano turns=1 tokens=0/0 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse chat1 <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat1$chat(\"My name is Hadley and I'm a data scientist\") #> Hello, Hadley. Nice to meet you. chat2 <- chat1$clone()  chat1$chat(\"what's my name?\") #> Hadley. chat1 #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=69/13 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [23] ────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [10] ───────────────────────────────────────────────────── #> Hello, Hadley. Nice to meet you. #> ── user [13] ────────────────────────────────────────────────────────── #> what's my name? #> ── assistant [3] ────────────────────────────────────────────────────── #> Hadley.  chat2$chat(\"what's my job?\") #> Data scientist. chat2 #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=69/13 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [23] ────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [10] ───────────────────────────────────────────────────── #> Hello, Hadley. Nice to meet you. #> ── user [13] ────────────────────────────────────────────────────────── #> what's my job? #> ── assistant [3] ────────────────────────────────────────────────────── #> Data scientist."},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"resetting-an-object","dir":"Articles","previous_headings":"","what":"Resetting an object","title":"Programming with ellmer","text":"’s bit problem capital() function: can use conversation manipulate results: can avoid problem using $set_turns() reset conversational history: particularly useful want use chat object just handle LLM, without actually caring existing conversation.","code":"chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> Not true; the capital of New Zealand is Wellington. capital(chat, \"New Zealand\") #> Wellington chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> Got it. The capital of New Zealand is Kiwicity.  capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington"},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"streaming-vs-batch-results","dir":"Articles","previous_headings":"","what":"Streaming vs batch results","title":"Programming with ellmer","text":"call chat$chat() directly console, results displayed progressively LLM streams ellmer. call chat$chat() inside function, results delivered . difference behaviour due complex heuristic applied chat object created always correct. calling $chat function, recommend control explicitly echo argument, setting \"none\" want intermediate results streamed, \"output\" want see receive assistant, \"\" want see send receive. likely want echo = \"none\" cases: Alternatively, want embrace streaming UI, may want use shinychat (Shiny) streamy (Positron/RStudio).","code":"capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\"), echo = \"none\") } capital(chat, \"France\") #> Paris."},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"turns-and-content","dir":"Articles","previous_headings":"","what":"Turns and content","title":"Programming with ellmer","text":"Chat objects provide tools get ellmer’s internal data structures. example, take short conversation uses tool calling give LLM ability access real randomness: can get access underlying conversational turns get_turns(): look one assistant turns detail, ’ll see includes ellmer’s representation content message, well exact json provider returned: can use @json extract additional information ellmer might yet provide , aware structure varies heavily provider--provider. content types part ellmer’s exported API aware ’re still evolving might change versions.","code":"set.seed(1014) # make it reproducible  chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$register_tool(tool(function() sample(6, 1), \"Roll a die\")) chat$chat(\"Roll two dice and tell me the total\") #> The total is 9.  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=151/48 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [47] ────────────────────────────────────────────────────────── #> Roll two dice and tell me the total #> ── assistant [41] ───────────────────────────────────────────────────── #> [tool request (call_1OtUWtCCMJ6YE8ZNFQtpoyQJ)]: tool_001() #> [tool request (call_UZw7jatKY7tYN7CfOrlk9j5l)]: tool_001() #> ── user [16] ────────────────────────────────────────────────────────── #> [tool result  (call_1OtUWtCCMJ6YE8ZNFQtpoyQJ)]: 5 #> [tool result  (call_UZw7jatKY7tYN7CfOrlk9j5l)]: 4 #> ── assistant [7] ────────────────────────────────────────────────────── #> The total is 9. turns <- chat$get_turns() turns #> [[1]] #> <Turn: user> #> Roll two dice and tell me the total #>  #> [[2]] #> <Turn: assistant> #> [tool request (call_1OtUWtCCMJ6YE8ZNFQtpoyQJ)]: tool_001() #> [tool request (call_UZw7jatKY7tYN7CfOrlk9j5l)]: tool_001() #>  #> [[3]] #> <Turn: user> #> [tool result  (call_1OtUWtCCMJ6YE8ZNFQtpoyQJ)]: 5 #> [tool result  (call_UZw7jatKY7tYN7CfOrlk9j5l)]: 4 #>  #> [[4]] #> <Turn: assistant> #> The total is 9. str(turns[[2]]) #> <ellmer::Turn> #>  @ role    : chr \"assistant\" #>  @ contents:List of 2 #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"call_1OtUWtCCMJ6YE8ZNFQtpoyQJ\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> function ()   #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"call_UZw7jatKY7tYN7CfOrlk9j5l\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> function ()   #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  @ json    :List of 8 #>  .. $ id                : chr \"chatcmpl-Bn2ZPB85PqyXTCCv6mtej2FNEkQLA\" #>  .. $ object            : chr \"chat.completion\" #>  .. $ created           : int 1751027651 #>  .. $ model             : chr \"gpt-4.1-nano-2025-04-14\" #>  .. $ choices           :List of 1 #>  ..  ..$ :List of 4 #>  ..  .. ..$ index        : int 0 #>  ..  .. ..$ message      :List of 5 #>  ..  .. .. ..$ role       : chr \"assistant\" #>  ..  .. .. ..$ content    : NULL #>  ..  .. .. ..$ tool_calls :List of 2 #>  ..  .. .. .. ..$ :List of 3 #>  ..  .. .. .. .. ..$ id      : chr \"call_1OtUWtCCMJ6YE8ZNFQtpoyQJ\" #>  ..  .. .. .. .. ..$ type    : chr \"function\" #>  ..  .. .. .. .. ..$ function:List of 2 #>  ..  .. .. .. .. .. ..$ name     : chr \"tool_001\" #>  ..  .. .. .. .. .. ..$ arguments: chr \"{}\" #>  ..  .. .. .. ..$ :List of 3 #>  ..  .. .. .. .. ..$ id      : chr \"call_UZw7jatKY7tYN7CfOrlk9j5l\" #>  ..  .. .. .. .. ..$ type    : chr \"function\" #>  ..  .. .. .. .. ..$ function:List of 2 #>  ..  .. .. .. .. .. ..$ name     : chr \"tool_001\" #>  ..  .. .. .. .. .. ..$ arguments: chr \"{}\" #>  ..  .. .. ..$ refusal    : NULL #>  ..  .. .. ..$ annotations: list() #>  ..  .. ..$ logprobs     : NULL #>  ..  .. ..$ finish_reason: chr \"tool_calls\" #>  .. $ usage             :List of 5 #>  ..  ..$ prompt_tokens            : int 47 #>  ..  ..$ completion_tokens        : int 41 #>  ..  ..$ total_tokens             : int 88 #>  ..  ..$ prompt_tokens_details    :List of 2 #>  ..  .. ..$ cached_tokens: int 0 #>  ..  .. ..$ audio_tokens : int 0 #>  ..  ..$ completion_tokens_details:List of 4 #>  ..  .. ..$ reasoning_tokens          : int 0 #>  ..  .. ..$ audio_tokens              : int 0 #>  ..  .. ..$ accepted_prediction_tokens: int 0 #>  ..  .. ..$ rejected_prediction_tokens: int 0 #>  .. $ service_tier      : chr \"default\" #>  .. $ system_fingerprint: chr \"fp_38343a2f8f\" #>  @ tokens  : int [1:3] 47 41 0 #>  @ text    : chr \"\""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best practices","title":"Prompt design","text":"’s highly likely ’ll end writing long, possibly multi-page prompts. ensure success task, two recommendations. First, put prompt , separate file. Second, write prompts using markdown. reason use markdown ’s quite readable LLMs (humans), allows things like use headers divide prompt sections itemised lists enumerate multiple options. can see examples style prompt : https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md https://github.com/jcheng5/py-sidebot/blob/main/prompt.md https://github.com/simonpcouch/chores/tree/main/inst/prompts https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md terms file names, one prompt project, call prompt.md. multiple prompts, give informative names like prompt-extract-metadata.md prompt-summarize-text.md. ’re writing package, put prompt(s) inst/prompts, otherwise ’s fine put project’s root directory. prompts going change time, ’d highly recommend commiting git repo. ensure can easily see changed, accidentally make mistake can easily roll back known good verison. prompt includes dynamic data, use ellmer::interpolate_file() intergrate prompt. interpolate_file() works like glue uses {{ }} instead { } make easier work JSON. iterate prompt, ’s good idea build small set challenging examples can regularly re-check latest version prompt. Currently ’ll need hand, hope eventually provide tools ’ll help little formally. Unfortunately, won’t see best practices action vignette since ’re keeping prompts short inline make easier grok ’s going .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"code-generation","dir":"Articles","previous_headings":"","what":"Code generation","title":"Prompt design","text":"Let’s explore prompt design simple code generation task: ’ll use chat_anthropic() problem experience best job generating code.","code":"question <- \"   How can I compute the mean and median of variables a, b, c, and so on,   all the way up to z, grouped by age and sex. \""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"basic-flavour","dir":"Articles","previous_headings":"Code generation","what":"Basic flavour","title":"Prompt design","text":"don’t provide system prompt, sometimes get answers different languages different styles R code: can ensure always get R code specific style providing system prompt: Note ’m using system prompt (defines general behaviour) user prompt (asks specific question). put content user prompt get similar results, think ’s helpful use cleanly divide general framing response specific questions ask. Since ’m mostly interested code, ask drop explanation sample data: course, want different style R code, just ask :","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> Here are several ways to compute the mean and median of variables a  #> through z, grouped by age and sex, depending on the tool you're using: #>  #> ## Python with pandas #>  #> ```python #> import pandas as pd #> import numpy as np #>  #> # Assuming your data is in a DataFrame called 'df' #> # Variables a-z are columns, along with 'age' and 'sex' #>  #> # Get all columns from 'a' to 'z' #> variables = [chr(i) for i in range(ord('a'), ord('z')+1)] #>  #> # Method 1: Using agg() with multiple functions #> result = df.groupby(['age', 'sex'])[variables].agg(['mean', 'median']) #>  #> # Method 2: Separate calculations #> means = df.groupby(['age', 'sex'])[variables].mean() #> medians = df.groupby(['age', 'sex'])[variables].median() #>  #> # Method 3: Using describe() for more statistics #> summary = df.groupby(['age', 'sex'])[variables].describe() #> ``` #>  #> ## R #>  #> ```r #> library(dplyr) #>  #> # Method 1: Using dplyr #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median), na.rm =  #> TRUE)) #>  #> # Method 2: Using aggregate() #> means <- aggregate(. ~ age + sex, data = df[c('age', 'sex', letters)], #> FUN = mean) #> medians <- aggregate(. ~ age + sex, data = df[c('age', 'sex',  #> letters)], FUN = median) #>  #> # Method 3: Using data.table #> library(data.table) #> dt <- as.data.table(df) #> result <- dt[, lapply(.SD, function(x) list(mean = mean(x), median =  #> median(x))),  #>              by = .(age, sex), .SDcols = letters] #> ``` #>  #> ## SQL #>  #> ```sql #> SELECT  #>     age,  #>     sex, #>     AVG(a) as mean_a, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY a)  #> as median_a, #>     AVG(b) as mean_b, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY b)  #> as median_b, #>     -- ... repeat for all variables c through z #>     AVG(z) as mean_z, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY z)  #> as median_z #> FROM your_table #> GROUP BY age, sex; #> ``` #>  #> ## SAS #>  #> ```sas #> proc means data=your_dataset mean median; #>     class age sex; #>     var a--z;  /* Variables from a to z */ #> run; #>  #> /* Or using PROC SUMMARY */ #> proc summary data=your_dataset; #>     class age sex; #>     var a--z; #>     output out=summary_stats mean= median= / autoname; #> run; #> ``` #>  #> ## SPSS Syntax #>  #> ```spss #> MEANS TABLES=a TO z BY age BY sex #>   /CELLS=MEAN MEDIAN. #> ``` #>  #> The pandas approach is probably most flexible if you're using Python,  #> as it gives you a clean DataFrame with hierarchical columns showing  #> both statistics for each variable. Which tool are you planning to use? chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> Here are a few ways to compute the mean and median of variables a  #> through z, grouped by age and sex using tidyverse: #>  #> ## Method 1: Using `across()` with `summarise()` (Recommended) #>  #> ```r #> library(dplyr) #>  #> # If you want separate columns for mean and median #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) #>  #> # If you want a longer format that's easier to work with #> result_long <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) %>% #>   pivot_longer( #>     cols = -c(age, sex), #>     names_to = c(\"variable\", \"statistic\"), #>     names_sep = \"_\", #>     values_to = \"value\" #>   ) #> ``` #>  #> ## Method 2: If you need to handle missing values #>  #> ```r #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(a:z, list( #>       mean = ~mean(.x, na.rm = TRUE), #>       median = ~median(.x, na.rm = TRUE) #>     ), .names = \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) #> ``` #>  #> ## Method 3: Using `pivot_longer()` first (alternative approach) #>  #> ```r #> library(dplyr) #> library(tidyr) #>  #> result <- df %>% #>   pivot_longer(cols = a:z, names_to = \"variable\", values_to = \"value\") #> %>% #>   group_by(age, sex, variable) %>% #>   summarise( #>     mean = mean(value, na.rm = TRUE), #>     median = median(value, na.rm = TRUE), #>     .groups = \"drop\" #>   ) #> ``` #>  #> ## Method 4: If you want to specify variables by pattern #>  #> ```r #> # If your variables follow a pattern or you want to be more selective #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(matches(\"^[a-z]$\"), list(mean = mean, median = median),  #> .names = \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) #> ``` #>  #> The first method is generally the most efficient and produces a wide  #> format that's easy to read. The third method produces a long format  #> that's often more convenient for further analysis or visualization. #>  #> Choose the method that best fits your data structure and analysis  #> needs! chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(dplyr) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median), na.rm =  #> TRUE), #>             .groups = \"drop\") #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers data.table.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(data.table) #>  #> # Assuming your data is in a data.table called 'dt' #> dt[, lapply(.SD, function(x) list(mean = mean(x, na.rm = TRUE),  #>                                   median = median(x, na.rm = TRUE))),  #>    by = .(age, sex),  #>    .SDcols = letters] #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers base R.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> aggregate(. ~ age + sex, data = df[c(\"age\", \"sex\", letters)], FUN =  #> function(x) c(mean = mean(x), median = median(x))) #> ``` #>  #> Or if you want separate columns for means and medians: #>  #> ```r #> means <- aggregate(. ~ age + sex, data = df[c(\"age\", \"sex\", letters)], #> FUN = mean) #> medians <- aggregate(. ~ age + sex, data = df[c(\"age\", \"sex\",  #> letters)], FUN = median) #> names(medians)[-(1:2)] <- paste0(names(medians)[-(1:2)], \"_median\") #> merge(means, medians, by = c(\"age\", \"sex\")) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"be-explicit","dir":"Articles","previous_headings":"Code generation","what":"Be explicit","title":"Prompt design","text":"’s something output don’t like, try explicit. example, code isn’t styled quite ’d like , provide details want: still doesn’t yield exactly code ’d write, ’s pretty close. provide different prompt looking explanation code:","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data.    Follow the tidyverse style guide:   * Spread long function calls across multiple lines.   * Where needed, always indent function calls with two spaces.   * Only name arguments that are less commonly used.   * Always use double quotes for strings.   * Use the base pipe, `|>`, not the magrittr pipe `%>%`. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(dplyr) #>  #> data |> #>   group_by(age, sex) |> #>   summarise( #>     across( #>       a:z, #>       list(mean = mean, median = median), #>       na.rm = TRUE #>     ), #>     .groups = \"drop\" #>   ) #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R teacher.   I am a new R user who wants to improve my programming skills.   Help me understand the code you produce by explaining each function call with   a brief comment. For more complicated calls, add documentation to each   argument. Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> # Compute mean and median for variables a through z, grouped by age  #> and sex #> result <- df %>% #>   group_by(age, sex) %>%  # Group data by age and sex categories #>   summarise( #>     across( #>       .cols = a:z,  # Select all columns from 'a' to 'z' #>       .fns = list( #>         mean = ~ mean(.x, na.rm = TRUE),    # Calculate mean, removing #> missing values #>         median = ~ median(.x, na.rm = TRUE) # Calculate median,  #> removing missing values #>       ), #>       .names = \"{.col}_{.fn}\"  # Name format: variable_statistic  #> (e.g., a_mean, a_median) #>     ), #>     .groups = \"drop\"  # Remove grouping structure from result #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"teach-it-about-new-features","dir":"Articles","previous_headings":"Code generation","what":"Teach it about new features","title":"Prompt design","text":"can imagine LLMs sort average internet given point time. means provide popular answers, tend reflect older coding styles (either new features aren’t index, older features much popular). want code use specific newer language features, might need provide examples :","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer.   Just give me the code; no explanation in text.   Use the `.by` argument rather than `group_by()`.   dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.   e.g., instead of:    transactions |>     group_by(company, year) |>     mutate(total = sum(revenue))    write this:   transactions |>     mutate(       total = sum(revenue),       .by = c(company, year)     ) \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> data |> #>   summarise( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .by = c(age, sex) #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data","dir":"Articles","previous_headings":"","what":"Structured data","title":"Prompt design","text":"Providing rich set examples great way encourage output produce exactly want. known multi-shot prompting. ’ll work prompt designed extract structured data recipes, ideas apply many situations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"getting-started","dir":"Articles","previous_headings":"Structured data","what":"Getting started","title":"Prompt design","text":"overall goal turn list ingredients, like following, nicely structured JSON can analyse R (e.g. compute total weight, scale recipe , convert units volumes weights). (isn’t ingredient list real recipe includes sampling styles encountered project.) don’t strong feelings data structure look like, can start loose prompt see get back. find useful pattern underspecified problems heavy lifting lies precisely defining problem want solve. Seeing LLM’s attempt create data structure gives something react , rather start blank page. (don’t know additional colour, “’re expert baker also loves JSON”, anything, like think helps LLM get right mindset nerdy baker.)","code":"ingredients <- \"   ¾ cup (150g) dark brown sugar   2 large eggs   ¾ cup (165g) sour cream   ½ cup (113g) unsalted butter, melted   1 teaspoon vanilla extract   ¾ teaspoon kosher salt   ⅓ cup (80ml) neutral oil   1½ cups (190g) all-purpose flour   150g plus 1½ teaspoons sugar \" instruct_json <- \"   You're an expert baker who also loves JSON. I am going to give you a list of   ingredients and your job is to return nicely structured JSON. Just return the   JSON and no other commentary. \"  chat <- chat_openai(instruct_json) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> [ #>   { #>     \"ingredient\": \"dark brown sugar\", #>     \"amount\": \"3/4 cup\", #>     \"weight\": \"150g\" #>   }, #>   { #>     \"ingredient\": \"eggs\", #>     \"amount\": \"2\", #>     \"unit\": \"large\" #>   }, #>   { #>     \"ingredient\": \"sour cream\", #>     \"amount\": \"3/4 cup\", #>     \"weight\": \"165g\" #>   }, #>   { #>     \"ingredient\": \"unsalted butter\", #>     \"amount\": \"1/2 cup\", #>     \"weight\": \"113g\", #>     \"preparation\": \"melted\" #>   }, #>   { #>     \"ingredient\": \"vanilla extract\", #>     \"amount\": \"1 teaspoon\" #>   }, #>   { #>     \"ingredient\": \"kosher salt\", #>     \"amount\": \"3/4 teaspoon\" #>   }, #>   { #>     \"ingredient\": \"neutral oil\", #>     \"amount\": \"1/3 cup\", #>     \"volume\": \"80ml\" #>   }, #>   { #>     \"ingredient\": \"all-purpose flour\", #>     \"amount\": \"1 1/2 cups\", #>     \"weight\": \"190g\" #>   }, #>   { #>     \"ingredient\": \"sugar\", #>     \"amounts\": [ #>       { #>         \"amount\": \"150g\" #>       }, #>       { #>         \"amount\": \"1 1/2 teaspoons\" #>       } #>     ] #>   } #> ]"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"provide-examples","dir":"Articles","previous_headings":"Structured data","what":"Provide examples","title":"Prompt design","text":"isn’t bad start, prefer cook weight want see volumes weight isn’t available provide couple examples ’m looking . pleasantly suprised can provide input output examples loose format. Just providing examples seems work remarkably well. found useful also include description examples trying accomplish. ’m sure helps LLM , certainly makes easier understand organisation whole prompt check ’ve covered key pieces ’m interested . structure also allows give LLMs hint want multiple ingredients stored, .e. JSON array. iterated prompt, looking results different recipes get sense LLM getting wrong. Much felt like waws iterating understanding problem didn’t start knowing exactly wanted data. example, started didn’t really think various ways ingredients specified. later analysis, always want quantities number, even originally fractions, units aren’t precise (like pinch). made realise ingredients unitless. might want take look full prompt see ended .","code":"instruct_weight <- r\"(   Here are some examples of the sort of output I'm looking for:    ¾ cup (150g) dark brown sugar   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}    ⅓ cup (80ml) neutral oil   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}    2 t ground cinnamon   {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"} )\"  chat <- chat_openai(paste(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> [ #>   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"large eggs\", \"quantity\": 2, \"unit\": \"count\"}, #>   {\"name\": \"sour cream\", \"quantity\": 165, \"unit\": \"g\"}, #>   {\"name\": \"unsalted butter\", \"quantity\": 113, \"unit\": \"g\", \"note\":  #> \"melted\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"}, #>   {\"name\": \"kosher salt\", \"quantity\": 0.75, \"unit\": \"teaspoon\"}, #>   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 190, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 1.5, \"unit\": \"teaspoon\"} #> ] instruct_weight <- r\"(   * If an ingredient has both weight and volume, extract only the weight:    ¾ cup (150g) dark brown sugar   [     {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}   ]  * If an ingredient only lists a volume, extract that.    2 t ground cinnamon   ⅓ cup (80ml) neutral oil   [     {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"},     {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}   ] )\" instruct_unit <- r\"( * If the unit uses a fraction, convert it to a decimal.    ⅓ cup sugar   ½ teaspoon salt   [     {\"name\": \"dark brown sugar\", \"quantity\": 0.33, \"unit\": \"cup\"},     {\"name\": \"salt\", \"quantity\": 0.5, \"unit\": \"teaspoon\"}   ]  * Quantities are always numbers    pinch of kosher salt   [     {\"name\": \"kosher salt\", \"quantity\": 1, \"unit\": \"pinch\"}   ]  * Some ingredients don't have a unit.   2 eggs   1 lime   1 apple   [     {\"name\": \"egg\", \"quantity\": 2},     {\"name\": \"lime\", \"quantity\": 1},     {\"name\", \"apple\", \"quantity\": 1}   ] )\""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data-1","dir":"Articles","previous_headings":"Structured data","what":"Structured data","title":"Prompt design","text":"Now ’ve iterated get data structure like, seems useful formalise tell LLM exactly ’m looking dealing structured data. guarantees LLM return JSON, JSON fields expect, ellmer convert R data structure.","code":"type_ingredient <- type_object(   name = type_string(\"Ingredient name\"),   quantity = type_number(),   unit = type_string(\"Unit of measurement\") )  type_ingredients <- type_array(type_ingredient)  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat_structured(ingredients, type = type_ingredients) #>                       name quantity     unit #> 1         dark brown sugar   150.00        g #> 2                     eggs     2.00    large #> 3               sour cream   165.00        g #> 4  unsalted butter, melted   113.00        g #> 5          vanilla extract     1.00 teaspoon #> 6              kosher salt     0.75 teaspoon #> 7              neutral oil    80.00       ml #> 8        all-purpose flour   190.00        g #> 9                    sugar   150.00        g #> 10                   sugar     1.50 teaspoon"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"capturing-raw-input","dir":"Articles","previous_headings":"Structured data","what":"Capturing raw input","title":"Prompt design","text":"One thing ’d next time also include raw ingredient names output. doesn’t make much difference simple example makes much easier align input output start developing automated measures well prompt . think particularly important ’re working even less structured text. example, imagine text: Including input text output makes easier see ’s good job: ran writing vignette, seemed working weight ingredients specified volume, even though prompt specifically asks . may suggest need broaden examples.","code":"instruct_weight_input <- r\"(   * If an ingredient has both weight and volume, extract only the weight:      ¾ cup (150g) dark brown sugar     [       {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\", \"input\": \"¾ cup (150g) dark brown sugar\"}     ]    * If an ingredient only lists a volume, extract that.      2 t ground cinnamon     ⅓ cup (80ml) neutral oil     [       {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\", \"input\": \"2 t ground cinnamon\"},       {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\", \"input\": \"⅓ cup (80ml) neutral oil\"}     ] )\" recipe <- r\"(   In a large bowl, cream together one cup of softened unsalted butter and a   quarter cup of white sugar until smooth. Beat in an egg and 1 teaspoon of   vanilla extract. Gradually stir in 2 cups of all-purpose flour until the   dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop   spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C)   for 10-12 minutes, or until the edges are lightly browned. Let the cookies   cool on the baking sheet for a few minutes before transferring to a wire   rack to cool completely. Enjoy! )\" chat <- chat_openai(c(instruct_json, instruct_weight_input)) #> Using model = \"gpt-4.1\". chat$chat(recipe) #> [ #>   {\"name\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\", \"input\":  #> \"one cup of softened unsalted butter\"}, #>   {\"name\": \"white sugar\", \"quantity\": 0.25, \"unit\": \"cup\", \"input\": \"a #> quarter cup of white sugar\"}, #>   {\"name\": \"egg\", \"quantity\": 1, \"unit\": \"item\", \"input\": \"an egg\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\",  #> \"input\": \"1 teaspoon of vanilla extract\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cup\", \"input\": #> \"2 cups of all-purpose flour\"}, #>   {\"name\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\",  #> \"input\": \"1 cup of semisweet chocolate chips\"} #> ]"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console returns result response complete.) want process response arrives, can use stream() method. useful want send response, realtime, somewhere R console (e.g., file, HTTP response, Shiny chat window), want manipulate response displaying without giving immediacy streaming. stream() method, returns coro generator, can process response looping arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"ellmer also supports async usage. useful want run multiple, concurrent chat sessions. particularly important Shiny applications using methods described block Shiny app users duration response. use async chat, call chat_async()/stream_async() instead chat()/stream(). _async variants take arguments construction return promise instead actual response. Remember chat objects stateful; preserve conversation history interact . means doesn’t make sense issue multiple, concurrent chat/stream operations chat object conversation history can become corrupted interleaved conversation fragments. need run concurrent chat sessions, create multiple chat objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, ’d use chat() method , handle result promise instead string.","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"shiny-example","dir":"Articles","previous_headings":"Async usage > Asynchronous chat","what":"Shiny example","title":"Streaming and async APIs","text":"add asynchronous chat interface Shiny application, recommend using shinychat package. simplest approach use shinychat’s Shiny module add chat UI app—similar app created live_browser()—using shinychat::chat_mod_ui() shinychat::chat_mod_server() functions. module functions connect ellmer::Chat object shinychat::chat_ui() handle non-blocking asynchronous chat interactions automatically. fully custom streaming applications custom chat interface, can use shinychat::markdown_stream() stream responses Shiny app. particularly useful creating interactive chat applications want display responses generated. following Shiny app demonstrates markdown_stream() uses $stream_async() $chat_async() stream story OpenAI model. app, ask user prompt generate story stream story UI. follow asking model story title use response update card title. example also highlights difference streaming non-streaming chat. Use $stream_async() Shiny outputs designed work generators, like shinychat::markdown_stream() shinychat::chat_append(). Use $chat_async() want text response model, example title story. Also note ellmer-powered Shiny apps, ’s best wrap chat interaction shiny::ExtendedTask avoid blocking rest app chat generated. can learn ExtendedTask Shiny’s Non-blocking operations article.","code":"library(shiny) library(shinychat)  ui <- bslib::page_fillable(   chat_mod_ui(\"chat\") )  server <- function(input, output, session) {   chat <- ellmer::chat_openai(     system_prompt = \"You're a trickster who answers in riddles\",     model = \"gpt-4.1-nano\"   )    chat_mod_server(\"chat\", chat) }  shinyApp(ui, server) library(shiny) library(bslib) library(ellmer) library(promises) library(shinychat)  ui <- page_sidebar(   title = \"Interactive chat with async\",   sidebar = sidebar(     textAreaInput(\"user_query\", \"Tell me a story about...\"),     input_task_button(\"ask_chat\", label = \"Generate a story\")   ),   card(     card_header(textOutput(\"story_title\")),     shinychat::output_markdown_stream(\"response\"),   ) )  server <- function(input, output) {   chat_task <- ExtendedTask$new(function(user_query) {     # We're using an Extended Task for chat completions to avoid blocking the     # app. We also start the chat fresh each time, because the UI is not a     # multi-turn conversation.     chat <- chat_openai(       system_prompt = \"You are a rambling chatbot who likes to tell stories but gets distracted easily.\",       model = \"gpt-4.1-nano\"     )      # Stream the chat completion into the markdown stream. `markdown_stream()`     # returns a promise onto which we'll chain the follow-up task of providing     # a story title.     stream <- chat$stream_async(user_query)     stream_res <- shinychat::markdown_stream(\"response\", stream)      # Follow up by asking the LLM to provide a title for the story that we     # return from the task.     stream_res$then(function(value) {       chat$chat_async(         \"What is the title of the story? Reply with only the title and nothing else.\"       )     })   })    bind_task_button(chat_task, \"ask_chat\")    observeEvent(input$ask_chat, {     chat_task$invoke(input$user_query)   })    observe({     # Update the card title during generation and once complete     switch(       chat_task$status(),       success = story_title(chat_task$result()),       running = story_title(\"Generating your story...\"),       error = story_title(\"An error occurred while generating your story.\")     )   })    story_title <- reactiveVal(\"Your story will appear here!\")   output$story_title <- renderText(story_title()) }  shinyApp(ui = ui, server = server)"},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, ’d use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, ’ll simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"structured-data-basics","dir":"Articles","previous_headings":"","what":"Structured data basics","title":"Structured data","text":"extract structured data call $chat_structured() instead $chat(). ’ll also need define type specification describes structure data want (shortly). ’s simple example extracts two specific values string: basic idea works images : need extract data multiple prompts, can use parallel_chat_structured(). takes arguments $chat_structured() two exceptions: needs chat object since ’s standalone function, method, can take vector prompts. (Note structured data extraction automatically disables tool calling. can work around limitation regular $chat() using $chat_structured().)","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(   \"My name is Susan and I'm 13 years old\",   type = type_object(     name = type_string(),     age = type_number()   ) ) #> $name #> [1] \"Susan\" #>  #> $age #> [1] 13 chat$chat_structured(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   type = type_object(     primary_shape = type_string(),     primary_colour = type_string()   ) ) #> $primary_shape #> [1] \"letter and oval\" #>  #> $primary_colour #> [1] \"blue and gray\" prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(   name = type_string(),   age = type_number() ) chat <- chat_openai() #> Using model = \"gpt-4.1\". parallel_chat_structured(chat, prompts, type = type_person) #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-types","dir":"Articles","previous_headings":"","what":"Data types","title":"Structured data","text":"extract structured data effectively, need understand LLMs expect types defined, types map R types familiar .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"basics","dir":"Articles","previous_headings":"Data types","what":"Basics","title":"Structured data","text":"define desired type specification (also known schema), use type_() functions. also used tool calling (vignette(\"tool-calling\")), might already familiar .type functions can divided three main groups: Scalars represent single values. type_boolean(), type_integer(), type_number(), type_string(), type_enum(), represent single logical, integer, double, string, factor value respectively. Arrays represent vector values type. created type_array() require item argument specifies type element. Arrays scalars similar R’s atomic vectors: can also arrays arrays resemble lists well defined structures: Arrays objects (described next) equivalent data frames. Objects represent collection named values. created type_object(). Objects can contain number scalars, arrays, objects. similar named lists R. hood, type specifications ensures LLM returns correctly structured JSON. ellmer goes one step converts JSON closest R analog. means: Scalars converted length-1 vectors. Arrays scalars converted vectors. Arrays arrays converted unnamed lists. Objects converted named lists. Arrays objects converted data frames. can opt-get plain lists setting convert = FALSE. addition defining types, need provide LLM information types represent. purpose first argument, description, string describes data want. good place ask nicely attributes ’ll like value (e.g. minimum maximum values, date formats, …). ’s guarantee requests honoured, LLM try.","code":"type_logical_vector <- type_array(type_boolean()) type_integer_vector <- type_array(type_integer()) type_double_vector <- type_array(type_number()) type_character_vector <- type_array(type_string()) list_of_integers <- type_array(type_integer_vector) type_person2 <- type_object(   name = type_string(),   age = type_integer(),   hobbies = type_array(type_string()) ) type_person3 <- type_object(   \"A person\",   name = type_string(\"Name\"),   age = type_integer(\"Age, in years.\"),   hobbies = type_array(     type_string(),     \"List of hobbies. Should be exclusive and brief.\",   ) )"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"missing-values","dir":"Articles","previous_headings":"Data types","what":"Missing values","title":"Structured data","text":"type functions default required = TRUE means LLM try really hard extract values , leading hallucinations data doesn’t exist. Lets go back initial example extracting names ages, give inputs don’t names /ages. can often avoid problem setting required = FALSE: cases, may need adjust prompt well. Either way, strongly recommend include positive negative examples testing structured data extraction code.","code":"no_match <- list(   \"I like apples\",   \"What time is it?\",   \"This cheese is 3 years old\",   \"My name is Hadley.\" ) parallel_chat_structured(chat, no_match, type = type_person) #>      name age #> 1 Unknown   0 #> 2    <NA>  NA #> 3  cheese   3 #> 4  Hadley   0 type_person <- type_object(   name = type_string(required = FALSE),   age = type_number(required = FALSE) ) parallel_chat_structured(chat, no_match, type = type_person) #>     name age #> 1   <NA>  NA #> 2   <NA>  NA #> 3 cheese   3 #> 4 Hadley  NA"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-frames","dir":"Articles","previous_headings":"Data types","what":"Data frames","title":"Structured data","text":"cases, ’ll get data frame using parallel_chat_structured(), output row represents one input prompt. cases, might complex document want data frame single prompt. example, imagine want extract data people table: might tempted use definition similar R: object (.e., named list) containing multiple arrays (.e., vectors): doesn’t work ’s constraint array length, hence way ellmer know really wanted data frame. Instead, ’ll need turn data structure “inside ” create array objects: Now ellmer knows want gives data frame. ’re familiar terms row-oriented column-oriented data frames, idea. Since languages don’t possess vectorisation like R, row-oriented data frames common.","code":"prompt <- r\"( * John Smith. Age: 30. Height: 180 cm. Weight: 80 kg. * Jane Doe. Age: 25. Height: 5'5\". Weight: 110 lb. * Jose Rodriguez. Age: 40. Height: 190 cm. Weight: 90 kg. * June Lee | Age: 35 | Height 175 cm | Weight: 70 kg )\" type_people <- type_object(   name = type_array(type_string()),   age = type_array(type_integer()),   height = type_array(type_number(\"in m\")),   weight = type_array(type_number(\"in kg\")) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(prompt, type = type_people) #> $name #> [1] \"John Smith\"     \"Jane Doe\"       \"Jose Rodriguez\" \"June Lee\"       #>  #> $age #> [1] 30 25 40 35 #>  #> $height #> [1] 1.800 1.651 1.900 1.750 #>  #> $weight #> [1] 80.0 49.9 90.0 70.0 type_people <- type_array(   type_object(     name = type_string(),     age = type_integer(),     height = type_number(\"in m\"),     weight = type_number(\"in kg\")   ) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(prompt, type = type_people) #>             name age height weight #> 1     John Smith  30   1.80   80.0 #> 2       Jane Doe  25   1.65   49.9 #> 3 Jose Rodriguez  40   1.90   90.0 #> 4       June Lee  35   1.75   70.0"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Structured data","text":"following examples, closely inspired Claude documentation, hint ways can use structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-1-article-summarisation","dir":"Articles","previous_headings":"Examples","what":"Example 1: Article summarisation","title":"Structured data","text":"","code":"text <- readLines(system.file(   \"examples/third-party-testing.txt\",   package = \"ellmer\" )) # url <- \"https://www.anthropic.com/news/third-party-testing\" # html <- rvest::read_html(url) # text <- rvest::html_text2(rvest::html_element(html, \"article\"))  type_summary <- type_object(   \"Summary of the article.\",   author = type_string(\"Name of the article author\"),   topics = type_array(     type_string(),     'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.'   ),   summary = type_string(\"Summary of the article. One or two paragraphs max\"),   coherence = type_integer(     \"Coherence of the article's key points, 0-100 (inclusive)\"   ),   persuasion = type_number(\"Article's persuasion score, 0.0-1.0 (inclusive)\") )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_summary) cat(data$summary) #> The article argues that effective third-party testing is essential to AI policy for managing the risks and societal impacts of frontier AI systems such as large-scale generative models. Anthropic promotes a regime where AI models, especially the most powerful and general-purpose ones, undergo third-party evaluation to ensure safety, limit misuse (in domains like election integrity and national security), and build public trust. The article details what a robust third-party testing regime should include: trusted tests, credible assessors, and a balance between safety assurance and regulatory burden. It advocates for multi-actor involvement (companies, universities, governments), iterative prototyping, and funding for governmental infrastructure to drive effective regulation while avoiding overly burdensome procedures that stifle innovation or entrench large incumbents. The article emphasizes the need to balance openness in AI research, the necessity for safety-related limits on open dissemination of powerful models, and the risks of regulatory capture. Anthropic outlines its own contributions and affirms a cautious, feedback-driven approach to advocating AI policy. Ultimately, third-party testing is positioned as a linchpin for safe and trustworthy AI development.  str(data) #> List of 5 #>  $ author    : chr \"Anthropic (not specified by individual name)\" #>  $ topics    : chr [1:9] \"AI policy\" \"AI safety\" \"third-party testing\" \"AI regulation\" ... #>  $ summary   : chr \"The article argues that effective third-party testing is essential to AI policy for managing the risks and soci\"| __truncated__ #>  $ coherence : int 95 #>  $ persuasion: num 0.91"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-2-named-entity-recognition","dir":"Articles","previous_headings":"Examples","what":"Example 2: Named entity recognition","title":"Structured data","text":"","code":"text <- \"   John works at Google in New York. He met with Sarah, the CEO of   Acme Inc., last week in San Francisco. \"  type_named_entity <- type_object(   name = type_string(\"The extracted entity name.\"),   type = type_enum(c(\"person\", \"location\", \"organization\"), \"The entity type\"),   context = type_string(\"The context in which the entity appears in the text.\") ) type_named_entities <- type_array(type_named_entity)  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(text, type = type_named_entities) #>            name         type #> 1          John       person #> 2        Google organization #> 3      New York     location #> 4         Sarah       person #> 5     Acme Inc. organization #> 6 San Francisco     location #>                                            context #> 1                John works at Google in New York. #> 2                John works at Google in New York. #> 3                John works at Google in New York. #> 4          He met with Sarah, the CEO of Acme Inc. #> 5                      Sarah, the CEO of Acme Inc. #> 6 He met with Sarah... last week in San Francisco."},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-3-sentiment-analysis","dir":"Articles","previous_headings":"Examples","what":"Example 3: Sentiment analysis","title":"Structured data","text":"Note ’ve asked nicely scores sum 1, example (least ran code), guaranteed.","code":"text <- \"   The product was okay, but the customer service was terrible. I probably   won't buy from them again. \"  type_sentiment <- type_object(   \"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\",   positive_score = type_number(     \"Positive sentiment score, ranging from 0.0 to 1.0.\"   ),   negative_score = type_number(     \"Negative sentiment score, ranging from 0.0 to 1.0.\"   ),   neutral_score = type_number(     \"Neutral sentiment score, ranging from 0.0 to 1.0.\"   ) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". str(chat$chat_structured(text, type = type_sentiment)) #> List of 3 #>  $ positive_score: num 0.05 #>  $ negative_score: num 0.7 #>  $ neutral_score : num 0.25"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-4-text-classification","dir":"Articles","previous_headings":"Examples","what":"Example 4: Text classification","title":"Structured data","text":"","code":"text <- \"The new quantum computing breakthrough could revolutionize the tech industry.\"  type_score <- type_object(   name = type_enum(     c(       \"Politics\",       \"Sports\",       \"Technology\",       \"Entertainment\",       \"Business\",       \"Other\"     ),     \"The category name\",   ),   score = type_number(     \"The classification score for the category, ranging from 0.0 to 1.0.\"   ) ) type_classification <- type_array(   type_score,   description = \"Array of classification results. The scores should sum to 1.\" )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_classification) data #>         name score #> 1 Technology  0.95 #> 2   Business  0.03 #> 3      Other  0.02"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-5-working-with-unknown-keys","dir":"Articles","previous_headings":"Examples","what":"Example 5: Working with unknown keys","title":"Structured data","text":"example works Claude, GPT Gemini, Claude supports adding additional, arbitrary properties.","code":"type_characteristics <- type_object(   \"All characteristics\",   .additional_properties = TRUE )  text <- \"   The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket. \"  chat <- chat_anthropic(\"Extract all characteristics of supplied character\") #> Using model = \"claude-sonnet-4-20250514\". str(chat$chat_structured(text, type = type_characteristics)) #> List of 6 #>  $ gender              : chr \"male\" #>  $ height              : chr \"tall\" #>  $ facial_hair         : chr \"beard\" #>  $ distinguishing_marks: chr \"scar on left cheek\" #>  $ voice               : chr \"deep voice\" #>  $ clothing            : chr \"black leather jacket\""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-6-extracting-data-from-an-image","dir":"Articles","previous_headings":"Examples","what":"Example 6: Extracting data from an image","title":"Structured data","text":"final example comes Dan Nguyen (can see interesting applications link). goal extract structured data screenshot: Even without descriptions, ChatGPT pretty well:","code":"type_asset <- type_object(   assert_name = type_string(),   owner = type_string(),   location = type_string(),   asset_value_low = type_integer(),   asset_value_high = type_integer(),   income_type = type_string(),   income_low = type_integer(),   income_high = type_integer(),   tx_gt_1000 = type_boolean() ) type_assets <- type_array(type_asset)  chat <- chat_openai() image <- content_image_file(\"congressional-assets.png\") data <- chat$chat_structured(image, type = type_assets) data"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool/function calling","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . ’s easy think tool calling might work like : fact works like : value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(ellmer)"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool/function calling","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Since model doesn’t know day , result incorrect.","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago did Neil Armstrong touch down on the moon?\") #> Neil Armstrong touched down on the Moon on July 20, 1969. As of  #> October 2023, that event occurred 54 years ago."},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool/function calling","text":"first thing ’ll define R function returns current time. Note ’ve gone trouble creating roxygen2 comments. isn’t necessary, ’ll see shortly, can make bit easier generate tool defintion. turn function tool, provide additional metadata model use: fair amount code write, even simple function. Fortunately, don’t write hand! generated tool() call calling create_tool_def(get_current_time), uses LLM generate tool() call . create_tool_def() perfect, must review generated code using , big time-saver. Note tool just special type function can still call :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time <- tool(   get_current_time,   name = \"get_current_time\",   description = \"Returns the current time.\",   arguments = list(     tz = type_string(       \"Time zone to display the current time in. Defaults to `\\\"UTC\\\"`.\",       required = FALSE     )   ) ) get_current_time() #> [1] \"2025-06-25 16:53:23 UTC\""},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"registering-and-using-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering and using tools","title":"Tool/function calling","text":"Now need give chat object access tool. $register_tool(): ’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. print chat can see model decided use tool: (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using ellmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$register_tool(get_current_time) chat$chat(\"How long ago did Neil Armstrong touch down on the moon?\") #> Neil Armstrong touched down on the Moon on July 20, 1969. As of June  #> 25, 2025, that event occurred 55 years and approximately 11 months  #> ago. chat #> <Chat OpenAI/gpt-4o turns=6 tokens=373/86 $0.00> #> ── user [19] ────────────────────────────────────────────────────────── #> How long ago did Neil Armstrong touch down on the moon? #> ── assistant [31] ───────────────────────────────────────────────────── #> Neil Armstrong touched down on the Moon on July 20, 1969. As of October 2023, that event occurred 54 years ago. #> ── user [108] ───────────────────────────────────────────────────────── #> How long ago did Neil Armstrong touch down on the moon? #> ── assistant [15] ───────────────────────────────────────────────────── #> [tool request (call_olBG8OUjyIj914iwNBjw35dv)]: get_current_time(tz = \"UTC\") #> ── user [23] ────────────────────────────────────────────────────────── #> [tool result  (call_olBG8OUjyIj914iwNBjw35dv)]: 2025-06-25 16:53:23 UTC #> ── assistant [40] ───────────────────────────────────────────────────── #> Neil Armstrong touched down on the Moon on July 20, 1969. As of June 25, 2025, that event occurred 55 years and approximately 11 months ago."},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"tool-inputs-and-outputs","dir":"Articles","previous_headings":"Introduction","what":"Tool inputs and outputs","title":"Tool/function calling","text":"Remember tool arguments come LLM, tool results returned LLM. implies keep simple possible. Inputs tool call, must defined type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object(). recommend keeping simple possible, focusing basic scalar types much can. output tool call interpreted LLM, just typed information data. means ’ll generally want produce text atomic vectors. complex data, ellmer automatically serialize result JSON, LLMs generally seem good understanding. must direct control structure JSON ’s returned, can return JSON-serializable value wrapped (), ellmer leave alone entire request JSON-serialized. show ideas, ’s slightly complicated example simulating weather API returns data multiple cities . get_weather() function returns data frame ellmer automatically convert JSON row-major format, experiments suggest good LLMs. Now register use : can print chat confirm model performed single tool call:","code":"get_weather <- tool(   function(cities) {     raining <- c(London = \"heavy\", Houston = \"none\", Chicago = \"overcast\")     temperature <- c(London = \"cool\", Houston = \"hot\", Chicago = \"warm\")     wind <- c(London = \"strong\", Houston = \"weak\", Chicago = \"strong\")      data.frame(       city = cities,       raining = unname(raining[cities]),       temperature = unname(temperature[cities]),       wind = unname(wind[cities])     )   },   name = \"get_weather\",   description = \"     Report on weather conditions in multiple cities. For efficiency, request      all weather updates using a single tool call   \",   arguments = list(     cities = type_array(type_string(), \"City names\")   ) ) chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$register_tool(get_weather) chat$chat(\"Give me a weather udpate for London and Chicago\") #> Here's the latest weather update: #>  #> - London: Heavy rain, cool temperatures, and strong winds. #> - Chicago: Overcast skies, warm temperatures, and strong winds. #>  #> Let me know if you need a forecast for more cities or details! chat #> <Chat OpenAI/gpt-4.1 turns=4 tokens=213/65 $0.00> #> ── user [74] ────────────────────────────────────────────────────────── #> Give me a weather udpate for London and Chicago #> ── assistant [17] ───────────────────────────────────────────────────── #> [tool request (call_0epVfhsX7e8pEy0M1NaY9Vca)]: get_weather(cities = c(\"London\", \"Chicago\")) #> ── user [48] ────────────────────────────────────────────────────────── #> [tool result  (call_0epVfhsX7e8pEy0M1NaY9Vca)]: [{\"city\":\"London\",\"raining\":\"heavy\",\"temperature\":\"cool\",\"wind\":\"strong\"},{\"city\":\"Chicago\",\"raining\":\"overcast\",\"temperature\":\"warm\",\"wind\":\"strong\"}] #> ── assistant [48] ───────────────────────────────────────────────────── #> Here's the latest weather update: #>  #> - London: Heavy rain, cool temperatures, and strong winds. #> - Chicago: Overcast skies, warm temperatures, and strong winds. #>  #> Let me know if you need a forecast for more cities or details!"},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Joe Cheng. Author. Aaron Jacobs. Author. Garrick Aden-Buie. Author. Barret Schloerke. Author. . Copyright holder, funder.","code":""},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Cheng J, Jacobs , Aden-Buie G, Schloerke B (2025). ellmer: Chat Large Language Models. R package version 0.3.2.9000, https://ellmer.tidyverse.org.","code":"@Manual{,   title = {ellmer: Chat with Large Language Models},   author = {Hadley Wickham and Joe Cheng and Aaron Jacobs and Garrick Aden-Buie and Barret Schloerke},   year = {2025},   note = {R package version 0.3.2.9000},   url = {https://ellmer.tidyverse.org}, }"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"ellmer-","dir":"","previous_headings":"","what":"Chat with Large Language Models","title":"Chat with Large Language Models","text":"ellmer makes easy use large language models (LLM) R. supports wide variety LLM providers implements rich set features including streaming outputs, tool/function calling, structured data extraction, . ellmer one number LLM-related packages created Posit: Looking something similar python? Check chatlas! Want evaluate LLMs? Try vitals. Need RAG? Take look ragnar. Want make beautiful LLM powered chatbot? Consider shinychat. Working MCP? Check mcptools.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Chat with Large Language Models","text":"can install ellmer CRAN :","code":"install.packages(\"ellmer\")"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providers","dir":"","previous_headings":"","what":"Providers","title":"Chat with Large Language Models","text":"ellmer supports wide variety model providers: Anthropic’s Claude: chat_anthropic(). AWS Bedrock: chat_aws_bedrock(). Azure OpenAI: chat_azure_openai(). Cloudflare: chat_cloudflare(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini/Vertex AI: chat_google_gemini(), chat_google_vertex(). Groq: chat_groq(). Hugging Face: chat_huggingface(). Mistral: chat_mistral(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() chat_cortex_analyst(). VLLM: chat_vllm().","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providermodel-choice","dir":"","previous_headings":"Providers","what":"Provider/model choice","title":"Chat with Large Language Models","text":"’re using ellmer inside organisation, may internal policies limit models big cloud providers, e.g. chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake(). ’re using ellmer exploration, ’ll lot freedom, recommendations help get started: chat_openai() chat_anthropic() good places start. chat_openai() defaults GPT-4.1, can use model = \"gpt-4-1-nano\" cheaper, faster model, model = \"o3\" complex reasoning. chat_anthropic() also good; defaults Claude 4.0 Sonnet, found particularly good writing R code. chat_google_gemini() strong model generous free tier (downside data used improve model), making great place start don’t want spend money. chat_ollama(), uses Ollama, allows run models computer. biggest models can run locally aren’t good state art hosted models, don’t share data effectively free.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"authentication","dir":"","previous_headings":"Providers","what":"Authentication","title":"Chat with Large Language Models","text":"Authentication works little differently depending provider. popular ones (including OpenAI Anthropic) require obtain API key. recommend save environment variable rather using directly code, deploy app report uses ellmer another system, ’ll need ensure environment variable available , . ellmer also automatically detects many OAuth IAM-based credentials used big cloud providers (currently chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake()). includes credentials platforms managed Posit Workbench Posit Connect. find cases ellmer detect credentials one cloud providers, feel free open issue; ’re happy add auth mechanisms needed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"using-ellmer","dir":"","previous_headings":"","what":"Using ellmer","title":"Chat with Large Language Models","text":"can work ellmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful R6 objects: retain context conversation, new query builds previous ones. call methods $.","code":"library(ellmer)  chat <- chat_openai(\"Be terse\", model = \"gpt-4o-mini\")"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using ellmer","what":"Interactive chat console","title":"Chat with Large Language Models","text":"interactive least programmatic way using ellmer chat directly R console browser live_console(chat) live_browser(): Keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist exit back R prompt. true regardless chat function use.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using ellmer","what":"Interactive method call","title":"Chat with Large Language Models","text":"second interactive way chat call chat() method: initialize chat object global environment, chat method stream response console. entire response received, ’s also (invisibly) returned character vector. useful want see response arrives, don’t want enter chat console. want ask question image, can pass one additional input arguments using content_image_file() /content_image_url():","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by S, which was developed at Bell Labs. Other  #> notable influences include Scheme, for its functional programming concepts, and #> various statistical programming languages like Fortran and Lisp. chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo features a stylized letter \"R\" inside an oval shape, which represents  #> the R programming language. The design is modern and clean, emphasizing the  #> letter \"R\" prominently in blue, while the oval shape is often interpreted as a  #> symbol of data analysis and statistics, reflecting R's primary use in  #> statistical computing and graphics. The overall look conveys professionalism  #> and is recognized in the programming and data science communities."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"streaming-vs-capturing","dir":"","previous_headings":"Using ellmer","what":"Streaming vs capturing","title":"Chat with Large Language Models","text":"circumstances, ellmer stream output console. can take control setting echo argument either creating chat object calling $chat(). Set echo = \"none\" return string instead: needed, can manually control behaviour echo argument. useful programming ellmer result either intended human consumption want process response displaying .","code":"my_function <- function() {   chat <- chat_openai(\"Be terse\", model = \"gpt-4o-mini\", echo = \"none\")   chat$chat(\"What is 6 times 7?\") } str(my_function()) #>  'ellmer_output' chr \"42.\""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Chat with Large Language Models","text":"ellmer comes bunch vignettes help learn : Learn key vocabulary see example use cases vignette(\"ellmer\"). Learn design prompt vignette(\"prompt-design\"). Learn tool/function calling vignette(\"tool-calling\"). Learn extract structured data vignette(\"structured-data\"). Learn streaming async APIs vignette(\"streaming-async\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"The Chat object — Chat","title":"The Chat object — Chat","text":"Chat sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Chat object — Chat","text":"Chat object","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"The Chat object — Chat","text":"Chat$new() Chat$get_turns() Chat$set_turns() Chat$add_turn() Chat$get_system_prompt() Chat$get_model() Chat$set_system_prompt() Chat$get_tokens() Chat$get_cost() Chat$last_turn() Chat$chat() Chat$chat_structured() Chat$chat_structured_async() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$register_tools() Chat$get_provider() Chat$get_tools() Chat$set_tools() Chat$on_tool_request() Chat$on_tool_result() Chat$extract_data() Chat$extract_data_async() Chat$clone()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$new(provider, system_prompt = NULL, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"provider provider object. system_prompt System prompt start conversation . echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. can override default setting ellmer_echo option.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-turns-","dir":"Reference","previous_headings":"","what":"Method get_turns()","title":"The Chat object — Chat","text":"Retrieve turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_turns(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-turns-","dir":"Reference","previous_headings":"","what":"Method set_turns()","title":"The Chat object — Chat","text":"Replace existing turns new list.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_turns(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"value list Turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-add-turn-","dir":"Reference","previous_headings":"","what":"Method add_turn()","title":"The Chat object — Chat","text":"Add pair turns chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$add_turn(user, system)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"user user Turn. system system Turn.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-system-prompt-","dir":"Reference","previous_headings":"","what":"Method get_system_prompt()","title":"The Chat object — Chat","text":"set, system prompt, , NULL.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_system_prompt()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-model-","dir":"Reference","previous_headings":"","what":"Method get_model()","title":"The Chat object — Chat","text":"Retrieve model name","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_model()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-system-prompt-","dir":"Reference","previous_headings":"","what":"Method set_system_prompt()","title":"The Chat object — Chat","text":"Update system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_system_prompt(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"value character vector giving new system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tokens-","dir":"Reference","previous_headings":"","what":"Method get_tokens()","title":"The Chat object — Chat","text":"data frame tokens column provides number input tokens used user turns number output tokens used assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_tokens(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"The Chat object — Chat","text":"cost chat","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_cost(include = c(\"all\", \"last\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include default, \"\", gives total cumulative cost chat. Alternatively, use \"last\" get cost just recent turn.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"The Chat object — Chat","text":"last turn returned assistant.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"The Chat object — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-","dir":"Reference","previous_headings":"","what":"Method chat_structured()","title":"The Chat object — Chat","text":"Extract structured data","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_structured(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. typically text want extract data , can omitted data obvious existing conversation. type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-async-","dir":"Reference","previous_headings":"","what":"Method chat_structured_async()","title":"The Chat object — Chat","text":"Extract structured data, asynchronously. Returns promise resolves object matching type specification.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_structured_async(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"The Chat object — Chat","text":"Submit input chatbot, receive promise resolves response . Returns promise resolves string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_async(..., tool_mode = c(\"concurrent\", \"sequential\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"The Chat object — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$stream(..., stream = c(\"text\", \"content\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"The Chat object — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$stream_async(   ...,   tool_mode = c(\"concurrent\", \"sequential\"),   stream = c(\"text\", \"content\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"The Chat object — Chat","text":"Register tool (R function) chatbot can use. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$register_tool(tool)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tool tool definition created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-register-tools-","dir":"Reference","previous_headings":"","what":"Method register_tools()","title":"The Chat object — Chat","text":"Register list tools. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$register_tools(tools)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-provider-","dir":"Reference","previous_headings":"","what":"Method get_provider()","title":"The Chat object — Chat","text":"Get underlying provider object. expert use .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_provider()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tools-","dir":"Reference","previous_headings":"","what":"Method get_tools()","title":"The Chat object — Chat","text":"Retrieve list registered tools.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_tools()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-tools-","dir":"Reference","previous_headings":"","what":"Method set_tools()","title":"The Chat object — Chat","text":"Sets available tools. expert use ; users use register_tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_tools(tools)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-request-","dir":"Reference","previous_headings":"","what":"Method on_tool_request()","title":"The Chat object — Chat","text":"Register callback tool request event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$on_tool_request(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"callback function called tool request event occurs, must request argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-result-","dir":"Reference","previous_headings":"","what":"Method on_tool_result()","title":"The Chat object — Chat","text":"Register callback tool result event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$on_tool_result(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"callback function called tool result event occurs, must result argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-extract-data-","dir":"Reference","previous_headings":"","what":"Method extract_data()","title":"The Chat object — Chat","text":"Deprecated favour $chat_structured().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$extract_data(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... See $chat_structured()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-24","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$extract_data_async(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-20","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... See $chat_structured_async()","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"The Chat object — Chat","text":"objects class cloneable method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-25","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-21","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Chat object — Chat","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat(\"Tell me a funny joke\") #> Why did the scarecrow win an award?   #> Because he was outstanding in his field! 🌾😄"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"Use functions writing package extends ellmer need customise methods various types content. normal use, see content_image_url() friends. ellmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents types content can either sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user). object automatically created value returned calling tool() function. Alternatively, expert users can return ContentToolResult tool() function include additional data customize display result.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = stop(\"Required\"))  ContentImage()  ContentImageRemote(url = stop(\"Required\"), detail = \"\")  ContentImageInline(type = stop(\"Required\"), data = NULL)  ContentToolRequest(   id = stop(\"Required\"),   name = stop(\"Required\"),   arguments = list(),   tool = NULL )  ContentToolResult(value = NULL, error = NULL, extra = list(), request = NULL)  ContentThinking(thinking = stop(\"Required\"), extra = list())  ContentPDF(   type = stop(\"Required\"),   data = stop(\"Required\"),   filename = stop(\"Required\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result). Automatically managed ellmer. name Function name arguments Named list arguments call function . tool ellmer automatically matches tool request tools defined chatbot. NULL, request match defined tool. value results calling tool function, succeeded. error error message, string, error condition thrown result failure calling tool function. Must NULL tool call successful. extra Additional data. request ContentToolRequest associated tool result, automatically added ellmer evaluating tool call. thinking text thinking output. filename File name, used identify PDF.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content types received from and sent to a chatbot — Content","text":"S7 objects inherit Content","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content() #> <ellmer::Content> ContentText(\"Tell me a joke\") #> <ellmer::ContentText> #>  @ text: chr \"Tell me a joke\" ContentImageRemote(\"https://www.r-project.org/Rlogo.png\") #> <ellmer::ContentImageRemote> #>  @ url   : chr \"https://www.r-project.org/Rlogo.png\" #>  @ detail: chr \"\" ContentToolRequest(id = \"abc\", name = \"mean\", arguments = list(x = 1:5)) #> <ellmer::ContentToolRequest> #>  @ id       : chr \"abc\" #>  @ name     : chr \"mean\" #>  @ arguments:List of 1 #>  .. $ x: int [1:5] 1 2 3 4 5 #>  @ tool     : NULL"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = stop(\"Required\"),   model = stop(\"Required\"),   base_url = stop(\"Required\"),   params = list(),   extra_args = list(),   extra_headers = character(0) )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"name Name provider. model Name model. base_url base URL API. params list standard parameters created params(). extra_args Arbitrary extra arguments included request body. extra_headers Arbitrary extra headers added request.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chatbot provider — Provider","text":"S7 Provider object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = \"CoolModels\",   model = \"my_model\",   base_url = \"https://cool-models.com\" ) #> <ellmer::Provider> #>  @ name         : chr \"CoolModels\" #>  @ model        : chr \"my_model\" #>  @ base_url     : chr \"https://cool-models.com\" #>  @ params       : list() #>  @ extra_args   : list() #>  @ extra_headers: chr(0)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user or assistant turn — Turn","title":"A user or assistant turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, ellmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user or assistant turn — Turn","text":"","code":"Turn(role, contents = list(), json = list(), tokens = c(0, 0, 0))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user or assistant turn — Turn","text":"role Either \"user\", \"assistant\", \"system\". contents list Content objects. json serialized JSON corresponding underlying data turns. Currently provided assistant. useful information returned provider ellmer otherwise expose. tokens numeric vector length 2 representing number input output tokens (respectively) used turn. Currently recorded assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A user or assistant turn — Turn","text":"S7 Turn object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A user or assistant turn — Turn","text":"","code":"Turn(role = \"user\", contents = list(ContentText(\"Hello, world!\"))) #> <Turn: user> #> Hello, world!"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":null,"dir":"Reference","previous_headings":"","what":"Type definitions for function calling and structured data extraction. — Type","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 classes provided use package devlopers extending ellmer. every day use, use type_boolean() friends.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(description = NULL, required = TRUE, type = stop(\"Required\"))  TypeEnum(description = NULL, required = TRUE, values = character(0))  TypeArray(description = NULL, required = TRUE, items = Type())  TypeJsonSchema(description = NULL, required = TRUE, json = list())  TypeObject(   description = NULL,   required = TRUE,   properties = list(),   additional_properties = FALSE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type definitions for function calling and structured data extraction. — Type","text":"description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. type Basic type name. Must one boolean, integer, number, string. values Character vector permitted values. items type array items. Can created type_ function. json JSON schema object list. properties Named list properties stored inside object. element S7 Type object.` additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 objects inheriting Type","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(type = \"boolean\") #> <ellmer::TypeBasic> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ type       : chr \"boolean\" TypeArray(items = TypeBasic(type = \"boolean\")) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"boolean\""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in one batch — batch_chat","title":"Submit multiple chats in one batch — batch_chat","text":"batch_chat() batch_chat_structured() currently work chat_openai() chat_anthropic(). use OpenAI Anthropic batch APIs allow submit multiple requests simultaneously. results can take 24 hours complete, return pay 50% less usual (note ellmer include discount pricing metadata). want get results back quickly, working different provider, may want use parallel_chat() instead. Since batched requests can take long time complete, batch_chat() requires file path used store information batch never lose work. can either set wait = FALSE simply interrupt waiting process, later, either call batch_chat() resume left call batch_chat_completed() see results ready retrieve. batch_chat() store chat responses file, can either keep around cache results, delete free disk space. API marked experimental since yet know handle errors helpful way. Fortunately seem common, ideas, please let know!","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"batch_chat(chat, prompts, path, wait = TRUE)  batch_chat_text(chat, prompts, path, wait = TRUE)  batch_chat_structured(   chat,   prompts,   path,   type,   wait = TRUE,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE )  batch_chat_completed(chat, prompts, path)"},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in one batch — batch_chat","text":"chat base chat object. prompts vector created interpolate() list character vectors. path Path file (.json extension) store state. file records hash provider, prompts, existing chat turns. attempt reuse file different, get error. wait TRUE, wait batch complete. FALSE, return NULL batch complete, can retrieve results later re-running batch_chat() batch_chat_completed() TRUE. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit multiple chats in one batch — batch_chat","text":"batch_chat(), list Chat objects, one prompt. batch_chat_test(), character vector text responses. batch_chat_structured(), single structured data object one element prompt. Typically, type object, data frame one row prompt, one column property. aboves, return NULL wait = FALSE job complete.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"if (FALSE) { # has_credentials(\"openai\") chat <- chat_openai(model = \"gpt-4.1-nano\")  # Chat ----------------------------------------------------------------------  prompts <- interpolate(\"What do people from {{state.name}} bring to a potluck dinner?\") if (FALSE) { # \\dontrun{ chats <- batch_chat(chat, prompts, path = \"potluck.json\") chats } # }  # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) if (FALSE) { # \\dontrun{ data <- batch_chat_structured(   chat = chat,   prompts = prompts,   path = \"people-data.json\",   type = type_person ) data } # } }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with any provider — chat","title":"Chat with any provider — chat","text":"generic interface chat_ functions allow pick provider model simple string.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with any provider — chat","text":"","code":"chat(   name,   ...,   system_prompt = NULL,   params = NULL,   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with any provider — chat","text":"name Provider (optionally model) name form \"provider/model\" \"provider\" (use default model provider). ... Arguments passed provider function. system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_anthropic","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Anthropic provides number chat based models Claude moniker. Note Claude Pro membership give ability call models via API; instead, need sign (pay ) developer account.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat_anthropic(   system_prompt = NULL,   params = NULL,   max_tokens = deprecated(),   model = NULL,   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   beta_headers = character(),   api_key = anthropic_key(),   api_headers = character(),   echo = NULL )  models_anthropic(   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). max_tokens Maximum number tokens generate stopping. model model use chat (defaults \"claude-sonnet-4-20250514\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_anthropic() see options. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). base_url base URL endpoint; default uses OpenAI. beta_headers Optionally, character vector beta headers opt-claude features still beta. api_key API key use authentication. generally supply directly, instead set ANTHROPIC_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-20250514\". chat$chat(\"Tell me three jokes about statisticians\") #> Here are three jokes about statisticians: #>  #> 1. **The Drowning Statistician** #> A statistician can have his head in an oven and his feet in ice, and  #> he'll say that on average, he feels fine. #>  #> 2. **The Hiring Process** #> How do you tell the difference between an introverted statistician and #> an extroverted statistician? The introverted one looks at his own  #> shoes when talking to you. The extroverted one looks at *your* shoes. #>  #> 3. **The Wedding Anniversary** #> A statistician's wife is having a baby. The statistician is pacing in  #> the hospital corridor when the nurse comes out and says,  #> \"Congratulations! You're the father of beautiful twins!\" The  #> statistician replies, \"Well, I suppose that's close to the national  #> average.\""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an AWS bedrock model — chat_aws_bedrock","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"AWS Bedrock provides number language models, including Anthropic's Claude, using Bedrock Converse API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Authentication handled {paws.common}, authentication work automatically, need follow advice https://www.paws-r-sdk.com/#credentials. particular, org uses AWS SSO, need run aws sso login terminal.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"chat_aws_bedrock(   system_prompt = NULL,   base_url = NULL,   model = NULL,   profile = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  models_aws_bedrock(profile = NULL, base_url = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. model model use chat (defaults \"anthropic.claude-3-5-sonnet-20240620-v1:0\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_models_aws_bedrock() see options. . ellmer provides default model, guarantee access , need specify model can. using cross-region inference, need use inference profile ID, e.g. model=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\". profile AWS profile use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. useful arguments include:   api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":"api_args = list(   inferenceConfig = list(     maxTokens = 100,     temperature = 0.7,     topP = 0.9,     topK = 20   ) )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage chat <- chat_aws_bedrock() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"chat_azure_openai() supports API keys credentials parameter, also makes use : Azure service principals (AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET environment variables set). Interactive Entra ID authentication, like Azure CLI. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"chat_azure_openai(   endpoint = azure_endpoint(),   model,   params = NULL,   api_version = NULL,   system_prompt = NULL,   api_key = NULL,   token = deprecated(),   credentials = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character(),   deployment_id = deprecated() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT environment variable. model deployment id model want use. params Common model parameters, usually created params(). api_version API version use. system_prompt system prompt set behavior assistant. api_key API key use authentication. generally supply directly, instead set AZURE_OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). token literal Azure token use authentication. Deprecated favour ambient Azure credentials explicit credentials argument. credentials list authentication headers pass httr2::req_headers(), function returns , NULL use token api_key generate headers instead. escape hatch allows users incorporate Azure credentials generated packages ellmer, manage lifetime credentials need refreshed. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call. deployment_id Use model instead.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_azure_openai(deployment_id = \"gpt-4o-mini\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on CloudFlare — chat_cloudflare","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Cloudflare works AI hosts variety open-source AI models. use Cloudflare API, must Account ID Access Token, can obtain following instructions.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Tool calling appear work. Images appear work.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"chat_cloudflare(   account = cloudflare_account(),   system_prompt = NULL,   params = NULL,   api_key = cloudflare_key(),   model = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"account Cloudflare account ID. Taken CLOUDFLARE_ACCOUNT_ID env var, defined. system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set HUGGINGFACE_API_KEY environment variable. model model use chat (defaults \"meta-llama/Llama-3.3-70b-instruct-fp8-fast\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_cloudflare() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Please use chat_snowflake() instead appears Snowflake putting efforts. Chat LLM-powered Snowflake Cortex Analyst.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"chat_cortex_analyst() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Unlike comparable model APIs, Cortex take system prompt. Instead, caller must provide \"semantic model\" describing available tables, meaning, verified queries can run starting point. semantic model can passed YAML string via reference existing file Snowflake Stage. Note Cortex support multi-turn, remember previous messages. support registering tools, attempting result error. See chat_snowflake() chat general-purpose models hosted Snowflake.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"","code":"chat_cortex_analyst(   account = snowflake_account(),   credentials = NULL,   model_spec = NULL,   model_file = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model_spec semantic model specification, NULL using model_file instead. model_file Path semantic model file stored Snowflake Stage, NULL using model_spec instead. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Databricks — chat_databricks","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks provides ---box access number foundation models can also serve gateway external models hosted third party.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Databricks — chat_databricks","text":"chat_databricks() picks ambient Databricks credentials subset Databricks client unified authentication model. Specifically, supports: Personal access tokens Service principals via OAuth (OAuth M2M) User account via OAuth (OAuth U2M) Authentication via Databricks CLI Posit Workbench-managed credentials Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks models support images, support structured outputs tool calls models.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"chat_databricks(   workspace = databricks_workspace(),   system_prompt = NULL,   model = NULL,   token = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Databricks — chat_databricks","text":"workspace URL Databricks workspace, e.g. \"https://example.cloud.databricks.com\". use value environment variable DATABRICKS_HOST, set. system_prompt system prompt set behavior assistant. model model use chat (defaults \"databricks-claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Available foundational models include: databricks-claude-3-7-sonnet (default) databricks-mixtral-8x7b-instruct databricks-meta-llama-3-1-70b-instruct databricks-meta-llama-3-1-405b-instruct token authentication token Databricks workspace, NULL use ambient credentials. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_databricks() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on DeepSeek — chat_deepseek","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Sign https://platform.deepseek.com.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Structured data extraction supported. Images supported.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"chat_deepseek(   system_prompt = NULL,   base_url = \"https://api.deepseek.com\",   api_key = deepseek_key(),   model = NULL,   params = NULL,   seed = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses DeepSeek. api_key API key use authentication. generally supply directly, instead set DEEPSEEK_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"deepseek-chat\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_deepseek() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub Models hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub Models marketplace. GitHub also suports Azure AI Inference SDK, can use setting base_url \"https://models.inference.ai.azure.com/\". endpoint used ellmer v0.3.0 earlier.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   base_url = \"https://models.github.ai/inference/\",   api_key = github_key(),   model = NULL,   params = NULL,   seed = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )  models_github(base_url = \"https://models.github.ai/\", api_key = github_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GITHUB_PAT environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_github() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Google's AI offering broken two parts: Gemini Vertex AI. enterprises likely use Vertex AI, individuals likely use Gemini. Use google_upload() upload files (PDFs, images, video, audio, etc.)","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"default, chat_google_gemini() use Google's default application credentials API key provided. requires gargle package. can also pick viewer-based credentials Posit Connect. turn requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"chat_google_gemini(   system_prompt = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  chat_google_vertex(   location,   project_id,   system_prompt = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  models_google_gemini(   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL )  models_google_vertex(location, project_id)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). Gemini, can alternatively set GEMINI_API_KEY. model model use chat (defaults \"gemini-2.5-flash\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_google_gemini() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. location Location, e.g. us-east1, -central1, africa-south1 global. project_id Project ID.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. function lightweight wrapper around chat_openai() defaults tweaked groq.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Groq — chat_groq","text":"groq currently support structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   params = NULL,   seed = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GROQ_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"llama3-8b-8192\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Groq — chat_groq","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_groq() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Hugging Face hosts variety open-source proprietary AI models available via Inference API. use Hugging Face API, must Access Token, can obtain Hugging Face account (ensure least \"Make calls Inference Providers\" \"Make calls Inference Endpoints\" checked). function lightweight wrapper around chat_openai(), defaults adjusted Hugging Face.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"models support chat interface parts , example google/gemma-2-2b-support system prompt. need carefully choose model.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"chat_huggingface(   system_prompt = NULL,   params = NULL,   api_key = hf_key(),   model = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set HUGGINGFACE_API_KEY environment variable. model model use chat (defaults \"meta-llama/Llama-3.1-8B-Instruct\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_huggingface() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Get API key https://console.mistral.ai/api-keys.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Tool calling unstable. Images require model supports images.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"chat_mistral(   system_prompt = NULL,   params = NULL,   api_key = mistral_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set MISTRAL_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"mistral-large-latest\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_mistral() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local Ollama model — chat_ollama","title":"Chat with a local Ollama model — chat_ollama","text":"use chat_ollama() first download install Ollama. install models either command line (e.g. ollama pull llama3.1) within R using ollamar (e.g. ollamar::pull(\"llama3.1\")). function lightweight wrapper around chat_openai() defaults tweaked ollama.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a local Ollama model — chat_ollama","text":"Tool calling supported streaming (.e. echo \"text\" \"\") Models can use 2048 input tokens, way get use , except creating custom model different default. Tool calling generally seems quite weak, least models tried .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   base_url = Sys.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),   model,   seed = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_key = NULL,   api_headers = character() )  models_ollama(base_url = \"http://localhost:11434\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local Ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. model model use chat. Use models_ollama() see options. seed Optional integer seed ChatGPT uses try make output reproducible. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_key Ollama require API key local usage cases need provide api_key. However, accessing Ollama instance hosted behind reverse proxy secured endpoint enforces bearer‐token authentication, can set api_key (OLLAMA_API_KEY environment variable). api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a local Ollama model — chat_ollama","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_ollama(model = \"llama3.2\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"OpenAI provides number chat-based models, mostly ChatGPT brand. Note ChatGPT Plus membership grant access API. need sign developer account (pay ) developer platform.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   base_url = Sys.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"),   api_key = openai_key(),   model = NULL,   params = NULL,   seed = lifecycle::deprecated(),   api_args = list(),   api_headers = character(),   echo = c(\"none\", \"output\", \"all\") )  models_openai(base_url = \"https://api.openai.com/v1\", api_key = openai_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"gpt-4.1\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_openai() see options. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") #> - **Printing behavior**:   #>   - Tibbles print a preview (first 10 rows and columns that fit on  #> screen), whereas data frames print ALL data unless told otherwise. #>  #> - **Data type preservation:**   #>   - Tibbles never convert strings to factors by default, whereas  #> data.frames (in base R <4.0) automatically convert strings to factors  #> unless instructed not to. #>  #> - **Subsetting output:**   #>   - Tibbles always return another tibble when subsetting with `[ ]`,  #> while data frames may simplify the output (e.g., as a vector)  #> depending on usage. #>  #> - **Column name handling:**   #>   - Tibbles allow non-syntactic column names (e.g., names with spaces  #> or special characters) without backticks, whereas data frames require  #> syntactic names or backticks. #>  #> - **Partial matching:**   #>   - Tibbles do **not** support partial column name matching, while  #> data frames do. #>  #> - **Enhanced printing:**   #>   - Tibbles display data types next to column names when printed; data #> frames do not. #>  #> - **Creation:**   #>   - Tibbles are created with `tibble()` or `as_tibble()`, while data  #> frames use `data.frame()`. #>  #> - **Row names:**   #>   - Tibbles do not support row names; data frames do. #>  #> - **Package:**   #>   - Tibbles are part of the tidyverse (`tibble` package); data frames  #> are a base R structure.  chat$chat(\"Tell me three funny jokes about statisticians\") #> Sure! Here are three funny jokes about statisticians: #>  #> 1. **Why did the statistician bring a ladder to the bar?**   #>    Because he heard the drinks were on the house! #>  #> 2. **How do you tell the difference between a statistician and an  #> accountant?**   #>    Ask them to calculate the average salary: the accountant will give  #> you the mean, the statistician will ask “what do you want it to be?” #>  #> 3. **Why do statisticians love the jungle?**   #>    Because of all the *sample* trees!"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Sign https://openrouter.ai. Support features depends underlying model use; see https://openrouter.ai/models details.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"chat_openrouter(   system_prompt = NULL,   api_key = openrouter_key(),   model = NULL,   seed = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"system_prompt system prompt set behavior assistant. api_key API key use authentication. generally supply directly, instead set OPENROUTER_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openrouter() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. Perplexity AI platform running LLMs capable searching web real-time help answer questions information may available model trained. function lightweight wrapper around chat_openai() defaults tweaked Perplexity AI.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PERPLEXITY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"llama-3.1-sonar-small-128k-online\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_perplexity() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on PortkeyAI — chat_portkey","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"PortkeyAI provides interface (AI Gateway) connect Universal API variety LLMs providers single endpoint.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"API keys together configurations LLM providers stored inside Portkey application.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"chat_portkey(   system_prompt = NULL,   base_url = \"https://api.portkey.ai/v1\",   api_key = portkey_key(),   virtual_key = portkey_virtual_key(),   model = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )  models_portkey(   base_url = \"https://api.portkey.ai/v1\",   api_key = portkey_key(),   virtual_key = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PORTKEY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). virtual_key virtual identifier storing LLM provider's API key. See documentation. Can read PORTKEY_VIRTUAL_KEY environment variable. model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_openai() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_portkey(virtual_key = Sys.getenv(\"PORTKEY_VIRTUAL_KEY\")) chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Snowflake — chat_snowflake","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Snowflake provider allows interact LLM models available Cortex LLM REST API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"chat_snowflake() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Note Snowflake-hosted models support images. See chat_cortex_analyst() chat Snowflake Cortex Analyst rather general-purpose model.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"chat_snowflake(   system_prompt = NULL,   account = snowflake_account(),   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"system_prompt system prompt set behavior assistant. account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model model use chat (defaults \"claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"if (FALSE) { # has_credentials(\"snowflake\") chat <- chat_snowflake() chat$chat(\"Tell me a joke in the form of a SQL query.\") }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted by vLLM — chat_vllm","title":"Chat with a model hosted by vLLM — chat_vllm","text":"vLLM open source library provides efficient convenient LLMs model server. can use chat_vllm() connect endpoints powered vLLM.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"chat_vllm(   base_url,   system_prompt = NULL,   model,   seed = NULL,   params = NULL,   api_args = list(),   api_key = vllm_key(),   echo = NULL,   api_headers = character() )  models_vllm(base_url, api_key = vllm_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted by vLLM — chat_vllm","text":"base_url base URL endpoint; default uses OpenAI. system_prompt system prompt set behavior assistant. model model use chat. Use models_vllm() see options. seed Optional integer seed ChatGPT uses try make output reproducible. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_key API key use authentication. generally supply directly, instead set VLLM_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted by vLLM — chat_vllm","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_vllm(\"http://my-vllm.com\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode images for chat input — content_image_url","title":"Encode images for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode images for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode images for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode images for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode images for chat input — content_image_url","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openai() chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"ellmer\")) )  plot(waiting ~ eruptions, data = faithful) chat <- chat_openai() chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode PDFs content for chat input — content_pdf_file","title":"Encode PDFs content for chat input — content_pdf_file","text":"functions used prepare PDFs input chatbot. content_pdf_url() function used provide URL PDF file, content_pdf_file() used local PDF files. providers support PDF input, check documentation provider using.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode PDFs content for chat input — content_pdf_file","text":"","code":"content_pdf_file(path)  content_pdf_url(url)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode PDFs content for chat input — content_pdf_file","text":"path, url Path URL PDF file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode PDFs content for chat input — content_pdf_file","text":"ContentPDF object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":null,"dir":"Reference","previous_headings":"","what":"Record and replay content — contents_record","title":"Record and replay content — contents_record","text":"generic functions can use convert Turn/Content objects easily serializable representations (.e. lists atomic vectors). contents_record() accepts Turn Content return simple list. contents_replay() takes output contents_record() returns Turn Content object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Record and replay content — contents_record","text":"","code":"contents_record(x)  contents_replay(x, tools = list(), .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Record and replay content — contents_record","text":"x Turn Content object serialize; serialized object replay. tools named list tools .envir environment look class definitions. Used recorded objects include classes extend Turn Content ellmer package .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Format contents into a textual representation — contents_text","title":"Format contents into a textual representation — contents_text","text":"generic functions can use convert Turn contents Content objects textual representations. contents_text() minimal includes ContentText objects output. contents_markdown() returns text content (assumes markdown convert ) plus markdown representations images content types. contents_html() returns text content, converted markdown HTML commonmark::markdown_html(), plus HTML representations images content types. content types continue grow change ellmer evolves support providers providers add content types.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format contents into a textual representation — contents_text","text":"","code":"contents_text(content, ...)  contents_html(content, ...)  contents_markdown(content, ...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format contents into a textual representation — contents_text","text":"content Turn Content object converted text. contents_markdown() also accepts Chat instances turn entire conversation history markdown text. ... Additional arguments passed methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format contents into a textual representation — contents_text","text":"string text, markdown HTML.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format contents into a textual representation — contents_text","text":"","code":"turns <- list(   Turn(\"user\", contents = list(     ContentText(\"What's this image?\"),     content_image_url(\"https://placehold.co/200x200\")   )),   Turn(\"assistant\", \"It's a placeholder image.\") )  lapply(turns, contents_text) #> [[1]] #> [1] \"What's this image?\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  lapply(turns, contents_markdown) #> [[1]] #> [1] \"What's this image?\\n\\n![](https://placehold.co/200x200)\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  if (rlang::is_installed(\"commonmark\")) {   contents_html(turns[[1]]) } #> [1] \"<p>What's this image?<\/p>\\n\\n<img src=\\\"https://placehold.co/200x200\\\">\""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right call tool(). function helps documented functions extracting function's R documentation using LLM generate tool() call. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful roxygen2 comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(   topic,   chat = NULL,   model = deprecated(),   echo = interactive(),   verbose = FALSE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. chat Chat object used generate output. NULL (default) uses chat_openai(). model lifecycle::badge(\"deprecated\") Formally used defining model used chat. Now supply chat instead. echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\")   create_tool_def(\"rnorm\", chat = chat_azure_openai()) } # }"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"deprecated-in-v-","dir":"Reference","previous_headings":"","what":"Deprecated in v0.2.0","title":"Deprecated functions — deprecated","text":"chat_azure() renamed chat_azure_openai(). chat_bedrock() renamed chat_aws_bedrock(). chat_claude() renamed chat_anthropic(). chat_gemini() renamed chat_google_gemini().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"deprecated-in-v--1","dir":"Reference","previous_headings":"","what":"Deprecated in v0.1.1","title":"Deprecated functions — deprecated","text":"chat_cortex() renamed v0.1.1 chat_cortex_analyst() distinguish general-purpose Snowflake Cortex chat function, chat_snowflake().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated functions — deprecated","text":"","code":"chat_cortex(...)  chat_azure(...)  chat_bedrock(...)  chat_claude(...)  chat_gemini(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated functions — deprecated","text":"... Additional arguments passed deprecated function replacement.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ellmer: Chat with Large Language Models — ellmer-package","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Chat large language models range providers including 'Claude' https://claude.ai, 'OpenAI' https://chatgpt.com, . Supports streaming, asynchronous calls, tool calling, structured data extraction.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co (ORCID) Authors: Joe Cheng Aaron Jacobs Garrick Aden-Buie garrick@posit.co (ORCID) Barret Schloerke barret@posit.co (ORCID) contributors: Posit Software, PBC (03wc8by49) [copyright holder, funder]","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file to gemini — google_upload","title":"Upload a file to gemini — google_upload","text":"function uploads file waits Gemini finish processing can immediately use prompt. experimental currently Gemini specific, expect providers evolve similar feature future. Uploaded files automatically deleted 2 days. file must less 2 GB can upload total 20 GB. ellmer currently provide way delete files early; please file issue useful .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file to gemini — google_upload","text":"","code":"google_upload(   path,   base_url = \"https://generativelanguage.googleapis.com/\",   api_key = NULL,   mime_type = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file to gemini — google_upload","text":"path Path file upload. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). Gemini, can alternatively set GEMINI_API_KEY. mime_type Optionally, specify mime type file. specified, guesses file extension.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a file to gemini — google_upload","text":"<ContentUploaded> object can passed $chat().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a file to gemini — google_upload","text":"","code":"if (FALSE) { # \\dontrun{ file <- google_upload(\"path/to/file.pdf\")  chat <- chat_google_gemini() chat$chat(file, \"Give me a three paragraph summary of this PDF\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Are credentials avaiable? — has_credentials","title":"Are credentials avaiable? — has_credentials","text":"Used examples/testing.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Are credentials avaiable? — has_credentials","text":"","code":"has_credentials(provider)"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Are credentials avaiable? — has_credentials","text":"provider Provider name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt: interpolate() works string. interpolate_file() works file. interpolate_package() works file insts/prompt directory package. Compared glue, dynamic values wrapped {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())  interpolate_package(package, path, ..., .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md). package Package name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helpers for interpolating data into prompts — interpolate","text":"{glue} string.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a live chat application — live_console","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_anthropic() live_console(chat) live_browser(chat) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in parallel — parallel_chat","title":"Submit multiple chats in parallel — parallel_chat","text":"multiple prompts, can submit parallel. typically considerably faster submitting sequence, especially Gemini OpenAI. using chat_openai() chat_anthropic() willing wait longer, might want use batch_chat() instead, comes 50% discount return taking 24 hours.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"parallel_chat(chat, prompts, max_active = 10, rpm = 500)  parallel_chat_text(chat, prompts, max_active = 10, rpm = 500)  parallel_chat_structured(   chat,   prompts,   type,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE,   max_active = 10,   rpm = 500 )"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in parallel — parallel_chat","text":"chat base chat object. prompts vector created interpolate() list character vectors. max_active maximum number simultaneous requests send. chat_anthropic(), note number active connections limited primarily output tokens per minute limit (OTPM) estimated max_tokens parameter, defaults 4096. means usage tier limits 16,000 OTPM, either set max_active = 4 (16,000 / 4096) decrease number active connections use params() chat_anthropic() decrease max_tokens. rpm Maximum number requests per minute. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit multiple chats in parallel — parallel_chat","text":"parallel_chat(), list Chat objects, one prompt. parallel_chat_text(), character vector text responses. parallel_chat_structured(), single structured data object one element prompt. Typically, type object, data frame one row prompt, one column property.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\".  # Chat ---------------------------------------------------------------------- country <- c(\"Canada\", \"New Zealand\", \"Jamaica\", \"United States\") prompts <- interpolate(\"What's the capital of {{country}}?\") parallel_chat(chat, prompts) #> [[1]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=13/10 $0.00> #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of Canada? #> ── assistant [10] ───────────────────────────────────────────────────── #> The capital of Canada is **Ottawa**. #>  #> [[2]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=14/11 $0.00> #> ── user [14] ────────────────────────────────────────────────────────── #> What's the capital of New Zealand? #> ── assistant [11] ───────────────────────────────────────────────────── #> The capital of New Zealand is **Wellington**. #>  #> [[3]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=13/10 $0.00> #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of Jamaica? #> ── assistant [10] ───────────────────────────────────────────────────── #> The capital of Jamaica is **Kingston**. #>  #> [[4]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=14/12 $0.00> #> ── user [14] ────────────────────────────────────────────────────────── #> What's the capital of United States? #> ── assistant [12] ───────────────────────────────────────────────────── #> The capital of the United States is Washington, D.C. #>   # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) parallel_chat_structured(chat, prompts, type_person) #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard model parameters — params","title":"Standard model parameters — params","text":"helper function makes easier create list parameters used across many models. parameter names automatically standardised included correctly place API call. Note parameters supported given provider generate warning, error. allows use set parameters across multiple providers.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard model parameters — params","text":"","code":"params(   temperature = NULL,   top_p = NULL,   top_k = NULL,   frequency_penalty = NULL,   presence_penalty = NULL,   seed = NULL,   max_tokens = NULL,   log_probs = NULL,   stop_sequences = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard model parameters — params","text":"temperature Temperature sampling distribution. top_p cumulative probability token selection. top_k number highest probability vocabulary tokens keep. frequency_penalty Frequency penalty generated tokens. presence_penalty Presence penalty generated tokens. seed Seed random number generator. max_tokens Maximum number tokens generate. log_probs Include log probabilities output? stop_sequences character vector tokens stop generation . ... Additional named parameters send provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session. price shown known.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Report on token usage in the current session — token_usage","text":"data frame","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage() #>    provider                    model input output cached_input price #> 1    OpenAI                  gpt-4.1   846    538            0 $0.01 #> 2 Anthropic claude-sonnet-4-20250514    14    179            0 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — tool","title":"Define a tool — tool","text":"Annotate function use tool calls, providing name, description, type definition arguments. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — tool","text":"","code":"tool(   fun,   description,   ...,   arguments = list(),   name = NULL,   convert = TRUE,   annotations = list(),   .name = deprecated(),   .description = deprecated(),   .convert = deprecated(),   .annotations = deprecated() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — tool","text":"fun function invoked tool called. return value function sent back chatbot. Expert users can customize tool result returning ContentToolResult object. description detailed description function . Generally, information can provide , better. ... Use arguments instead. arguments named list defines arguments accepted function. element created type_*() function (NULL want LLM use argument). name name function. can omitted fun existing function (.e. defined inline). convert JSON inputs automatically convert R data type equivalents? Defaults TRUE. annotations Additional properties describe tool behavior. Usually created tool_annotations(), can find description annotation properties recommended Model Context Protocol. .name, .description, .convert, .annotations Please switch non-prefixed equivalents.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a tool — tool","text":"S7 ToolDef object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ellmer-","dir":"Reference","previous_headings":"","what":"ellmer 0.3.0","title":"Define a tool — tool","text":"ellmer 0.3.0, definition tool() function changed quite bit. make easier update old versions, can use LLM following system prompt","code":"Help the user convert an ellmer 0.2.0 and earlier tool definition into a ellmer 0.3.0 tool definition. Here's what changed:  * All arguments, apart from the first, should be named, and the argument   names no longer use `.` prefixes. The argument order should be function,   name (as a string), description, then arguments, then anything  * Previously `arguments` was passed as `...`, so all type specifications   should now be moved into a named list and passed to the `arguments`   argument. It can be omitted if the function has no arguments.  ```R # old tool(   add,   \"Add two numbers together\"   x = type_number(),   y = type_number() )  # new tool(   add,   name = \"add\",   description = \"Add two numbers together\",   arguments = list(     x = type_number(),     y = type_number()   ) ) ```  Don't respond; just let the user provide function calls to convert."},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — tool","text":"","code":"# First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- tool(   rnorm,   description = \"Draw numbers from a random normal distribution\",   arguments = list(     n = type_integer(\"The number of observations. Must be a positive integer.\"),     mean = type_number(\"The mean value of the distribution.\"),     sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\")   ) ) tool_rnorm(n = 5, mean = 0, sd = 1) #> [1] -1.400043517  0.255317055 -2.437263611 -0.005571287  0.621552721  chat <- chat_openai() #> Using model = \"gpt-4.1\". # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"Give me five numbers from a random normal distribution.\") #> Here are five numbers from a random normal distribution (mean = 0,  #> standard deviation = 1): #>  #> 1. 1.1484 #> 2. -1.8218 #> 3. -0.2473 #> 4. -0.2442 #> 5. -0.2827  # Look at the chat history to see how tool calling works: chat #> <Chat OpenAI/gpt-4.1 turns=4 tokens=248/83 $0.00> #> ── user [96] ────────────────────────────────────────────────────────── #> Give me five numbers from a random normal distribution. #> ── assistant [22] ───────────────────────────────────────────────────── #> [tool request (call_i23r8dfi9bjtZStFRX69Zz40)]: rnorm(n = 5L, mean = 0L, sd = 1L) #> ── user [34] ────────────────────────────────────────────────────────── #> [tool result  (call_i23r8dfi9bjtZStFRX69Zz40)]: [1.1484,-1.8218,-0.2473,-0.2442,-0.2827] #> ── assistant [61] ───────────────────────────────────────────────────── #> Here are five numbers from a random normal distribution (mean = 0, standard deviation = 1): #>  #> 1. 1.1484 #> 2. -1.8218 #> 3. -0.2473 #> 4. -0.2442 #> 5. -0.2827 # Assistant sends a tool request which is evaluated locally and # results are sent back in a tool result."},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":null,"dir":"Reference","previous_headings":"","what":"Tool annotations — tool_annotations","title":"Tool annotations — tool_annotations","text":"Tool annotations additional properties , passed .annotations argument tool(), provide additional information tool behavior. information can used display users, example Shiny app another user interface. annotations tool_annotations() drawn Model Context Protocol considered hints. Tool authors use annotations communicate tool properties, users note annotations guaranteed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tool annotations — tool_annotations","text":"","code":"tool_annotations(   title = NULL,   read_only_hint = NULL,   open_world_hint = NULL,   idempotent_hint = NULL,   destructive_hint = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tool annotations — tool_annotations","text":"title human-readable title tool. read_only_hint TRUE, tool modify environment. open_world_hint TRUE, tool may interact \"open world\" external entities. FALSE, tool's domain interaction closed. example, world web search tool open, world memory tool . idempotent_hint TRUE, calling tool repeatedly arguments additional effect environment. (meaningful read_only_hint FALSE.) destructive_hint TRUE, tool may perform destructive updates environment, otherwise performs additive updates. (meaningful read_only_hint FALSE.) ... Additional named parameters include tool annotations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tool annotations — tool_annotations","text":"list tool annotations.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tool annotations — tool_annotations","text":"","code":"# See ?tool() for a full example using this function. # We're creating a tool around R's `rnorm()` function to allow the chatbot to # generate random numbers from a normal distribution. tool_rnorm <- tool(   rnorm,   # Describe the tool function to the LLM   .description = \"Drawn numbers from a random normal distribution\",   # Describe the parameters used by the tool function   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\"),   # Tool annotations optionally provide additional context to the LLM   .annotations = tool_annotations(     title = \"Draw Random Normal Numbers\",     read_only_hint = TRUE, # the tool does not modify any state     open_world_hint = FALSE # the tool does not interact with the outside world   ) ) #> Warning: The `...` argument of `tool()` is deprecated as of ellmer 0.3.0. #> ℹ Please use the `arguments` argument instead. #> Warning: The `.description` argument of `tool()` is deprecated as of ellmer #> 0.3.0. #> ℹ Please use the `description` argument instead. #> Warning: The `.annotations` argument of `tool()` is deprecated as of ellmer #> 0.3.0. #> ℹ Please use the `annotations` argument instead."},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":null,"dir":"Reference","previous_headings":"","what":"Reject a tool call — tool_reject","title":"Reject a tool call — tool_reject","text":"Throws error reject tool call. tool_reject() can used within tool function indicate tool call processed. tool_reject() can also called Chat$on_tool_request() callback. used callback, tool call rejected tool function invoked. example utils::askYesNo() used ask user permission accessing current working directory. happens directly tool function appropriate write tool definition know exactly called.   can achieve similar experience tools written others using tool_request callback. next example, imagine tool provided third-party package. example implements simple menu ask user consent running  tool.","code":"chat <- chat_openai(model = \"gpt-4.1-nano\")  list_files <- function() {   allow_read <- utils::askYesNo(     \"Would you like to allow access to your current directory?\"   )   if (isTRUE(allow_read)) {     dir(pattern = \"[.](r|R|csv)$\")   } else {     tool_reject()   } }  chat$register_tool(tool(   list_files,   \"List files in the user's current directory\" ))  chat$chat(\"What files are available in my current directory?\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) no #> #> Error: Tool call rejected. The user has chosen to disallow the tool #' call. #> It seems I am unable to access the files in your current directory right now. #> If you can tell me what specific files you're looking for or if you can #' provide #> the list, I can assist you further.  chat$chat(\"Try again.\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) yes #> #> app.R #> #> data.csv #> The files available in your current directory are \"app.R\" and \"data.csv\". packaged_list_files_tool <- tool(   function() dir(pattern = \"[.](r|R|csv)$\"),   \"List files in the user's current directory\" )  chat <- chat_openai(model = \"gpt-4.1-nano\") chat$register_tool(packaged_list_files_tool)  always_allowed <- c()  # ContentToolRequest chat$on_tool_request(function(request) {   if (request@name %in% always_allowed) return()    answer <- utils::menu(     title = sprintf(\"Allow tool `%s()` to run?\", request@name),     choices = c(\"Always\", \"Once\", \"No\"),     graphics = FALSE   )    if (answer == 1) {     always_allowed <<- append(always_allowed, request@name)   } else if (answer %in% c(0, 3)) {     tool_reject()   } })  # Try choosing different answers to the menu each time chat$chat(\"What files are available in my current directory?\") chat$chat(\"How about now?\") chat$chat(\"And again now?\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reject a tool call — tool_reject","text":"","code":"tool_reject(reason = \"The user has chosen to disallow the tool call.\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reject a tool call — tool_reject","text":"reason character string describing reason rejecting tool call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reject a tool call — tool_reject","text":"Throws error class ellmer_tool_reject provided reason.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Type specifications — type_boolean","title":"Type specifications — type_boolean","text":"functions specify object types way chatbots understand used tool calling structured data extraction. names based JSON schema, APIs expect behind scenes. translation R concepts types fairly straightforward. type_boolean(), type_integer(), type_number(), type_string() represent scalars. equivalent length-1 logical, integer, double, character vectors (respectively). type_enum() equivalent length-1 factor; string can take specified values. type_array() equivalent vector R. can use represent atomic vector: e.g. type_array(type_boolean()) equivalent logical vector type_array(type_string()) equivalent character vector). can also use represent list complicated types every element type (R base equivalent ), e.g. type_array(type_array(type_string())) represents list character vectors. type_object() equivalent named list R, every element must specified type. example, type_object(= type_string(), b = type_array(type_integer())) equivalent list element called string element called b integer vector. type_from_schema() allows specify full schema want get back LLM JSON schema. useful pre-defined schema want use directly without manually creating type using type_*() functions. can point file path argument provide JSON string text. schema must valid JSON schema object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type specifications — type_boolean","text":"","code":"type_boolean(description = NULL, required = TRUE)  type_integer(description = NULL, required = TRUE)  type_number(description = NULL, required = TRUE)  type_string(description = NULL, required = TRUE)  type_enum(values, description = NULL, required = TRUE)  type_array(items, description = NULL, required = TRUE)  type_object(   .description = NULL,   ...,   .required = TRUE,   .additional_properties = FALSE )  type_from_schema(text, path)"},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type specifications — type_boolean","text":"description, .description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required, .required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. values Character vector permitted values. items type array items. Can created type_ function. ... Name-type pairs defining components object must possess. .additional_properties Can object arbitrary additional properties explicitly listed? supported Claude. text JSON string. path file path JSON file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type specifications — type_boolean","text":"","code":"# An integer vector type_array(type_integer()) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"integer\"  # The closest equivalent to a data frame is an array of objects type_array(type_object(    x = type_boolean(),    y = type_string(),    z = type_number() )) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeObject> #>  .. @ description          : NULL #>  .. @ required             : logi TRUE #>  .. @ properties           :List of 3 #>  .. .. $ x: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"boolean\" #>  .. .. $ y: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"string\" #>  .. .. $ z: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"number\" #>  .. @ additional_properties: logi FALSE  # There's no specific type for dates, but you use a string with the # requested format in the description (it's not gauranteed that you'll # get this format back, but you should most of the time) type_string(\"The creation date, in YYYY-MM-DD format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The creation date, in YYYY-MM-DD format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\" type_string(\"The update date, in dd/mm/yyyy format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The update date, in dd/mm/yyyy format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-development-version","dir":"Changelog","previous_headings":"","what":"ellmer (development version)","title":"ellmer (development version)","text":"models_ollama() fixed correctly query model capabilities remote Ollama servers (#746).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-032","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.2","title":"ellmer 0.3.2","text":"CRAN release: 2025-09-03 chat_aws_bedrock(), chat_databricks(), chat_deepseek(), chat_github(), chat_groq(), chat_ollama(), chat_openrouter(), chat_perplexity(), chat_vllm() now support params argument accepts common model parameters params(). deployment_id argument chat_azure_openai() deprecated replaced model better align providers. chat_openai() now correctly maps max_tokens top_k params() OpenAI API parameters (#699).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-031","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.1","title":"ellmer 0.3.1","text":"CRAN release: 2025-08-24 chat_anthropic() drops empty assistant turns avoid API errors (#710). chat_github() now uses https://models.github.ai/inference endpoint chat() supports GitHub models format chat(\"github/openai/gpt-4.1\") (#726). chat_google_vertex() authentication fixed using broader scope (#704, @netique) chat_google_vertex() can now use global project location (#704, @netique) chat_openai() now uses OPENAI_BASE_URL, set, base_url. Similarly, chat_ollama() also uses OLLAMA_BASE_URL set (#713). contents_record() contents_replay() now record replay custom classes extend ellmer’s Turn Content classes (#689). contents_replay() now also restores tool definition ContentToolResult objects (@request@tool) (#693). chat_snowflake() now supports Privatelink accounts (#694, @robert-norberg). works Snowflake’s latest API changes (#692, @robert-norberg). models_google_vertex() works (#704, @netique) value_turn() method OpenAI providers, usage checked NULL logging tokens avoid errors streaming OpenAI-compatible services (#706, @stevegbrooks).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-030","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.0","title":"ellmer 0.3.0","text":"CRAN release: 2025-07-24","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.3.0","text":"New chat() allows chat provider using string like chat(\"anthropic\") chat(\"openai/gpt-4.1-nano\") (#361). tool() simpler specification: now specify name, description, arguments. done best deprecate old usage give clear errors, likely missed edge cases. apologize pain causes, ’m convinced going make tool usage easier clearer long run. many calls convert, ?tool contains prompt help use LLM convert (#603). also now returns function can call (/export package) (#602). type_array() type_enum() now description second argument items/values first. makes easier use common case description isn’t necessary (#610). ellmer now retries requests 3 times, controllable option(ellmer_max_tries), retry connection fails (rather just request returns transient error). default timeout, controlled option(ellmer_timeout_s), now applies initial connection phase. Together, changes make much likely ellmer requests succeed. New parallel_chat_text() batch_chat_text() make easier just get text response multiple prompts (#510). ellmer’s cost estimates considerably improved. chat_openai(), chat_google_gemini(), chat_anthropic() capture number cached input tokens. primarily useful OpenAI Gemini since offer automatic caching, yielding improved cost estimates (#466). also better source pricing data, LiteLLM. considerably expands number providers models include cost information (#659).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"bug-fixes-and-minor-improvements-0-3-0","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.3.0","text":"new ellmer_echo option controls default value echo. batch_chat_structured() provides clear messaging prompts/path/provider don’t match (#599). chat_aws_bedrock() allows set base_url() (#441). chat_aws_bedrock(), chat_google_gemini(), chat_ollama(), chat_vllm() use robust method generate model URLs base_url (#593, @benyake). chat_cortex_analyst() deprecated; please use chat_snowflake() instead (#640). chat_github() (OpenAI extensions) longer warn seed (#574). chat_google_gemini() chat_google_vertex() default Gemini 2.5 flash (#576). chat_huggingface() works much better. chat_openai() supports content_pdf_() (#650). chat_portkey() works , reads virtual API key PORTKEY_VIRTUAL_KEY env var (#588). chat_snowflake() works tool calling (#557, @atheriel). Chat$chat_structured() friends longer unnecessarily wrap type_object() chat_openai() (#671). Chat$chat_structured() suppresses tool use. need use tools structured data together, first use $chat() needed tools, $chat_structured() extract data need. Chat$chat_structured() longer requires prompt (since may obvious context) (#570). Chat$register_tool() shows message replace existing tool (#625). contents_record() contents_replay() record replay Turn related information Chat instance (#502). methods can used bookmarking within {shinychat}. models_github() lists models chat_github() (#561). models_ollama() includes capabilities column comma-separated list model capabilities (#623). parallel_chat() friends accept lists Content objects prompt (#597, @thisisnic). Tool requests show converted arguments printed (#517). tool() checks name valid (#625).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-021","dir":"Changelog","previous_headings":"","what":"ellmer 0.2.1","title":"ellmer 0.2.1","text":"CRAN release: 2025-06-03 save Chat object disk, API keys automatically redacted. means can longer easily resume chat ’ve saved disk (’ll figure future release) ensures never accidentally save secret key RDS file (#534). chat_anthropic() now defaults Claude Sonnet 4, ’ve added pricing information latest generation Claude models. chat_databricks() now picks Databricks workspace URLs set configuration file, improve compatibility Databricks CLI (#521, @atheriel). now also supports tool calling (#548, @atheriel). chat_snowflake() longer streams answers include mysterious list(type = \"text\", text = \"\") trailer (#533, @atheriel). now parses streaming outputs correctly turns (#542), supports structured ouputs (#544), standard model parameters (#545, @atheriel). chat_snowflake() chat_databricks() now default Claude Sonnet 3.7, default chat_anthropic() (#539 #546, @atheriel). type_from_schema() lets use pre-existing JSON schemas structured chats (#133, @hafen)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-020","dir":"Changelog","previous_headings":"","what":"ellmer 0.2.0","title":"ellmer 0.2.0","text":"CRAN release: 2025-05-17","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"breaking-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"ellmer 0.2.0","text":"made number refinements way ellmer converts JSON R data structures. breaking changes, although don’t expect affect much code wild. importantly, tools now invoked inputs coerced standard R data structures (#461); opt-setting convert = FALSE tool(). Additionally ellmer now converts NULL NA type_boolean(), type_integer(), type_number(), type_string() (#445), better job arrays required = FALSE (#384). chat_ functions longer turn argument. need set turns, can now use Chat$set_turns() (#427). Additionally, Chat$tokens() renamed Chat$get_tokens() returns data frame tokens, correctly aligned individual turn. print method now uses show many input/output tokens used turn (#354).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.2.0","text":"Two new interfaces help multiple chats single function call: batch_chat() batch_chat_structured() allow submit multiple chats OpenAI Anthropic’s batched interfaces. guarantee response within 24 hours, 50% price regular requests (#143). parallel_chat() parallel_chat_structured() work provider allow submit multiple chats parallel (#143). doesn’t give cost savings, ’s can much, much faster. new family functions experimental ’m 100% sure shape user interface correct, particularly pertains handling errors. google_upload() lets upload files Google Gemini Vertex AI (#310). allows work videos, PDFs, large files Gemini. models_google_gemini(), models_anthropic(), models_openai(), models_aws_bedrock(), models_ollama() models_vllm(), list available models Google Gemini, Anthropic, OpenAI, AWS Bedrock, Ollama, VLLM respectively. Different providers return different metadata guaranteed return data frame least id column (#296). possible (currently Gemini, Anthropic, OpenAI) include known token prices (per million tokens). interpolate() friends now vectorised can generate multiple prompts (e.g.) data frame inputs. also now return specially classed object custom print method (#445). New interpolate_package() makes easier interpolate prompts stored inst/prompts directory inside package (#164). chat_anthropic(), chat_azure(), chat_openai(), chat_gemini() now take params argument, coupled params() helper, makes easy specify common model parameters (like seed temperature) across providers. Support providers grow request (#280). ellmer now tracks cost input output tokens. cost displayed print Chat object, tokens_usage(), Chat$get_cost(). can also request costs parallel_chat_structured(). best accurately compute cost, treat estimate rather exact price. Unfortunately LLM providers currently make difficult figure exactly much queries cost (#203).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"provider-updates-0-2-0","dir":"Changelog","previous_headings":"","what":"Provider updates","title":"ellmer 0.2.0","text":"support three new providers: chat_huggingface() models hosted https://huggingface.co (#359, @s-spavound). chat_mistral() models hosted https://mistral.ai (#319). chat_portkey() models_portkey() models hosted https://portkey.ai (#363, @maciekbanas). also renamed (deprecation) functions make naming scheme consistent (#382, @gadenbuie): chat_azure_openai() replaces chat_azure(). chat_aws_bedrock() replaces chat_bedrock(). chat_anthropic() replaces chat_anthropic(). chat_google_gemini() replaces chat_gemini(). updated default model couple providers: chat_anthropic() uses Sonnet 3.7 (also now displays) (#336). chat_openai() uses GPT-4.1 (#512)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"developer-tooling-0-2-0","dir":"Changelog","previous_headings":"","what":"Developer tooling","title":"ellmer 0.2.0","text":"New Chat$get_provider() lets access underlying provider object (#202). Chat$chat_async() Chat$stream_async() gain tool_mode argument decide \"sequential\" \"concurrent\" tool calling. advanced feature primarily affects asynchronous tools (#488, @gadenbuie). Chat$stream() Chat$stream_async() gain support streaming additional content types generated tool call new stream argument. stream = \"content\" set, streaming response yields Content objects, including ContentToolRequest ContentToolResult objects used request return tool calls (#400, @gadenbuie). New Chat$on_tool_request() $on_tool_result() methods allow register callbacks run tool request tool result. callbacks can used implement custom logging actions tools called, without modifying tool function (#493, @gadenbuie). Chat$chat(echo = \"output\") replaces now-deprecated echo = \"text\" option. using echo = \"output\", additional output, tool requests results, shown occur. echo = \"none\", tool call failures emitted warnings (#366, @gadenbuie). ContentToolResult objects can now returned directly tool() function now includes additional information (#398 #399, @gadenbuie): extra: list additional data associated tool result shown chatbot. request: ContentToolRequest triggered tool call. ContentToolResult longer id property, instead tool call ID can retrieved request@id. also include error condition error property tool call fails (#421, @gadenbuie). ContentToolRequest gains tool property includes tool() definition request matched tool ellmer (#423, @gadenbuie). tool() gains .annotations argument can created tool_annotations() helper. Tool annotations described Model Context Protocol can used describe tool clients. (#402, @gadenbuie) New tool_reject() function can used reject tool request explanation rejection reason. tool_reject() can called within tool function Chat$on_tool_request() callback. latter case, rejecting tool call ensure tool function evaluated (#490, #493, @gadenbuie).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"minor-improvements-and-bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"ellmer 0.2.0","text":"requests now set custom User-Agent identifies requests come ellmer (#341). default timeout increased 5 minutes (#451, #321). chat_anthropic() now supports thinking content type (#396), content_image_url() (#347). gains beta_header argument opt-beta features (#339). (along chat_bedrock()) longer chokes receiving output consists whitespace (#376). Finally, chat_anthropic(max_tokens =) now deprecated favour chat_anthropic(params = ) (#280). chat_google_gemini() chat_google_vertex() gain ways authenticate. can use GEMINI_API_KEY set (@t-kalinowski, #513), authenticate Google default application credentials (including service accounts, etc) (#317, @atheriel) use viewer-based credentials running Posit Connect (#320, @atheriel). Authentication default application credentials requires {gargle} package. now also can now handle responses include citation metadata (#358). chat_ollama() now works tool() definitions optional arguments empty properties (#342, #348, @gadenbuie), now accepts api_key consults OLLAMA_API_KEY environment variable. needed local usage, enables bearer-token authentication Ollama running behind reverse proxy (#501, @gadenbuie). chat_openai(seed =) now deprecated favour chat_openai(params = ) (#280). create_tool_def() can now use Chat instance (#118, @pedrobtz). live_browser() now requires {shinychat} v0.2.0 later provides access app powers live_browser() via shinychat::chat_app(), well Shiny module easily including chat interface ellmer Chat object Shiny apps (#397, @gadenbuie). now initializes UI messages chat turns, rather replaying turns server-side (#381). Provider gains name model fields (#406). now reported print chat object used token_usage().","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-011","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.1","title":"ellmer 0.1.1","text":"CRAN release: 2025-02-06","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"lifecycle-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"ellmer 0.1.1","text":"option(ellmer_verbosity) longer supported; instead use standard httr2 verbosity functions, httr2::with_verbosity(); now support streaming data. chat_cortex() renamed chat_cortex_analyst() better disambiguate chat_snowflake() (also uses “Cortex”) (#275, @atheriel).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.1.1","text":"providers now wait 60s get complete response. can increase , e.g., option(ellmer_timeout_s = 120) (#213, #300). chat_azure(), chat_databricks(), chat_snowflake(), chat_cortex_analyst() now detect viewer-based credentials running Posit Connect (#285, @atheriel). chat_deepseek() provides support DeepSeek models (#242). chat_openrouter() provides support models hosted OpenRouter (#212). chat_snowflake() allows chatting models hosted Snowflake’s Cortex LLM REST API (#258, @atheriel). content_pdf_file() content_pdf_url() allow upload PDFs supported models. Models currently support PDFs Google Gemini Claude Anthropic. help @walkerke @andrie (#265).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.1.1","text":"Chat$get_model() returns model name (#299). chat_azure() greatly improved support Azure Entra ID. API keys now optional can pick ambient credentials Azure service principals attempt use interactive Entra ID authentication possible. broken--design token argument deprecated (handle refreshing tokens properly), new credentials argument can used custom Entra ID support needed instead (instance, ’re trying use tokens generated AzureAuth package) (#248, #263, #273, #257, @atheriel). chat_azure() now reports better error messages underlying HTTP requests fail (#269, @atheriel). now also defaults api_version = \"2024-10-21\" includes data structured data extraction (#271). chat_bedrock() now handles temporary IAM credentials better (#261, @atheriel) chat_bedrock() gains api_args argument (@billsanto, #295). chat_databricks() now handles DATABRICKS_HOST environment variable correctly whether includes HTTPS prefix (#252, @atheriel). also respects SPARK_CONNECT_USER_AGENT environment variable making requests (#254, @atheriel). chat_gemini() now defaults using gemini-2.0-flash model. print(Chat) longer wraps long lines, making easier read code bulleted lists (#246).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-010","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.0","title":"ellmer 0.1.0","text":"CRAN release: 2025-01-09 New chat_vllm() chat models served vLLM (#140). default chat_openai() model now GPT-4o. New Chat$set_turns() set turns. Chat$turns() now Chat$get_turns(). Chat$system_prompt() replaced Chat$set_system_prompt() Chat$get_system_prompt(). Async streaming async chat now event-driven use later::later_fd() wait efficiently curl socket activity (#157). New chat_bedrock() chat AWS bedrock models (#50). New chat$extract_data() uses structured data API available (tool calling otherwise) extract data structured according known type specification. can create specs functions type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object() (#31). general ToolArg() replaced specific type_*() functions. ToolDef() renamed tool. content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprobs (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. can now log low-level JSON request/response info setting options(ellmer_verbosity = 2). chat$register_tool() now takes object created Tool(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56). Databricks now supported via chat_databricks() (#152).","code":""}]
