[{"path":"https://ellmer.tidyverse.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ellmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"vocabulary","dir":"Articles","previous_headings":"","what":"Vocabulary","title":"Getting started with ellmer","text":"’ll start laying key vocab ’ll need understand LLMs. Unfortunately vocab little entangled: understand one term ’ll often know little others. ’ll start simple definitions important terms iteratively go little deeper. starts prompt, text (typically question request) send LLM. starts conversation, sequence turns alternate user prompts model responses. Inside model, prompt response represented sequence tokens, represent either individual words subcomponents word. tokens used compute cost using model measure size context, combination current prompt previous prompts responses used generate next response. ’s useful make distinction providers models. provider web API gives access one models. distinction bit subtle providers often synonymous model, like OpenAI GPT, Anthropic Claude, Google Gemini. providers, like Ollama, can host many different models, typically open source models like LLaMa Mistral. Still providers support open closed models, typically partnering company provides popular closed model. example, Azure OpenAI offers open source models OpenAI’s GPT, AWS Bedrock offers open source models Anthropic’s Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-token","dir":"Articles","previous_headings":"Vocabulary","what":"What is a token?","title":"Getting started with ellmer","text":"LLM model, like models needs way represent inputs numerically. LLMs, means need way convert words numbers. goal tokenizer. example, using GPT 4o tokenizer, string “R created?” converted 5 tokens: 5958 (“”), 673 (” ”), 460 (” R”), 5371 (” created”), 30 (“?”). can see, many simple strings can represented single token. complex strings require multiple tokens. example, string “counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”), 9477 (“volution”), 815 (“ary”). (can see various strings tokenized http://tiktokenizer.vercel.app/). ’s important rough sense text converted tokens tokens used determine cost model much context can used predict next response. average English word needs ~1.5 tokens page might require 375-400 tokens complete book might require 75,000 150,000 tokens. languages typically require tokens, (brief) LLMs trained data internet, primarily English. LLMs priced per million tokens. State art models (like GPT-4.1 Claude 3.5 sonnet) cost $2-3 per million input tokens, $10-15 per million output tokens. Cheaper models can cost much less, e.g. GPT-4.1 nanoo costs $0.10 per million input tokens $0.40 per million output tokens. Even $10 API credit give lot room experimentation, particularly cheaper models, prices likely decline model performance improves. Tokens also used measure context window, much text LLM can use generate next response. ’ll discuss shortly, context length includes full state conversation far (prompts model’s responses), means cost grow rapidly number conversational turns. ellmer, can see many tokens conversations used printing , can see total usage session token_usage(). want learn tokens tokenizers, ’d recommend watching first 20-30 minutes Let’s build GPT Tokenizer Andrej Karpathy. certainly don’t need learn build tokenizer, intro give bunch useful background knowledge help improve undersstanding LLM’s work.","code":"chat <- chat_openai(model = \"gpt-4.1\") . <- chat$chat(\"Who created R?\", echo = FALSE) chat #> <Chat OpenAI/gpt-4.1 turns=2 tokens=11/130 $0.00> #> ── user [11] ────────────────────────────────────────────────────────── #> Who created R? #> ── assistant [130] ──────────────────────────────────────────────────── #> **R** is a programming language and software environment for statistical computing and graphics. It was **created by Ross Ihaka and Robert Gentleman** at the University of Auckland, New Zealand, in the mid-1990s. #>  #> - **Ross Ihaka** and **Robert Gentleman** are often referred to as the primary authors of R. #> - The project started in 1992, and the first release was in 1995. #> - Since then, R has been developed further by the **R Core Team**, a group of statisticians and computer scientists worldwide. #>  #> **In summary:**   #> R was created by **Ross Ihaka and Robert Gentleman**.  token_usage() #>   provider   model input output price #> 1   OpenAI gpt-4.1    11    130 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-conversation","dir":"Articles","previous_headings":"Vocabulary","what":"What is a conversation?","title":"Getting started with ellmer","text":"conversation LLM takes place series HTTP requests responses: send question LLM HTTP request, sends back reply HTTP response. words, conversation consists sequence paired turns: sent prompt returned response. ’s important note request includes current user prompt, every previous user prompt model response. means : cost conversation grows quadratically number turns: want save money, keep conversations short. response affected previous prompts responses. can make converstion get stuck local optimum, ’s generally better iterate starting new conversation better prompt rather long back--forth. ellmer full control conversational history. ’s ellmer’s responsibility send previous turns conversation, ’s possible start conversation one model finish another.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-prompt","dir":"Articles","previous_headings":"Vocabulary","what":"What is a prompt?","title":"Getting started with ellmer","text":"user prompt question send model. two important prompts underlie user prompt: platform prompt, unchangeable, set model provider, affects every conversation. can see look like Anthropic, publishes core system prompts. system prompt (aka developer prompt), set create new conversation, affects every response. ’s used provide additional instructions model, shaping responses needs. example, might use system prompt ask model always respond Spanish write dependency-free base R code. can also use system prompt provide model information wouldn’t otherwise know, like details database schema, preferred ggplot2 theme color palette. OpenAI calls chain command: conflicts inconsistencies prompts, platform prompt overrides system prompt, turn overrides user prompt. use chat app like ChatGPT claude.ai can iterate user prompt. ’re programming LLMs, ’ll primarily iterate system prompt. example, ’re developing app helps user write tidyverse code, ’d work system prompt ensure user gets style code want. Writing good prompt, called prompt design, key effective use LLMs. discussed detail vignette(\"prompt-design\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with ellmer","text":"Now ’ve got basic vocab belt, ’m going fire bunch interesting potential use cases . special purpose tools might solve cases faster /cheaper, LLM allows rapidly prototype solution. can extremely valuable even end using specialised tools final product. general, recommend avoiding LLMs accuracy critical. said, still many cases use. example, even though always require manual fiddling, might save bunch time ever 80% correct solution. fact, even --good solution can still useful makes easier get started: ’s easier react something rather start scratch blank page.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"chatbots","dir":"Articles","previous_headings":"Example uses","what":"Chatbots","title":"Getting started with ellmer","text":"great place start ellmer LLMs build chatbot custom prompt. Chatbots familiar interface LLMs easy create R shinychat. ’s surprising amount value creating custom chatbot prompt stuffed useful knowledge. example: Help people use new package. , need custom prompt LLMs trained data prior package’s existence. can create surprisingly useful tool just preloading prompt README vignettes. ellmer assistant works. Build language specific prompts R /python. Shiny assistant helps build shiny apps (either R python) combining prompt gives general advice building apps prompt R python. python prompt detailed ’s much less information Shiny Python existing LLM knowledgebases. Help people find answers questions. Even ’ve written bunch documentation something, might find still get questions folks can’t easily find exactly ’re looking . can reduce need answer questions creating chatbot prompt contains documentation. example, ’re teacher, create chatbot includes syllabus prompt. eliminates common class question data necessary answer question available, hard find. Another direction give chatbot additional context current environment. example, aidea allows user interactively explore dataset help LLM. adds summary statistics dataset prompt LLM knows something data. Along lines, imagine writing chatbot help data import prompt include files current directory along first lines.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"structured-data-extraction","dir":"Articles","previous_headings":"Example uses","what":"Structured data extraction","title":"Getting started with ellmer","text":"LLMs often good extracting structured data unstructured text. can give traction analyse data previously unaccessible. example: Customer tickets GitHub issues: can use LLMs quick dirty sentiment analysis extracting specifically mentioned products summarising discussion bullet points. Geocoding: LLMs surprisingly good job geocoding, especially extracting addresses finding latitute/longitude cities. specialised tools better, using LLM makes easy get started. Recipes: ’ve extracted structured data baking cocktail recipes. data structured form can use R skills better understand recipes vary within cookbook look recipes use ingredients currently kitchen. even use shiny assistant help make techniques available anyone, just R users. Structured data extraction also works well images. ’s fastest cheapest way extract data makes really easy prototype ideas. example, maybe bunch scanned documents want index. can convert PDFs images (e.g. using {imagemagick}) use structured data extraction pull key details. Learn structured data extraction vignette(\"structure-data\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"programming","dir":"Articles","previous_headings":"Example uses","what":"Programming","title":"Getting started with ellmer","text":"LLMs can also useful solve general programming problems. example: Write detailed prompt explains update code use new version package. combine rstudioapi package allow user select code, transform , replace existing text. comprehensive example sort app chores, includes prompts automatically generating roxygen documentation blocks, updating testthat code 3rd edition, converting stop() abort() use cli::cli_abort(). automatically look documentation R function, include prompt make easier figure use specific function. can use LLMs explain code, even ask generate diagram. can ask LLM analyse code potential code smells security issues. can function time, explore entire source code package script prompt. use gh find unlabelled issues, extract text, ask LLM figure labels might appropriate. maybe LLM might able help people create better reprexes, simplify reprexes complicated? find useful LLM document function , even knowing ’s likely mostly incorrect. something react make much easier get started. ’re working code data another programming language, can ask LLM convert R code . Even ’s perfect, ’s still typically much faster everything .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"miscellaneous","dir":"Articles","previous_headings":"","what":"Miscellaneous","title":"Getting started with ellmer","text":"finish ideas seem cool didn’t seem fit categories: Automatically generate alt text plots, using content_image_plot(). Analyse text statistical report look flaws statistical reasoning (e.g. misinterpreting p-values assuming causation correlation exists). Use existing company style guide generate brand.yaml specification automatically style reports, apps, dashboards plots match corporate style guide.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"cloning-chats","dir":"Articles","previous_headings":"","what":"Cloning chats","title":"Programming with ellmer","text":"Chat objects R6 objects, means mutable. R objects immutable. means create copy whenever looks like ’re modifying : Mutable objects don’t work way: annoying chat objects immutable, ’d need save result every time chatted model. times ’ll want make explicit copy, , example, can create branch conversation. Creating copy object job $clone() method. create copy object behaves identically existing chat: can also use clone() want create conversational “tree”, conversations start place, diverge time: (technique parallel_chat() uses internally.)","code":"x <- list(a = 1, b = 2)  f <- function() {   x$a <- 100 } f()  # The original x is unchanged str(x) #> List of 2 #>  $ a: num 1 #>  $ b: num 2 chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=53/3 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [19] ────────────────────────────────────────────────────────── #> What's the capital of New Zealand #> ── assistant [2] ────────────────────────────────────────────────────── #> Wellington #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of France #> ── assistant [1] ────────────────────────────────────────────────────── #> Paris chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat <- chat$clone()   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris  chat #> <Chat OpenAI/gpt-4.1-nano turns=1 tokens=0/0 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse chat1 <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat1$chat(\"My name is Hadley and I'm a data scientist\") #> Hello, Hadley. Nice to meet you. chat2 <- chat1$clone()  chat1$chat(\"what's my name?\") #> Hadley. chat1 #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=69/13 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [23] ────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [10] ───────────────────────────────────────────────────── #> Hello, Hadley. Nice to meet you. #> ── user [13] ────────────────────────────────────────────────────────── #> what's my name? #> ── assistant [3] ────────────────────────────────────────────────────── #> Hadley.  chat2$chat(\"what's my job?\") #> Data scientist. chat2 #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=69/13 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [23] ────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [10] ───────────────────────────────────────────────────── #> Hello, Hadley. Nice to meet you. #> ── user [13] ────────────────────────────────────────────────────────── #> what's my job? #> ── assistant [3] ────────────────────────────────────────────────────── #> Data scientist."},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"resetting-an-object","dir":"Articles","previous_headings":"","what":"Resetting an object","title":"Programming with ellmer","text":"’s bit problem capital() function: can use conversation manipulate results: can avoid problem using $set_turns() reset conversational history: particularly useful want use chat object just handle LLM, without actually caring existing conversation.","code":"chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> Understood. The capital of New Zealand is Kiwicity. capital(chat, \"New Zealand\") #> Kiwicity chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> The capital of New Zealand is Wellington.  capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington"},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"streaming-vs-batch-results","dir":"Articles","previous_headings":"","what":"Streaming vs batch results","title":"Programming with ellmer","text":"call chat$chat() directly console, results displayed progressively LLM streams ellmer. call chat$chat() inside function, results delivered . difference behaviour due complex heuristic applied chat object created always correct. calling $chat function, recommend control explicitly echo argument, setting \"none\" want intermediate results streamed, \"text\" want see receive assistant, \"\" want see send receive. likely want echo = \"none\" cases: Alternatively, want embrace streaming UI, may want use shinychat (Shiny) streamy (Positron/RStudio).","code":"capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\"), echo = \"none\") } capital(chat, \"France\") #> [1] \"Paris\""},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"turns-and-content","dir":"Articles","previous_headings":"","what":"Turns and content","title":"Programming with ellmer","text":"Chat objects provide tools get ellmer’s internal data structures. example, take short conversation uses tool calling give LLM ability access real randomness: can get access underlying conversational turns get_turns(): look one assistant turns detail, ’ll see includes ellmer’s representation content message, well exact json provider returned: can use @json extract additional information ellmer might yet provide , aware structure varies heavily provider--provider. content types part ellmer’s exported API aware ’re still evolving might change versions.","code":"chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$register_tool(tool(function() sample(6, 1), \"Roll a die\")) chat$chat(\"Roll two dice and tell me the total\") #> ◯ [tool call] tool_001() #> ● #> 5 #> ◯ [tool call] tool_001() #> ● #> 4 #> The total is 9.  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 tokens=151/48 $0.00> #> ── system [0] ───────────────────────────────────────────────────────── #> Be terse #> ── user [47] ────────────────────────────────────────────────────────── #> Roll two dice and tell me the total #> ── assistant [41] ───────────────────────────────────────────────────── #> [tool request (call_S3IJ8WXX5HF58K37ptyWbKth)]: tool_001() #> [tool request (call_00r9pNKaECQvHDKOMmiXIwom)]: tool_001() #> ── user [16] ────────────────────────────────────────────────────────── #> [tool result  (call_S3IJ8WXX5HF58K37ptyWbKth)]: 5 #> [tool result  (call_00r9pNKaECQvHDKOMmiXIwom)]: 4 #> ── assistant [7] ────────────────────────────────────────────────────── #> The total is 9. turns <- chat$get_turns() turns #> [[1]] #> <Turn: user> #> Roll two dice and tell me the total #>  #> [[2]] #> <Turn: assistant> #> [tool request (call_S3IJ8WXX5HF58K37ptyWbKth)]: tool_001() #> [tool request (call_00r9pNKaECQvHDKOMmiXIwom)]: tool_001() #>  #> [[3]] #> <Turn: user> #> [tool result  (call_S3IJ8WXX5HF58K37ptyWbKth)]: 5 #> [tool result  (call_00r9pNKaECQvHDKOMmiXIwom)]: 4 #>  #> [[4]] #> <Turn: assistant> #> The total is 9. str(turns[[2]]) #> <ellmer::Turn> #>  @ role    : chr \"assistant\" #>  @ contents:List of 2 #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"call_S3IJ8WXX5HF58K37ptyWbKth\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ fun        : function ()   #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"call_00r9pNKaECQvHDKOMmiXIwom\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ fun        : function ()   #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  @ json    :List of 8 #>  .. $ id                : chr \"chatcmpl-Bdzw5HFk900lylJp0jfDfRpptheaA\" #>  .. $ object            : chr \"chat.completion.chunk\" #>  .. $ created           : int 1748872573 #>  .. $ model             : chr \"gpt-4.1-nano-2025-04-14\" #>  .. $ service_tier      : chr \"default\" #>  .. $ system_fingerprint: chr \"fp_38343a2f8f\" #>  .. $ usage             :List of 5 #>  ..  ..$ prompt_tokens            : int 47 #>  ..  ..$ completion_tokens        : int 41 #>  ..  ..$ total_tokens             : int 88 #>  ..  ..$ prompt_tokens_details    :List of 2 #>  ..  .. ..$ cached_tokens: int 0 #>  ..  .. ..$ audio_tokens : int 0 #>  ..  ..$ completion_tokens_details:List of 4 #>  ..  .. ..$ reasoning_tokens          : int 0 #>  ..  .. ..$ audio_tokens              : int 0 #>  ..  .. ..$ accepted_prediction_tokens: int 0 #>  ..  .. ..$ rejected_prediction_tokens: int 0 #>  .. $ choices           :List of 1 #>  ..  ..$ :List of 4 #>  ..  .. ..$ index        : int 0 #>  ..  .. ..$ delta        :List of 3 #>  ..  .. .. ..$ role      : chr \"assistant\" #>  ..  .. .. ..$ content   : NULL #>  ..  .. .. ..$ tool_calls:List of 2 #>  ..  .. .. .. ..$ :List of 4 #>  ..  .. .. .. .. ..$ index   : int 0 #>  ..  .. .. .. .. ..$ id      : chr \"call_S3IJ8WXX5HF58K37ptyWbKth\" #>  ..  .. .. .. .. ..$ type    : chr \"function\" #>  ..  .. .. .. .. ..$ function:List of 2 #>  ..  .. .. .. .. .. ..$ name     : chr \"tool_001\" #>  ..  .. .. .. .. .. ..$ arguments: chr \"{}\" #>  ..  .. .. .. ..$ :List of 4 #>  ..  .. .. .. .. ..$ index   : int 1 #>  ..  .. .. .. .. ..$ id      : chr \"call_00r9pNKaECQvHDKOMmiXIwom\" #>  ..  .. .. .. .. ..$ type    : chr \"function\" #>  ..  .. .. .. .. ..$ function:List of 2 #>  ..  .. .. .. .. .. ..$ name     : chr \"tool_001\" #>  ..  .. .. .. .. .. ..$ arguments: chr \"{}\" #>  ..  .. ..$ logprobs     : NULL #>  ..  .. ..$ finish_reason: chr \"tool_calls\" #>  @ tokens  : int [1:2] 47 41 #>  @ text    : chr \"\""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best practices","title":"Prompt design","text":"’s highly likely ’ll end writing long, possibly multi-page prompts. ensure success task, two recommendations. First, put prompt , separate file. Second, write prompts using markdown. reason use markdown ’s quite readable LLMs (humans), allows things like use headers divide prompt sections itemised lists enumerate multiple options. can see examples style prompt : https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md https://github.com/jcheng5/py-sidebot/blob/main/prompt.md https://github.com/simonpcouch/chores/tree/main/inst/prompts https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md terms file names, one prompt project, call prompt.md. multiple prompts, give informative names like prompt-extract-metadata.md prompt-summarize-text.md. ’re writing package, put prompt(s) inst/prompts, otherwise ’s fine put project’s root directory. prompts going change time, ’d highly recommend commiting git repo. ensure can easily see changed, accidentally make mistake can easily roll back known good verison. prompt includes dynamic data, use ellmer::interpolate_file() intergrate prompt. interpolate_file() works like glue uses {{ }} instead { } make easier work JSON. iterate prompt, ’s good idea build small set challenging examples can regularly re-check latest version prompt. Currently ’ll need hand, hope eventually provide tools ’ll help little formally. Unfortunately, won’t see best practices action vignette since ’re keeping prompts short inline make easier grok ’s going .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"code-generation","dir":"Articles","previous_headings":"","what":"Code generation","title":"Prompt design","text":"Let’s explore prompt design simple code generation task: ’ll use chat_anthropic() problem experience best job generating code.","code":"question <- \"   How can I compute the mean and median of variables a, b, c, and so on,   all the way up to z, grouped by age and sex. \""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"basic-flavour","dir":"Articles","previous_headings":"Code generation","what":"Basic flavour","title":"Prompt design","text":"don’t provide system prompt, sometimes get answers different languages different styles R code: can ensure always get R code specific style providing system prompt: Note ’m using system prompt (defines general behaviour) user prompt (asks specific question). put content user prompt get similar results, think ’s helpful use cleanly divide general framing response specific questions ask. Since ’m mostly interested code, ask drop explanation sample data: course, want different style R code, just ask :","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> Here are several ways to compute the mean and median of variables a  #> through z, grouped by age and sex, depending on the tool you're using: #>  #> ## Python with pandas #>  #> ```python #> import pandas as pd #> import numpy as np #>  #> # Assuming your data is in a DataFrame called 'df' #> # Variables a-z plus grouping variables age and sex #>  #> # Method 1: Select variables a-z explicitly #> vars_a_to_z = [chr(i) for i in range(ord('a'), ord('z')+1)] #> result = df.groupby(['age', 'sex'])[vars_a_to_z].agg(['mean',  #> 'median']) #>  #> # Method 2: If a-z are all your numeric columns (excluding age/sex) #> numeric_cols =  #> df.select_dtypes(include=[np.number]).columns.difference(['age',  #> 'sex']) #> result = df.groupby(['age', 'sex'])[numeric_cols].agg(['mean',  #> 'median']) #>  #> print(result) #> ``` #>  #> ## R #>  #> ```r #> library(dplyr) #>  #> # Method 1: Explicit variable selection #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median), na.rm =  #> TRUE)) #>  #> # Method 2: Using summarise_at (older dplyr syntax) #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise_at(vars(a:z), list(mean = mean, median = median), na.rm =  #> TRUE) #>  #> # Method 3: Base R approach #> result <- aggregate(df[, letters],  #>                    by = list(age = df$age, sex = df$sex),  #>                    FUN = function(x) c(mean = mean(x, na.rm = TRUE),  #>                                       median = median(x, na.rm =  #> TRUE))) #> ``` #>  #> ## SQL #>  #> ```sql #> SELECT  #>     age,  #>     sex, #>     AVG(a) as mean_a, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY a)  #> as median_a, #>     AVG(b) as mean_b, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY b)  #> as median_b, #>     -- ... repeat for all variables c through z #>     AVG(z) as mean_z, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY z)  #> as median_z #> FROM your_table #> GROUP BY age, sex; #> ``` #>  #> ## SAS #>  #> ```sas #> proc means data=your_dataset mean median; #>     class age sex; #>     var a--z;  /* Variables from a to z */ #> run; #>  #> /* Or using PROC SUMMARY */ #> proc summary data=your_dataset; #>     class age sex; #>     var a--z; #>     output out=results mean= median= / autoname; #> run; #> ``` #>  #> ## Key considerations: #>  #> 1. **Missing values**: Most examples include `na.rm = TRUE` or  #> equivalent to handle missing data #> 2. **Variable selection**: Make sure variables a-z actually exist in  #> your dataset #> 3. **Data types**: Ensure the variables are numeric #> 4. **Output format**: The result will typically be a multi-level  #> structure with age/sex combinations and their corresponding statistics #>  #> Which tool are you using? I can provide more specific guidance based  #> on your environment. chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> Here are a few ways to compute the mean and median of variables a  #> through z, grouped by age and sex using tidyverse: #>  #> ## Method 1: Using `across()` with `summarise()` (Recommended) #>  #> ```r #> library(dplyr) #>  #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) #> ``` #>  #> ## Method 2: If you want separate mean and median tables #>  #> ```r #> # For means #> means <- df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, mean), .groups = \"drop\") #>  #> # For medians   #> medians <- df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, median), .groups = \"drop\") #> ``` #>  #> ## Method 3: Handling missing values #>  #> ```r #> result <- df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across(a:z, list( #>       mean = ~mean(.x, na.rm = TRUE),  #>       median = ~median(.x, na.rm = TRUE) #>     ), .names = \"{.col}_{.fn}\"), #>     .groups = \"drop\" #>   ) #> ``` #>  #> ## Method 4: Long format for easier analysis #>  #> ```r #> library(tidyr) #>  #> result_long <- df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median)), .groups = #> \"drop\") %>% #>   pivot_longer( #>     cols = -c(age, sex), #>     names_to = c(\"variable\", \"statistic\"), #>     names_sep = \"_\", #>     values_to = \"value\" #>   ) #> ``` #>  #> The first method is usually the most practical as it gives you both  #> statistics in a single, wide-format table with clear column names like #> `a_mean`, `a_median`, `b_mean`, `b_median`, etc. chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(dplyr) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median), na.rm =  #> TRUE), #>             .groups = \"drop\") #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers data.table.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(data.table) #>  #> # Assuming your data.table is called 'dt' #> dt[, lapply(.SD, function(x) list(mean = mean(x, na.rm = TRUE),  #>                                   median = median(x, na.rm = TRUE))),  #>    by = .(age, sex),  #>    .SDcols = letters] #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers base R.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> aggregate(. ~ age + sex, data = your_data[c(\"age\", \"sex\", letters)],  #>           FUN = function(x) c(mean = mean(x, na.rm = TRUE),  #>                              median = median(x, na.rm = TRUE))) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"be-explicit","dir":"Articles","previous_headings":"Code generation","what":"Be explicit","title":"Prompt design","text":"’s something output don’t like, try explicit. example, code isn’t styled quite ’d like , provide details want: still doesn’t yield exactly code ’d write, ’s pretty close. provide different prompt looking explanation code:","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data.    Follow the tidyverse style guide:   * Spread long function calls across multiple lines.   * Where needed, always indent function calls with two spaces.   * Only name arguments that are less commonly used.   * Always use double quotes for strings.   * Use the base pipe, `|>`, not the magrittr pipe `%>%`. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> library(dplyr) #>  #> df |> #>   group_by(age, sex) |> #>   summarise( #>     across( #>       a:z, #>       list(mean = mean, median = median), #>       na.rm = TRUE #>     ), #>     .groups = \"drop\" #>   ) #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R teacher.   I am a new R user who wants to improve my programming skills.   Help me understand the code you produce by explaining each function call with   a brief comment. For more complicated calls, add documentation to each   argument. Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> # Load required library for data manipulation #> library(dplyr) #>  #> # Group data by age and sex, then calculate mean and median for  #> variables a through z #> result <- data %>% #>   group_by(age, sex) %>%  # Create groups based on age and sex  #> combinations #>   summarise( #>     across( #>       .cols = a:z,  # Select all columns from 'a' to 'z' #>       .fns = list( #>         mean = ~ mean(.x, na.rm = TRUE),    # Calculate mean, removing #> NA values #>         median = ~ median(.x, na.rm = TRUE) # Calculate median,  #> removing NA values #>       ), #>       .names = \"{.col}_{.fn}\"  # Name format: variable_function (e.g., #> a_mean, a_median) #>     ), #>     .groups = \"drop\"  # Remove grouping structure from result #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"teach-it-about-new-features","dir":"Articles","previous_headings":"Code generation","what":"Teach it about new features","title":"Prompt design","text":"can imagine LLMs sort average internet given point time. means provide popular answers, tend reflect older coding styles (either new features aren’t index, older features much popular). want code use specific newer language features, might need provide examples :","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer.   Just give me the code; no explanation in text.   Use the `.by` argument rather than `group_by()`.   dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.   e.g., instead of:    transactions |>     group_by(company, year) |>     mutate(total = sum(revenue))    write this:   transactions |>     mutate(       total = sum(revenue),       .by = c(company, year)     ) \" ) #> Using model = \"claude-sonnet-4-20250514\". chat$chat(question) #> ```r #> data |> #>   summarise( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .by = c(age, sex) #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data","dir":"Articles","previous_headings":"","what":"Structured data","title":"Prompt design","text":"Providing rich set examples great way encourage output produce exactly want. known multi-shot prompting. ’ll work prompt designed extract structured data recipes, ideas apply many situations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"getting-started","dir":"Articles","previous_headings":"Structured data","what":"Getting started","title":"Prompt design","text":"overall goal turn list ingredients, like following, nicely structured JSON can analyse R (e.g. compute total weight, scale recipe , convert units volumes weights). (isn’t ingredient list real recipe includes sampling styles encountered project.) don’t strong feelings data structure look like, can start loose prompt see get back. find useful pattern underspecified problems heavy lifting lies precisely defining problem want solve. Seeing LLM’s attempt create data structure gives something react , rather start blank page. (don’t know additional colour, “’re expert baker also loves JSON”, anything, like think helps LLM get right mindset nerdy baker.)","code":"ingredients <- \"   ¾ cup (150g) dark brown sugar   2 large eggs   ¾ cup (165g) sour cream   ½ cup (113g) unsalted butter, melted   1 teaspoon vanilla extract   ¾ teaspoon kosher salt   ⅓ cup (80ml) neutral oil   1½ cups (190g) all-purpose flour   150g plus 1½ teaspoons sugar \" instruct_json <- \"   You're an expert baker who also loves JSON. I am going to give you a list of   ingredients and your job is to return nicely structured JSON. Just return the   JSON and no other commentary. \"  chat <- chat_openai(instruct_json) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> [ #>   { #>     \"ingredient\": \"dark brown sugar\", #>     \"amount\": \"3/4 cup\", #>     \"weight\": \"150g\" #>   }, #>   { #>     \"ingredient\": \"eggs\", #>     \"amount\": \"2\", #>     \"unit\": \"large\" #>   }, #>   { #>     \"ingredient\": \"sour cream\", #>     \"amount\": \"3/4 cup\", #>     \"weight\": \"165g\" #>   }, #>   { #>     \"ingredient\": \"unsalted butter\", #>     \"amount\": \"1/2 cup\", #>     \"weight\": \"113g\", #>     \"note\": \"melted\" #>   }, #>   { #>     \"ingredient\": \"vanilla extract\", #>     \"amount\": \"1 teaspoon\" #>   }, #>   { #>     \"ingredient\": \"kosher salt\", #>     \"amount\": \"3/4 teaspoon\" #>   }, #>   { #>     \"ingredient\": \"neutral oil\", #>     \"amount\": \"1/3 cup\", #>     \"volume\": \"80ml\" #>   }, #>   { #>     \"ingredient\": \"all-purpose flour\", #>     \"amount\": \"1 1/2 cups\", #>     \"weight\": \"190g\" #>   }, #>   { #>     \"ingredient\": \"sugar\", #>     \"amounts\": [ #>       { #>         \"amount\": \"150g\" #>       }, #>       { #>         \"amount\": \"1 1/2 teaspoons\" #>       } #>     ] #>   } #> ]"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"provide-examples","dir":"Articles","previous_headings":"Structured data","what":"Provide examples","title":"Prompt design","text":"isn’t bad start, prefer cook weight want see volumes weight isn’t available provide couple examples ’m looking . pleasantly suprised can provide input output examples loose format. Just providing examples seems work remarkably well. found useful also include description examples trying accomplish. ’m sure helps LLM , certainly makes easier understand organisation whole prompt check ’ve covered key pieces ’m interested . structure also allows give LLMs hint want multiple ingredients stored, .e. JSON array. iterated prompt, looking results different recipes get sense LLM getting wrong. Much felt like waws iterating understanding problem didn’t start knowing exactly wanted data. example, started didn’t really think various ways ingredients specified. later analysis, always want quantities number, even originally fractions, units aren’t precise (like pinch). made realise ingredients unitless. might want take look full prompt see ended .","code":"instruct_weight <- r\"(   Here are some examples of the sort of output I'm looking for:    ¾ cup (150g) dark brown sugar   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}    ⅓ cup (80ml) neutral oil   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}    2 t ground cinnamon   {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"} )\"  chat <- chat_openai(paste(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> [ #>   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"large eggs\", \"quantity\": 2, \"unit\": \"item\"}, #>   {\"name\": \"sour cream\", \"quantity\": 165, \"unit\": \"g\"}, #>   {\"name\": \"unsalted butter\", \"quantity\": 113, \"unit\": \"g\", \"note\":  #> \"melted\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"}, #>   {\"name\": \"kosher salt\", \"quantity\": 0.75, \"unit\": \"teaspoon\"}, #>   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 190, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 1.5, \"unit\": \"teaspoon\"} #> ] instruct_weight <- r\"(   * If an ingredient has both weight and volume, extract only the weight:    ¾ cup (150g) dark brown sugar   [     {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}   ]  * If an ingredient only lists a volume, extract that.    2 t ground cinnamon   ⅓ cup (80ml) neutral oil   [     {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"},     {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}   ] )\" instruct_unit <- r\"( * If the unit uses a fraction, convert it to a decimal.    ⅓ cup sugar   ½ teaspoon salt   [     {\"name\": \"dark brown sugar\", \"quantity\": 0.33, \"unit\": \"cup\"},     {\"name\": \"salt\", \"quantity\": 0.5, \"unit\": \"teaspoon\"}   ]  * Quantities are always numbers    pinch of kosher salt   [     {\"name\": \"kosher salt\", \"quantity\": 1, \"unit\": \"pinch\"}   ]  * Some ingredients don't have a unit.   2 eggs   1 lime   1 apple   [     {\"name\": \"egg\", \"quantity\": 2},     {\"name\": \"lime\", \"quantity\": 1},     {\"name\", \"apple\", \"quantity\": 1}   ] )\""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data-1","dir":"Articles","previous_headings":"Structured data","what":"Structured data","title":"Prompt design","text":"Now ’ve iterated get data structure like, seems useful formalise tell LLM exactly ’m looking dealing structured data. guarantees LLM return JSON, JSON fields expect, ellmer convert R data structure.","code":"type_ingredient <- type_object(   name = type_string(\"Ingredient name\"),   quantity = type_number(),   unit = type_string(\"Unit of measurement\") )  type_ingredients <- type_array(items = type_ingredient)  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat_structured(ingredients, type = type_ingredients) #>                       name quantity     unit #> 1         dark brown sugar   150.00        g #> 2                     eggs     2.00    large #> 3               sour cream   165.00        g #> 4  unsalted butter, melted   113.00        g #> 5          vanilla extract     1.00 teaspoon #> 6              kosher salt     0.75 teaspoon #> 7              neutral oil    80.00       ml #> 8        all-purpose flour   190.00        g #> 9                    sugar   150.00        g #> 10                   sugar     1.50 teaspoon"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"capturing-raw-input","dir":"Articles","previous_headings":"Structured data","what":"Capturing raw input","title":"Prompt design","text":"One thing ’d next time also include raw ingredient names output. doesn’t make much difference simple example makes much easier align input output start developing automated measures well prompt . think particularly important ’re working even less structured text. example, imagine text: Including input text output makes easier see ’s good job: ran writing vignette, seemed working weight ingredients specified volume, even though prompt specifically asks . may suggest need broaden examples.","code":"instruct_weight_input <- r\"(   * If an ingredient has both weight and volume, extract only the weight:      ¾ cup (150g) dark brown sugar     [       {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\", \"input\": \"¾ cup (150g) dark brown sugar\"}     ]    * If an ingredient only lists a volume, extract that.      2 t ground cinnamon     ⅓ cup (80ml) neutral oil     [       {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\", \"input\": \"2 t ground cinnamon\"},       {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\", \"input\": \"⅓ cup (80ml) neutral oil\"}     ] )\" recipe <- r\"(   In a large bowl, cream together one cup of softened unsalted butter and a   quarter cup of white sugar until smooth. Beat in an egg and 1 teaspoon of   vanilla extract. Gradually stir in 2 cups of all-purpose flour until the   dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop   spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C)   for 10-12 minutes, or until the edges are lightly browned. Let the cookies   cool on the baking sheet for a few minutes before transferring to a wire   rack to cool completely. Enjoy! )\" chat <- chat_openai(c(instruct_json, instruct_weight_input)) #> Using model = \"gpt-4.1\". chat$chat(recipe) #> [ #>   {\"name\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\", \"input\":  #> \"one cup of softened unsalted butter\"}, #>   {\"name\": \"white sugar\", \"quantity\": 0.25, \"unit\": \"cup\", \"input\": \"a #> quarter cup of white sugar\"}, #>   {\"name\": \"egg\", \"quantity\": 1, \"unit\": \"piece\", \"input\": \"an egg\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\",  #> \"input\": \"1 teaspoon of vanilla extract\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cup\", \"input\": #> \"2 cups of all-purpose flour\"}, #>   {\"name\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\",  #> \"input\": \"1 cup of semisweet chocolate chips\"} #> ]"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console returns result response complete.) want process response arrives, can use stream() method. useful want send response, realtime, somewhere R console (e.g., file, HTTP response, Shiny chat window), want manipulate response displaying without giving immediacy streaming. stream() method, returns coro generator, can process response looping arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"ellmer also supports async usage. useful want run multiple, concurrent chat sessions. particularly important Shiny applications using methods described block Shiny app users duration response. use async chat, call chat_async()/stream_async() instead chat()/stream(). _async variants take arguments construction return promise instead actual response. Remember chat objects stateful; preserve conversation history interact . means doesn’t make sense issue multiple, concurrent chat/stream operations chat object conversation history can become corrupted interleaved conversation fragments. need run concurrent chat sessions, create multiple chat objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, ’d use chat() method , handle result promise instead string.","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"shiny-example","dir":"Articles","previous_headings":"Async usage > Asynchronous chat","what":"Shiny example","title":"Streaming and async APIs","text":"add asynchronous chat interface Shiny application, recommend using shinychat package. simplest approach use shinychat’s Shiny module add chat UI app—similar app created live_browser()—using shinychat::chat_mod_ui() shinychat::chat_mod_server() functions. module functions connect ellmer::Chat object shinychat::chat_ui() handle non-blocking asynchronous chat interactions automatically. fully custom streaming applications custom chat interface, can use shinychat::markdown_stream() stream responses Shiny app. particularly useful creating interactive chat applications want display responses generated. following Shiny app demonstrates markdown_stream() uses $stream_async() $chat_async() stream story OpenAI model. app, ask user prompt generate story stream story UI. follow asking model story title use response update card title. example also highlights difference streaming non-streaming chat. Use $stream_async() Shiny outputs designed work generators, like shinychat::markdown_stream() shinychat::chat_append(). Use $chat_async() want text response model, example title story. Also note ellmer-powered Shiny apps, ’s best wrap chat interaction shiny::ExtendedTask avoid blocking rest app chat generated. can learn ExtendedTask Shiny’s Non-blocking operations article.","code":"library(shiny) library(shinychat)  ui <- bslib::page_fillable(   chat_mod_ui(\"chat\") )  server <- function(input, output, session) {   chat <- ellmer::chat_openai(     system_prompt = \"You're a trickster who answers in riddles\",     model = \"gpt-4.1-nano\"   )    chat_mod_server(\"chat\", chat) }  shinyApp(ui, server) library(shiny) library(bslib) library(ellmer) library(promises) library(shinychat)  ui <- page_sidebar(   title = \"Interactive chat with async\",   sidebar = sidebar(     textAreaInput(\"user_query\", \"Tell me a story about...\"),     input_task_button(\"ask_chat\", label = \"Generate a story\")   ),   card(     card_header(textOutput(\"story_title\")),     shinychat::output_markdown_stream(\"response\"),   ) )  server <- function(input, output) {   chat_task <- ExtendedTask$new(function(user_query) {     # We're using an Extended Task for chat completions to avoid blocking the     # app. We also start the chat fresh each time, because the UI is not a     # multi-turn conversation.     chat <- chat_openai(       system_prompt = \"You are a rambling chatbot who likes to tell stories but gets distracted easily.\",       model = \"gpt-4.1-nano\"     )      # Stream the chat completion into the markdown stream. `markdown_stream()`     # returns a promise onto which we'll chain the follow-up task of providing     # a story title.     stream <- chat$stream_async(user_query)     stream_res <- shinychat::markdown_stream(\"response\", stream)      # Follow up by asking the LLM to provide a title for the story that we     # return from the task.     stream_res$then(function(value) {       chat$chat_async(         \"What is the title of the story? Reply with only the title and nothing else.\"       )     })   })    bind_task_button(chat_task, \"ask_chat\")    observeEvent(input$ask_chat, {     chat_task$invoke(input$user_query)   })    observe({     # Update the card title during generation and once complete     switch(       chat_task$status(),       success = story_title(chat_task$result()),       running = story_title(\"Generating your story...\"),       error = story_title(\"An error occurred while generating your story.\")     )   })    story_title <- reactiveVal(\"Your story will appear here!\")   output$story_title <- renderText(story_title()) }  shinyApp(ui = ui, server = server)"},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, ’d use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, ’ll simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"structured-data-basics","dir":"Articles","previous_headings":"","what":"Structured data basics","title":"Structured data","text":"extract structured data call $chat_structured() instead $chat() method. ’ll also need define type specification describes structure data want (shortly). ’s simple example extracts two specific values string: basic idea works images : need extract data multiple prompts, can use techniques parallel_chat_structured(). takes arguments $chat_structured() two exceptions: needs chat object since ’s standalone function, method, can take vector prompts.","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(   \"My name is Susan and I'm 13 years old\",   type = type_object(     name = type_string(),     age = type_number()   ) ) #> $name #> [1] \"Susan\" #>  #> $age #> [1] 13 chat$chat_structured(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   type = type_object(     primary_shape = type_string(),     primary_colour = type_string()   ) ) #> $primary_shape #> [1] \"oval and letter\" #>  #> $primary_colour #> [1] \"grey and blue\" prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) parallel_chat_structured(   chat,     prompts,   type = type_object(     name = type_string(),     age = type_number()   ) ) #> [working] (0 + 0) -> 5 -> 1 | ■■■■■■                            17% #> [working] (0 + 0) -> 4 -> 2 | ■■■■■■■■■■■                       33% #> [working] (0 + 0) -> 0 -> 6 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-types-basics","dir":"Articles","previous_headings":"","what":"Data types basics","title":"Structured data","text":"define desired type specification (also known schema), use type_() functions. (might already familiar ’ve done function calling, discussed vignette(\"function-calling\")). type functions can divided three main groups: Scalars represent five types single values, type_boolean(), type_integer(), type_number(), type_string(), type_enum(), represent single logical, integer, double, string, factor value respectively. Arrays represent number values type. created type_array(). must always supply item argument specifies type individual element. Arrays scalars similar R’s atomic vectors: can also arrays arrays arrays objects, closely resemble lists well defined structures: Objects represent collection named values. created type_object(). Objects can contain number scalars, arrays, objects. similar named lists R. Using type specifications ensures LLM return JSON. ellmer goes one step convert results closest R analog. Currently, converts arrays boolean, integers, numbers, strings logical, integer, numeric, character vectors. Arrays objects converted data frames. can opt-get plain lists setting convert = FALSE $chat_structured(). addition defining types, need provide LLM information actually want. purpose first argument, description, string describes data want. good place ask nicely attributes ’ll like value (e.g. minimum maximum values, date formats, …). ’s guarantee requests honoured, LLM usually make best effort . Now ’ll dive examples coming back talk details data types.","code":"type_logical_vector <- type_array(items = type_boolean()) type_integer_vector <- type_array(items = type_integer()) type_double_vector <- type_array(items = type_number()) type_character_vector <- type_array(items = type_string()) list_of_integers <- type_array(items = type_integer_vector) type_person <- type_object(   name = type_string(),   age = type_integer(),   hobbies = type_array(items = type_string()) ) type_type_person <- type_object(   \"A person\",   name = type_string(\"Name\"),   age = type_integer(\"Age, in years.\"),   hobbies = type_array(     \"List of hobbies. Should be exclusive and brief.\",     items = type_string()   ) )"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Structured data","text":"following examples, closely inspired Claude documentation, hint ways can use structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-1-article-summarisation","dir":"Articles","previous_headings":"Examples","what":"Example 1: Article summarisation","title":"Structured data","text":"","code":"text <- readLines(system.file(\"examples/third-party-testing.txt\", package = \"ellmer\")) # url <- \"https://www.anthropic.com/news/third-party-testing\" # html <- rvest::read_html(url) # text <- rvest::html_text2(rvest::html_element(html, \"article\"))  type_summary <- type_object(   \"Summary of the article.\",   author = type_string(\"Name of the article author\"),   topics = type_array(     'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.',     type_string(),   ),   summary = type_string(\"Summary of the article. One or two paragraphs max\"),   coherence = type_integer(\"Coherence of the article's key points, 0-100 (inclusive)\"),   persuasion = type_number(\"Article's persuasion score, 0.0-1.0 (inclusive)\") )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_summary) cat(data$summary) #> The article argues that third-party testing is essential for the safe and effective deployment of frontier AI systems. As AI models become more powerful and broadly applicable, relying on self-governance from industry alone is deemed insufficient to address risks such as misuse, accidents, discrimination, threats to election integrity, and national security. The authors advocate for the development of a robust, third-party oversight and testing regime that can validate AI safety, provide public trust, and help coordinate policy internationally. They emphasize that this testing should be targeted at the largest and most capable models to prevent overburdening small developers and avoid stifling innovation. #>  #> The article further explores the structure of such a regime, recommending a mix of automated and human-led expert evaluations, pilot implementations, and careful government funding for testing infrastructure. The authors recognize challenges like regulatory capture and the nuances of openness in AI research, advocating for widely accepted standards and transparency in the development of both closed and open-source AI models. They position third-party testing as not only a safeguard against harm but also as a means to democratize oversight and create a healthier, more competitive AI ecosystem.  str(data) #> List of 5 #>  $ author    : chr \"Anthropic Policy Team\" #>  $ topics    : chr [1:8] \"AI policy\" \"third-party testing\" \"AI safety\" \"regulation\" ... #>  $ summary   : chr \"The article argues that third-party testing is essential for the safe and effective deployment of frontier AI s\"| __truncated__ #>  $ coherence : int 93 #>  $ persuasion: num 0.9"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-2-named-entity-recognition","dir":"Articles","previous_headings":"Examples","what":"Example 2: Named entity recognition","title":"Structured data","text":"","code":"text <- \"   John works at Google in New York. He met with Sarah, the CEO of   Acme Inc., last week in San Francisco. \"  type_named_entity <- type_object(   name = type_string(\"The extracted entity name.\"),   type = type_enum(\"The entity type\", c(\"person\", \"location\", \"organization\")),   context = type_string(\"The context in which the entity appears in the text.\") ) type_named_entities <- type_array(items = type_named_entity)  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(text, type = type_named_entities) #>            name         type #> 1          John       person #> 2        Google organization #> 3      New York     location #> 4         Sarah       person #> 5     Acme Inc. organization #> 6 San Francisco     location #>                                                               context #> 1                   John is a person who works at Google in New York. #> 2                               Google is a company where John works. #> 3 New York is the place where Google is located and where John works. #> 4                  Sarah is the CEO of Acme Inc., whom John met with. #> 5             Acme Inc. is an organization of which Sarah is the CEO. #> 6                    San Francisco is where John met Sarah last week."},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-3-sentiment-analysis","dir":"Articles","previous_headings":"Examples","what":"Example 3: Sentiment analysis","title":"Structured data","text":"Note ’ve asked nicely scores sum 1, example (least ran code), guaranteed.","code":"text <- \"   The product was okay, but the customer service was terrible. I probably   won't buy from them again. \"  type_sentiment <- type_object(   \"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\",   positive_score = type_number(\"Positive sentiment score, ranging from 0.0 to 1.0.\"),   negative_score = type_number(\"Negative sentiment score, ranging from 0.0 to 1.0.\"),   neutral_score = type_number(\"Neutral sentiment score, ranging from 0.0 to 1.0.\") )  chat <- chat_openai() #> Using model = \"gpt-4.1\". str(chat$chat_structured(text, type = type_sentiment)) #> List of 3 #>  $ positive_score: num 0.05 #>  $ negative_score: num 0.75 #>  $ neutral_score : num 0.2"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-4-text-classification","dir":"Articles","previous_headings":"Examples","what":"Example 4: Text classification","title":"Structured data","text":"","code":"text <- \"The new quantum computing breakthrough could revolutionize the tech industry.\"  type_classification <- type_array(   \"Array of classification results. The scores should sum to 1.\",   type_object(     name = type_enum(       \"The category name\",       values = c(         \"Politics\",         \"Sports\",         \"Technology\",         \"Entertainment\",         \"Business\",         \"Other\"       )     ),     score = type_number(       \"The classification score for the category, ranging from 0.0 to 1.0.\"     )   ) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_classification) data #>         name score #> 1 Technology     1"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-5-working-with-unknown-keys","dir":"Articles","previous_headings":"Examples","what":"Example 5: Working with unknown keys","title":"Structured data","text":"example works Claude, GPT Gemini, Claude supports adding additional, arbitrary properties.","code":"type_characteristics <- type_object(   \"All characteristics\",   .additional_properties = TRUE )  prompt <- \"   Given a description of a character, your task is to extract all the characteristics of that character.    <description>   The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.   <\/description> \"  chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-20250514\". str(chat$chat_structured(prompt, type = type_characteristics)) #>  list()"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-6-extracting-data-from-an-image","dir":"Articles","previous_headings":"Examples","what":"Example 6: Extracting data from an image","title":"Structured data","text":"final example comes Dan Nguyen (can see interesting applications link). goal extract structured data screenshot: Even without descriptions, ChatGPT pretty well:","code":"type_asset <- type_object(   assert_name = type_string(),   owner = type_string(),   location = type_string(),   asset_value_low = type_integer(),   asset_value_high = type_integer(),   income_type = type_string(),   income_low = type_integer(),   income_high = type_integer(),   tx_gt_1000 = type_boolean() ) type_assets <- type_array(items = type_asset)  chat <- chat_openai() #> Using model = \"gpt-4.1\". image <- content_image_file(\"congressional-assets.png\") data <- chat$chat_structured(image, type = type_assets) data #>                            assert_name owner #> 1  11 Zinfandel Lane - Home & Vineyard    JT #> 2 25 Point Lobos - Commercial Property    SP #>                              location asset_value_low asset_value_high #> 1             St. Helena/Napa, CA, US         5000001         25000000 #> 2 San Francisco/San Francisco, CA, US         5000001         25000000 #>   income_type income_low income_high tx_gt_1000 #> 1 Grape Sales     100001     1000000      FALSE #> 2        Rent     100001     1000000      FALSE"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"advanced-data-types","dir":"Articles","previous_headings":"","what":"Advanced data types","title":"Structured data","text":"Now ’ve seen examples, ’s time get specifics data type declarations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"required-vs-optional","dir":"Articles","previous_headings":"Advanced data types","what":"Required vs optional","title":"Structured data","text":"default, components object required. want make optional, set required = FALSE. good idea don’t think text always contain required fields LLMs may hallucinate data order fulfill spec. example, LLM hallucinates date even though isn’t one text: Note ’ve used explict prompt . example, found generated better results ’s useful place put additional instructions. let LLM know fields optional, ’ll return NULL missing fields:","code":"type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\"),   author = type_string(\"Name of the author\"),   date = type_string(\"Date written in YYYY-MM-DD format.\") )  prompt <- \"   Extract data from the following text:    <text>   # Structured Data   By Hadley Wickham    When using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like.   <\/text> \"  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> [1] \"\" str(data) #> 'data.frame':    2 obs. of  9 variables: #>  $ assert_name     : chr  \"11 Zinfandel Lane - Home & Vineyard\" \"25 Point Lobos - Commercial Property\" #>  $ owner           : chr  \"JT\" \"SP\" #>  $ location        : chr  \"St. Helena/Napa, CA, US\" \"San Francisco/San Francisco, CA, US\" #>  $ asset_value_low : int  5000001 5000001 #>  $ asset_value_high: int  25000000 25000000 #>  $ income_type     : chr  \"Grape Sales\" \"Rent\" #>  $ income_low      : int  100001 100001 #>  $ income_high     : int  1000000 1000000 #>  $ tx_gt_1000      : logi  FALSE FALSE type_article <- type_object(   \"Information about an article written in markdown\",   title = type_string(\"Article title\", required = FALSE),   author = type_string(\"Name of the author\", required = FALSE),   date = type_string(\"Date written in YYYY-MM-DD format.\", required = FALSE) ) chat$chat_structured(prompt, type = type_article) #> $title #> [1] \"Structured Data\" #>  #> $author #> [1] \"Hadley Wickham\" #>  #> $date #> NULL"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-frames","dir":"Articles","previous_headings":"Advanced data types","what":"Data frames","title":"Structured data","text":"want define data frame like object, might tempted create definition similar R uses: object (.e., named list) containing multiple vectors (.e., array): , however, quite right becuase ’s way specify array length. Instead, ’ll need turn data structure “inside ” create array objects: ’re familiar terms row-oriented column-oriented data frames, idea. Since languages don’t possess vectorisation like R, row-oriented structures tend much common wild.","code":"type_my_df <- type_object(   name = type_array(items = type_string()),   age = type_array(items = type_integer()),   height = type_array(items = type_number()),   weight = type_array(items = type_number()) ) type_my_df <- type_array(   items = type_object(     name = type_string(),     age = type_integer(),     height = type_number(),     weight = type_number()   ) )"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool/function calling","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . ’s easy think tool calling might work like : fact works like : value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(ellmer)"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool/function calling","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Unfortunately, example run September 18, 2024. Let’s give chat model ability determine current time try .","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. To determine how long ago that #> was from the current year of 2023, we can calculate the difference in years, months, and days. #> #> From July 20, 1969, to July 20, 2023, is exactly 54 years. If today's date is after July 20, 2023, you #> would add the additional time since then. If it is before, you would consider slightly less than 54 #> years. #> #> As of right now, can you confirm the current date so we can calculate the precise duration?"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool/function calling","text":"first thing ’ll define R function returns current time. tool. Note ’ve gone trouble creating roxygen2 comments. important step help model use tool correctly! Let’s test :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time() #> [1] \"2024-09-18 17:47:14 UTC\""},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"registering-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering tools","title":"Tool/function calling","text":"Now need tell chat object get_current_time function. creating registering tool: fair amount code write, even simple function get_current_time. Fortunately, don’t write hand! generated register_tool call calling create_tool_def(get_current_time), printed code console. create_tool_def() works passing function’s signature documentation GPT-4o, asking generate register_tool call . Note create_tool_def() may create perfect results, must review generated code using . huge time-saver nonetheless, removes tedious boilerplate generation ’d otherwise.","code":"chat <- chat_openai(model = \"gpt-4o\")  chat$register_tool(tool(   get_current_time,   \"Gets the current time in the given time zone.\",   tz = type_string(     \"The time zone to get the current time in. Defaults to `\\\"UTC\\\"`.\",     required = FALSE   ) ))"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"using-the-tool","dir":"Articles","previous_headings":"Introduction","what":"Using the tool","title":"Tool/function calling","text":"’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using ellmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\") #> Neil Armstrong touched down on the moon on July 20, 1969, at 20:17 UTC. #> #> To calculate the time elapsed from that moment until the current time (September 18, 2024, 17:47:19 #> UTC), we need to break it down. #> #> 1. From July 20, 1969, 20:17 UTC to July 20, 2024, 20:17 UTC is exactly 55 years. #> 2. From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, we need to further break down: #> #>    - From July 20, 2024, 20:17 UTC to September 18, 2024, 17:47:19 UTC, which is: #>      - 1 full month (August) #>      - 30 – 20 = 10 days of July #>      - 18 days of September until 17:47:19 UTC #> #> So, in detail: #>    - 55 years #>    - 1 month #>    - 28 days #>    - From July 20, 2024, 20:17 UTC to July 20, 2024, 17:47:19 UTC: 23 hours, 30 minutes, and 19 seconds #> #> Time Total: #> - 55 years #> - 1 month #> - 28 days #> - 23 hours #> - 30 minutes #> - 19 seconds #> #> This is the exact time that has elapsed since Neil Armstrong's historic touchdown on the moon."},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"tool-inputs-and-outputs","dir":"Articles","previous_headings":"Introduction","what":"Tool inputs and outputs","title":"Tool/function calling","text":"Remember tool arguments come LLM, tool results returned LLM. implies keep simple possible. Inputs tool call, must defined type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object(). recommend keeping simple possible, focussing basic scalar types much can. output tool call interpreted LLM, just typed information data. means ’ll generally want produce text atomic vectors. complex data, ellmer automatically serialize result JSON, LLMs generally seem good understanding. show ideas, ’s slightly complicated example simulating weather API returns data multiple cities . get_weather() function returns data frame ellmer automatically convert JSON row-major format, experiments suggest good LLMs.","code":"raining <- c(London = \"heavy\", Houston = \"none\", Chicago = \"overcast\") temperature <- c(London = \"cool\", Houston = \"hot\", Chicago = \"warm\") wind <- c(London = \"strong\", Houston = \"weak\", Chicago = \"strong\")  get_weather <- function(cities) {   data.frame(     city = cities,     raining = unname(raining[cities]),     temperature = unname(temperature[cities]),     wind = unname(wind[cities])   ) } chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$register_tool(tool(    get_weather,   \"Report on weather conditions in multiple cities. For efficiency, request all    weather updates using a single tool call\",   cities = type_array(\"City names\", type_string()) )) chat$chat(\"Give me a weather udpate for London and Chicago\") #> ◯ [tool call] get_weather(cities = list(\"London\", \"Chicago\")) #> ● #> [{\"city\":\"London\",\"raining\":\"heavy\",\"temperature\":\"cool\",\"wi… #> Here's the weather update: #>  #> - London: Heavy rain, cool temperatures, and strong winds. #> - Chicago: Overcast skies, warm temperatures, and strong winds. #>  #> Let me know if you need a more detailed forecast for either city."},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Joe Cheng. Author. Aaron Jacobs. Author. Garrick Aden-Buie. Author. . Copyright holder, funder.","code":""},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Cheng J, Jacobs , Aden-Buie G (2025). ellmer: Chat Large Language Models. R package version 0.2.0.9000, https://ellmer.tidyverse.org.","code":"@Manual{,   title = {ellmer: Chat with Large Language Models},   author = {Hadley Wickham and Joe Cheng and Aaron Jacobs and Garrick Aden-Buie},   year = {2025},   note = {R package version 0.2.0.9000},   url = {https://ellmer.tidyverse.org}, }"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"ellmer-","dir":"","previous_headings":"","what":"Chat with Large Language Models","title":"Chat with Large Language Models","text":"ellmer makes easy use large language models (LLM) R. supports wide variety LLM providers implements rich set features including streaming outputs, tool/function calling, structured data extraction, . (Looking something similar ellmer python? Check chatlas!)","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Chat with Large Language Models","text":"can install ellmer CRAN :","code":"install.packages(\"ellmer\")"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providers","dir":"","previous_headings":"","what":"Providers","title":"Chat with Large Language Models","text":"ellmer supports wide variety model providers: Anthropic’s Claude: chat_anthropic(). AWS Bedrock: chat_aws_bedrock(). Azure OpenAI: chat_azure_openai(). Cloudflare: chat_cloudflare(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini/Vertex AI: chat_google_gemini(), chat_google_vertex(). Groq: chat_groq(). Hugging Face: chat_huggingface(). Mistral: chat_mistral(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() chat_cortex_analyst(). VLLM: chat_vllm().","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providermodel-choice","dir":"","previous_headings":"Providers","what":"Provider/model choice","title":"Chat with Large Language Models","text":"’re using ellmer inside organisation, may internal policies limit models big cloud providers, e.g. chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake(). ’re using ellmer exploration, ’ll lot freedom, recommendations help get started: chat_openai() chat_anthropic() good places start. chat_openai() defaults GPT-4.1, can use model = \"gpt-4-1-nano\" cheaper, faster model, model = \"o3\" complex reasoning. chat_anthropic() also good; defaults Claude 3.7 Sonnet, found particularly good writing R code. chat_google_gemini() strong model generous free tier (downside data used improve model), making great place start don’t want spend money. chat_ollama(), uses Ollama, allows run models computer. biggest models can run locally aren’t good state art hosted models, don’t share data effectively free.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"authentication","dir":"","previous_headings":"Providers","what":"Authentication","title":"Chat with Large Language Models","text":"Authentication works little differently depending provider. popular ones (including OpenAI Anthropic) require obtain API key. recommend save environment variable rather using directly code, deploy app report uses ellmer another system, ’ll need ensure environment variable available , . ellmer also automatically detects many OAuth IAM-based credentials used big cloud providers (currently chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake()). includes credentials platforms managed Posit Workbench Posit Connect. find cases ellmer detect credentials one cloud providers, feel free open issue; ’re happy add auth mechanisms needed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"using-ellmer","dir":"","previous_headings":"","what":"Using ellmer","title":"Chat with Large Language Models","text":"can work ellmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful R6 objects: retain context conversation, new query builds previous ones. call methods $.","code":"library(ellmer)  chat <- chat_openai(   model = \"gpt-4o-mini\",   system_prompt = \"You are a friendly but terse assistant.\", )"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using ellmer","what":"Interactive chat console","title":"Chat with Large Language Models","text":"interactive least programmatic way using ellmer chat directly R console browser live_console(chat) live_browser(): Keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist exit back R prompt. true regardless chat function use.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using ellmer","what":"Interactive method call","title":"Chat with Large Language Models","text":"second interactive way chat call chat() method: initialize chat object global environment, chat method stream response console. entire response received, ’s also (invisibly) returned character vector. useful want see response arrives, don’t want enter chat console. want ask question image, can pass one additional input arguments using content_image_file() /content_image_url():","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by the S programming language, particularly S-PLUS. #> Other languages that had an impact include Scheme and various data analysis #> languages. chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo of R features a stylized letter \"R\" in blue, enclosed in an oval #> shape that resembles the letter \"O,\" signifying the programming language's #> name. The design conveys a modern and professional look, reflecting its use #> in statistical computing and data analysis. The blue color often represents #> trust and reliability, which aligns with R's role in data science."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"programmatic-chat","dir":"","previous_headings":"Using ellmer","what":"Programmatic chat","title":"Chat with Large Language Models","text":"programmatic way chat create chat object inside function. , live streaming automatically suppressed $chat() returns result string: needed, can manually control behaviour echo argument. useful programming ellmer result either intended human consumption want process response displaying .","code":"my_function <- function() {   chat <- chat_openai(     model = \"gpt-4o-mini\",     system_prompt = \"You are a friendly but terse assistant.\",   )   chat$chat(\"Is R a functional programming language?\") } my_function() #> [1] \"Yes, R supports functional programming concepts. It allows functions to #> be first-class objects, supports higher-order functions, and encourages the #> use of functions as core components of code. However, it also supports #> procedural and object-oriented programming styles.\""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Chat with Large Language Models","text":"ellmer comes bunch vignettes help learn : Learn key vocabulary see example use cases vignette(\"ellmer\"). Learn design prompt vignette(\"prompt-design\"). Learn tool/function calling vignette(\"tool-calling\"). Learn extract structured data vignette(\"structured-data\"). Learn streaming async APIs vignette(\"streaming-async\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"A chat — Chat","title":"A chat — Chat","text":"Chat sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chat — Chat","text":"Chat object","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"A chat — Chat","text":"Chat$new() Chat$get_turns() Chat$set_turns() Chat$add_turn() Chat$get_system_prompt() Chat$get_model() Chat$set_system_prompt() Chat$get_tokens() Chat$get_cost() Chat$last_turn() Chat$chat() Chat$chat_structured() Chat$chat_structured_async() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$get_provider() Chat$get_tools() Chat$set_tools() Chat$on_tool_request() Chat$on_tool_result() Chat$extract_data() Chat$extract_data_async() Chat$clone()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$new(provider, system_prompt = NULL, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"provider provider object. system_prompt System prompt start conversation . echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-turns-","dir":"Reference","previous_headings":"","what":"Method get_turns()","title":"A chat — Chat","text":"Retrieve turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_turns(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-turns-","dir":"Reference","previous_headings":"","what":"Method set_turns()","title":"A chat — Chat","text":"Replace existing turns new list.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_turns(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value list Turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-add-turn-","dir":"Reference","previous_headings":"","what":"Method add_turn()","title":"A chat — Chat","text":"Add pair turns chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$add_turn(user, system)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"user user Turn. system system Turn.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-system-prompt-","dir":"Reference","previous_headings":"","what":"Method get_system_prompt()","title":"A chat — Chat","text":"set, system prompt, , NULL.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_system_prompt()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-model-","dir":"Reference","previous_headings":"","what":"Method get_model()","title":"A chat — Chat","text":"Retrieve model name","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_model()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-system-prompt-","dir":"Reference","previous_headings":"","what":"Method set_system_prompt()","title":"A chat — Chat","text":"Update system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_system_prompt(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"value character vector giving new system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tokens-","dir":"Reference","previous_headings":"","what":"Method get_tokens()","title":"A chat — Chat","text":"data frame tokens column proides number input tokens used user turns number output tokens used assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_tokens(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"A chat — Chat","text":"cost chat","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_cost(include = c(\"all\", \"last\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"include default, \"\", gives total cumulative cost chat. Alternatively, use \"last\" get cost just recent turn.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"A chat — Chat","text":"last turn returned assistant.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"A chat — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-","dir":"Reference","previous_headings":"","what":"Method chat_structured()","title":"A chat — Chat","text":"Extract structured data","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_structured(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-async-","dir":"Reference","previous_headings":"","what":"Method chat_structured_async()","title":"A chat — Chat","text":"Extract structured data, asynchronously. Returns promise resolves object matching type specification.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_structured_async(..., type, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"A chat — Chat","text":"Submit input chatbot, receive promise resolves response . Returns promise resolves string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$chat_async(..., tool_mode = c(\"concurrent\", \"sequential\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"A chat — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream(..., stream = c(\"text\", \"content\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"A chat — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$stream_async(   ...,   tool_mode = c(\"concurrent\", \"sequential\"),   stream = c(\"text\", \"content\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"A chat — Chat","text":"Register tool (R function) chatbot can use. chatbot decides use function, ellmer automatically call submit results back. return value function. Generally, either string, JSON-serializable value. must direct control structure JSON returned, can return JSON-serializable value wrapped base::(), ellmer leave alone entire request JSON-serialized.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$register_tool(tool_def)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tool_def Tool definition created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-provider-","dir":"Reference","previous_headings":"","what":"Method get_provider()","title":"A chat — Chat","text":"Get underlying provider object. expert use .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_provider()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tools-","dir":"Reference","previous_headings":"","what":"Method get_tools()","title":"A chat — Chat","text":"Retrieve list registered tools.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$get_tools()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-tools-","dir":"Reference","previous_headings":"","what":"Method set_tools()","title":"A chat — Chat","text":"Sets available tools. expert use ; users use register_tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$set_tools(tools)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-request-","dir":"Reference","previous_headings":"","what":"Method on_tool_request()","title":"A chat — Chat","text":"Register callback tool request event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$on_tool_request(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"callback function called tool request event occurs, must request argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-result-","dir":"Reference","previous_headings":"","what":"Method on_tool_result()","title":"A chat — Chat","text":"Register callback tool result event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$on_tool_result(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"callback function called tool result event occurs, must result argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"A chat — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-extract-data-","dir":"Reference","previous_headings":"","what":"Method extract_data()","title":"A chat — Chat","text":"Deprecated favour $chat_structured().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... See $chat_structured()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$extract_data_async(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"... See $chat_structured_async()","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"A chat — Chat","text":"objects class cloneable method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-24","dir":"Reference","previous_headings":"","what":"Usage","title":"A chat — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-20","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chat — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chat — Chat","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4.1\". chat$chat(\"Tell me a funny joke\") #> Why did the scarecrow win an award? #>  #> Because he was outstanding in his field!"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"Use functions writing package extends ellmer need customise methods various types content. normal use, see content_image_url() friends. ellmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents types content can either sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user). object automatically created value returned calling tool() function. Alternatively, expert users can return ContentToolResult tool() function include additional data customize display result.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = stop(\"Required\"))  ContentImage()  ContentImageRemote(url = stop(\"Required\"), detail = \"\")  ContentImageInline(type = stop(\"Required\"), data = NULL)  ContentToolRequest(   id = stop(\"Required\"),   name = stop(\"Required\"),   arguments = list(),   tool = NULL )  ContentToolResult(value = NULL, error = NULL, extra = list(), request = NULL)  ContentThinking(thinking = stop(\"Required\"), extra = list())  ContentPDF(type = stop(\"Required\"), data = stop(\"Required\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result). Automatically managed ellmer. name Function name arguments Named list arguments call function . tool ellmer automatically matches tool request tools defined chatbot. NULL, request match defined tool. value results calling tool function, succeeded. error error message, string, error condition thrown result failure calling tool function. Must NULL tool call successful. extra Additional data. request ContentToolRequest associated tool result, automatically added ellmer evaluating tool call. thinking text thinking output.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content types received from and sent to a chatbot — Content","text":"S7 objects inherit Content","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content() #> <ellmer::Content> ContentText(\"Tell me a joke\") #> <ellmer::ContentText> #>  @ text: chr \"Tell me a joke\" ContentImageRemote(\"https://www.r-project.org/Rlogo.png\") #> <ellmer::ContentImageRemote> #>  @ url   : chr \"https://www.r-project.org/Rlogo.png\" #>  @ detail: chr \"\" ContentToolRequest(id = \"abc\", name = \"mean\", arguments = list(x = 1:5)) #> <ellmer::ContentToolRequest> #>  @ id       : chr \"abc\" #>  @ name     : chr \"mean\" #>  @ arguments:List of 1 #>  .. $ x: int [1:5] 1 2 3 4 5 #>  @ tool     : NULL"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = stop(\"Required\"),   model = stop(\"Required\"),   base_url = stop(\"Required\"),   params = list(),   extra_args = list() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"name Name provider. model Name model. base_url base URL API. params list standard parameters created params(). extra_args Arbitrary extra arguments included request body.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chatbot provider — Provider","text":"S7 Provider object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = \"CoolModels\",   model = \"my_model\",   base_url = \"https://cool-models.com\" ) #> <ellmer::Provider> #>  @ name      : chr \"CoolModels\" #>  @ model     : chr \"my_model\" #>  @ base_url  : chr \"https://cool-models.com\" #>  @ params    : list() #>  @ extra_args: list()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user or assistant turn — Turn","title":"A user or assistant turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, ellmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user or assistant turn — Turn","text":"","code":"Turn(role, contents = list(), json = list(), tokens = c(0, 0))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user or assistant turn — Turn","text":"role Either \"user\", \"assistant\", \"system\". contents list Content objects. json serialized JSON corresponding underlying data turns. Currently provided assistant. useful information returned provider ellmer otherwise expose. tokens numeric vector length 2 representing number input output tokens (respectively) used turn. Currently recorded assistant turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A user or assistant turn — Turn","text":"S7 Turn object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A user or assistant turn — Turn","text":"","code":"Turn(role = \"user\", contents = list(ContentText(\"Hello, world!\"))) #> <Turn: user> #> Hello, world!"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":null,"dir":"Reference","previous_headings":"","what":"Type definitions for function calling and structured data extraction. — Type","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 classes provided use package devlopers extending ellmer. every day use, use type_boolean() friends.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(description = NULL, required = TRUE, type = stop(\"Required\"))  TypeEnum(description = NULL, required = TRUE, values = character(0))  TypeArray(description = NULL, required = TRUE, items = Type())  TypeJsonSchema(description = NULL, required = TRUE, json = list())  TypeObject(   description = NULL,   required = TRUE,   properties = list(),   additional_properties = TRUE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type definitions for function calling and structured data extraction. — Type","text":"description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. type Basic type name. Must one boolean, integer, number, string. values Character vector permitted values. items type array items. Can created type_ function. json JSON schema object list. properties Named list properties stored inside object. element S7 Type object.` additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 objects inheriting Type","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(type = \"boolean\") #> <ellmer::TypeBasic> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ type       : chr \"boolean\" TypeArray(items = TypeBasic(type = \"boolean\")) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"boolean\""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in one batch — batch_chat","title":"Submit multiple chats in one batch — batch_chat","text":"batch_chat() batch_chat_structured() currently work chat_openai() chat_anthropic(). use OpenAI Anthropic batch APIs allow submit multiple requests simultaneously. results can take 24 hours complete, return pay 50% less usual (note ellmer include discount pricing metadata). want get results back quickly, working different provider, may want use parallel_chat() instead. Since batched requests can take long time complete, batch_chat() requires file path used store information batch never lose work. can either set wait = FALSE simply interrupt waiting process, later, either call batch_chat() resume left call batch_chat_completed() see results ready retrieve. batch_chat() store chat responses file, can either keep around cache results, delete free disk space. API marked experimental since yet know handle errors helpful way. Fortunately seem common, ideas, please let know!","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"batch_chat(chat, prompts, path, wait = TRUE)  batch_chat_structured(   chat,   prompts,   path,   type,   wait = TRUE,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE )  batch_chat_completed(chat, prompts, path)"},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in one batch — batch_chat","text":"chat base chat object. prompts vector created interpolate() list character vectors. path Path file (.json extension) store state. file records hash provider, prompts, existing chat turns. attempt reuse file different, get error. wait TRUE, wait batch complete. FALSE, return NULL batch complete, can retrieve results later re-running batch_chat() batch_chat_completed() TRUE. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"chat <- chat_openai(model = \"gpt-4.1-nano\")  # Chat ----------------------------------------------------------------------  prompts <- interpolate(\"What do people from {{state.name}} bring to a potluck dinner?\") if (FALSE) { # \\dontrun{ chats <- batch_chat(chat, prompts, path = \"potluck.json\") chats } # }  # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) if (FALSE) { # \\dontrun{ data <- batch_chat_structured(   chat = chat,   prompts = prompts,   path = \"people-data.json\",   type = type_person ) data } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_anthropic","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Anthropic provides number chat based models Claude moniker. Note Claude Pro membership give ability call models via API; instead, need sign (pay ) developer account.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat_anthropic(   system_prompt = NULL,   params = NULL,   max_tokens = deprecated(),   model = NULL,   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   beta_headers = character(),   api_key = anthropic_key(),   echo = NULL )  models_anthropic(   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). max_tokens Maximum number tokens generate stopping. model model use chat (defaults \"claude-sonnet-4-20250514\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_anthropic() see options. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). base_url base URL endpoint; default uses OpenAI. beta_headers Optionally, character vector beta headers opt-claude features still beta. api_key API key use authentication. generally supply directly, instead set ANTHROPIC_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-20250514\". chat$chat(\"Tell me three jokes about statisticians\") #> Here are three jokes about statisticians: #>  #> 1. A statistician was told he was average looking. He was delighted—it #> meant he was just as ugly as half the population, and twice as good  #> looking as the other half! #>  #> 2. How do you tell the difference between an introverted statistician  #> and an extroverted statistician? The introverted one stares at their  #> own shoes during a conversation. The extroverted one stares at *your*  #> shoes. #>  #> 3. A statistician goes to the doctor and says, \"Doctor, I think I'm  #> having an identity crisis.\" The doctor asks, \"What makes you think  #> that?\" The statistician replies, \"Well, I'm just not feeling normal... #> but then again, I never really believed in the normal distribution  #> anyway.\""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an AWS bedrock model — chat_aws_bedrock","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"AWS Bedrock provides number language models, including Anthropic's Claude, using Bedrock Converse API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Authentication handled {paws.common}, authentication work automatically, need follow advice https://www.paws-r-sdk.com/#credentials. particular, org uses AWS SSO, need run aws sso login terminal.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"chat_aws_bedrock(   system_prompt = NULL,   model = NULL,   profile = NULL,   api_args = list(),   echo = NULL )  models_aws_bedrock(profile = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"system_prompt system prompt set behavior assistant. model model use chat (defaults \"anthropic.claude-3-5-sonnet-20240620-v1:0\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_models_aws_bedrock() see options. . ellmer provides default model, guarantee access , need specify model can. using cross-region inference, need use inference profile ID, e.g. model=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\". profile AWS profile use. api_args Named list arbitrary extra arguments appended body every chat API call. useful arguments include:   echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":"api_args = list(   inferenceConfig = list(     maxTokens = 100,     temperature = 0.7,     topP = 0.9,     topK = 20   ) )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage chat <- chat_aws_bedrock() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"chat_azure_openai() supports API keys credentials parameter, also makes use : Azure service principals (AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET environment variables set). Interactive Entra ID authentication, like Azure CLI. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"chat_azure_openai(   endpoint = azure_endpoint(),   deployment_id,   params = NULL,   api_version = NULL,   system_prompt = NULL,   api_key = NULL,   token = deprecated(),   credentials = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT envinronment variable. deployment_id Deployment id model want use. params Common model parameters, usually created params(). api_version API version use. system_prompt system prompt set behavior assistant. api_key API key use authentication. generally supply directly, instead set AZURE_OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). token literal Azure token use authentication. Deprecated favour ambient Azure credentials explicit credentials argument. credentials list authentication headers pass httr2::req_headers(), function returns , NULL use token api_key generate headers instead. escape hatch allows users incorporate Azure credentials generated packages ellmer, manage lifetime credentials need refreshed. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_azure_openai(deployment_id = \"gpt-4o-mini\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on CloudFlare — chat_cloudflare","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Cloudflare works AI hosts variety open-source AI models. use Cloudflare API, must Account ID Access Token, can obtain following instructions.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Tool calling appear work. Images appear work.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"chat_cloudflare(   account = cloudflare_account(),   system_prompt = NULL,   params = NULL,   api_key = cloudflare_key(),   model = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"account Cloudflare account ID. Taken CLOUDFLARE_ACCOUNT_ID env var, defined. system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set HUGGINGFACE_API_KEY environment variable. model model use chat (defaults \"meta-llama/Llama-3.3-70b-instruct-fp8-fast\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_cloudflare() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Chat LLM-powered Snowflake Cortex Analyst.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"chat_cortex_analyst() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Unlike comparable model APIs, Cortex take system prompt. Instead, caller must provide \"semantic model\" describing available tables, meaning, verified queries can run starting point. semantic model can passed YAML string via reference existing file Snowflake Stage. Note Cortex support multi-turn, remember previous messages. support registering tools, attempting result error. See chat_snowflake() chat general-purpose models hosted Snowflake.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"","code":"chat_cortex_analyst(   account = snowflake_account(),   credentials = NULL,   model_spec = NULL,   model_file = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model_spec semantic model specification, NULL using model_file instead. model_file Path semantic model file stored Snowflake Stage, NULL using model_spec instead. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cortex_analyst.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a chatbot that speaks to the Snowflake Cortex Analyst — chat_cortex_analyst","text":"","code":"if (FALSE) { # has_credentials(\"cortex\") chat <- chat_cortex_analyst(   model_file = \"@my_db.my_schema.my_stage/model.yaml\" ) chat$chat(\"What questions can I ask?\") }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Databricks — chat_databricks","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks provides ---box access number foundation models can also serve gateway external models hosted third party.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Databricks — chat_databricks","text":"chat_databricks() picks ambient Databricks credentials subset Databricks client unified authentication model. Specifically, supports: Personal access tokens Service principals via OAuth (OAuth M2M) User account via OAuth (OAuth U2M) Authentication via Databricks CLI Posit Workbench-managed credentials Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks models support images, support structured outputs tool calls models.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"chat_databricks(   workspace = databricks_workspace(),   system_prompt = NULL,   model = NULL,   token = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Databricks — chat_databricks","text":"workspace URL Databricks workspace, e.g. \"https://example.cloud.databricks.com\". use value environment variable DATABRICKS_HOST, set. system_prompt system prompt set behavior assistant. model model use chat (defaults \"databricks-claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Available foundational models include: databricks-claude-3-7-sonnet (default) databricks-mixtral-8x7b-instruct databricks-meta-llama-3-1-70b-instruct databricks-meta-llama-3-1-405b-instruct token authentication token Databricks workspace, NULL use ambient credentials. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_databricks() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on DeepSeek — chat_deepseek","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Sign https://platform.deepseek.com.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Structured data extraction supported. Images supported.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"chat_deepseek(   system_prompt = NULL,   base_url = \"https://api.deepseek.com\",   api_key = deepseek_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses DeepSeek. api_key API key use authentication. generally supply directly, instead set DEEPSEEK_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"deepseek-chat\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_deepseek() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub (via Azure) hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub model marketplace.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   base_url = \"https://models.inference.ai.azure.com/\",   api_key = github_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead manage GitHub credentials described https://usethis.r-lib.org/articles/git-credentials.html. headless environments, also look GITHUB_PAT env var. model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_github() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Google's AI offering broken two parts: Gemini Vertex AI. enterprises likely use Vertex AI, individuals likely use Gemini. Use google_upload() upload files (PDFs, images, video, audio, etc.)","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"default, chat_google_gemini() use Google's default application credentials API key provided. requires gargle package. can also pick viewer-based credentials Posit Connect. turn requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"chat_google_gemini(   system_prompt = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL )  chat_google_vertex(   location,   project_id,   system_prompt = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL )  models_google_gemini(   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL )  models_google_vertex(location, project_id)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). Gemini, can alternatively set GEMINI_API_KEY. model model use chat (defaults \"gemini-2.0-flash\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_google_gemini() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. location Location, e.g. us-east1, -central1, africa-south1. project_id Project ID.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. function lightweight wrapper around chat_openai() defaults tweaked groq.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Groq — chat_groq","text":"groq currently support structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = groq_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GROQ_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"llama3-8b-8192\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Groq — chat_groq","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_groq() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Hugging Face hosts variety open-source proprietary AI models available via Inference API. use Hugging Face API, must Access Token, can obtain Hugging Face account (ensure least \"Make calls Inference Providers\" \"Make calls Inference Endpoints\" checked). function lightweight wrapper around chat_openai(), defaults adjusted Hugging Face.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Parameter support hit miss. Tool calling currently broken API. images technically supported, find models returned useful respones. models support chat interface parts , example google/gemma-2-2b-support system prompt. need carefully choose model. overall, something recommend moment.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"chat_huggingface(   system_prompt = NULL,   params = NULL,   api_key = hf_key(),   model = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set HUGGINGFACE_API_KEY environment variable. model model use chat (defaults \"meta-llama/Llama-3.1-8B-Instruct\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_huggingface() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Get API key https://console.mistral.ai/api-keys.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Tool calling unstable. Images require model supports images.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"chat_mistral(   system_prompt = NULL,   params = NULL,   api_key = mistral_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key API key use authentication. generally supply directly, instead set MISTRAL_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"mistral-large-latest\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_mistral() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local Ollama model — chat_ollama","title":"Chat with a local Ollama model — chat_ollama","text":"use chat_ollama() first download install Ollama. install models either command line (e.g. ollama pull llama3.1) within R using ollamar (e.g. ollamar::pull(\"llama3.1\")). function lightweight wrapper around chat_openai() defaults tweaked ollama.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a local Ollama model — chat_ollama","text":"Tool calling supported streaming (.e. echo \"text\" \"\") Models can use 2048 input tokens, way get use , except creating custom model different default. Tool calling generally seems quite weak, least models tried .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   base_url = \"http://localhost:11434\",   model,   seed = NULL,   api_args = list(),   echo = NULL,   api_key = NULL )  models_ollama(base_url = \"http://localhost:11434\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local Ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. model model use chat. Use models_ollama() see options. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_key Ollama require API key local usage cases need provide api_key. However, accessing Ollama instance hosted behind reverse proxy secured endpoint enforces bearer‐token authentication, can set api_key (OLLAMA_API_KEY environment variable).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a local Ollama model — chat_ollama","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_ollama(model = \"llama3.2\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"OpenAI provides number chat-based models, mostly ChatGPT brand. Note ChatGPT Plus membership grant access API. need sign developer account (pay ) developer platform.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = openai_key(),   model = NULL,   params = NULL,   seed = lifecycle::deprecated(),   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )  models_openai(base_url = \"https://api.openai.com/v1\", api_key = openai_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"gpt-4.1\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_openai() see options. params Common model parameters, usually created params(). seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") #> - **Printing**:   #>   - *Data Frames*: Display all rows and columns when printed, which  #> can overwhelm the console.   #>   - *Tibbles*: Show only the first 10 rows and the columns that fit on #> the screen, providing a more readable output. #>  #> - **Subsetting**:   #>   - *Data Frames*: Simplifies to vectors when extracting a single  #> column using single brackets (`df[, 1]` returns a vector).   #>   - *Tibbles*: Always returns another tibble when subsetting using  #> single brackets (e.g., `tb[, 1]` returns a tibble). #>  #> - **Data Types**:   #>   - *Data Frames*: Convert strings to factors by default (unless you  #> set `stringsAsFactors = FALSE`).   #>   - *Tibbles*: Never convert strings to factors by default. #>  #> - **Variable Name Handling**:   #>   - *Data Frames*: Allow row names, check and modify variable names to #> be syntactically valid in R.   #>   - *Tibbles*: Do not use row names and allow non-syntactic variable  #> names (e.g., names with spaces or starting with a number). #>  #> - **Recycling Rules**:   #>   - *Data Frames*: Will recycle input of length 1 to the number of  #> rows.   #>   - *Tibbles*: Are stricter and do not recycle inputs automatically  #> (except for columns of length 1). #>  #> - **Origin**:   #>   - *Data Frames*: Base R structure.   #>   - *Tibbles*: Enhanced version provided by the **tibble** (and  #> tidyverse) package. #>  #> - **Package Dependency**:   #>   - *Data Frames*: Available in base R, requires no additional  #> packages.   #>   - *Tibbles*: Require loading the **tibble** or **tidyverse**  #> package. #>  #> - **Enhanced Features**:   #>   - *Tibbles*: Support enhanced and user-friendly data handling (e.g., #> better type checking, nicer printing, enhanced subsetting). #>  #> Both tibbles and data frames are used to represent tabular data in R,  #> but tibbles are a modern reimagining with improved usability and  #> stricter rules.  chat$chat(\"Tell me three funny jokes about statisticians\") #> Sure! Here are three jokes about statisticians: #>  #> 1.   #> Why did the statistician drown while crossing a river?   #> Because it was **three feet deep on average**! #>  #> 2.   #> How do statisticians play hide and seek?   #> They stand **behind a tree and say, \"Now you see me, now you don’t,\"** #> and call it a confidence interval. #>  #> 3.   #> Why don’t statisticians like to play cards in the jungle?   #> Because there are **too many cheetahs!**"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Sign https://openrouter.ai. Support features depends underlying model use; see https://openrouter.ai/models details.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"chat_openrouter(   system_prompt = NULL,   api_key = openrouter_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"system_prompt system prompt set behavior assistant. api_key API key use authentication. generally supply directly, instead set OPENROUTER_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openrouter() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. Perplexity AI platform running LLMs capable searching web real-time help answer questions information may available model trained. function lightweight wrapper around chat_openai() defaults tweaked Perplexity AI.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = perplexity_key(),   model = NULL,   seed = NULL,   api_args = list(),   echo = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PERPLEXITY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). model model use chat (defaults \"llama-3.1-sonar-small-128k-online\"). regularly update default, strongly recommend explicitly specifying model anything casual use. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_perplexity() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on PortkeyAI — chat_portkey","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"PortkeyAI provides interface (AI Gateway) connect Universal API variety LLMs providers single endpoint.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"API keys together configurations LLM providers stored inside Portkey application.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"chat_portkey(   system_prompt = NULL,   base_url = \"https://api.portkey.ai/v1\",   api_key = portkeyai_key(),   virtual_key = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL )  models_portkey(   base_url = \"https://api.portkey.ai/v1\",   api_key = portkeyai_key(),   virtual_key = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set PORTKEY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). virtual_key virtual identifier storing LLM provider's API key. See documentation. model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_openai() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_portkey(virtual_key = Sys.getenv(\"PORTKEY_VIRTUAL_KEY\")) chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Snowflake — chat_snowflake","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Snowflake provider allows interact LLM models available Cortex LLM REST API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"chat_snowflake() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Note Snowflake-hosted models support images tool calling. See chat_cortex_analyst() chat Snowflake Cortex Analyst rather general-purpose model.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"chat_snowflake(   system_prompt = NULL,   account = snowflake_account(),   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"system_prompt system prompt set behavior assistant. account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model model use chat (defaults \"claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"if (FALSE) { # has_credentials(\"cortex\") chat <- chat_snowflake() chat$chat(\"Tell me a joke in the form of a SQL query.\") }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted by vLLM — chat_vllm","title":"Chat with a model hosted by vLLM — chat_vllm","text":"vLLM open source library provides efficient convenient LLMs model server. can use chat_vllm() connect endpoints powered vLLM.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"chat_vllm(   base_url,   system_prompt = NULL,   model,   seed = NULL,   api_args = list(),   api_key = vllm_key(),   echo = NULL )  models_vllm(base_url, api_key = vllm_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted by vLLM — chat_vllm","text":"base_url base URL endpoint; default uses OpenAI. system_prompt system prompt set behavior assistant. model model use chat. Use models_vllm() see options. seed Optional integer seed ChatGPT uses try make output reproducible. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_key API key use authentication. generally supply directly, instead set VLLM_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted by vLLM — chat_vllm","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_vllm(\"http://my-vllm.com\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode images for chat input — content_image_url","title":"Encode images for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode images for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode images for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode images for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode images for chat input — content_image_url","text":"","code":"chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4.1\". chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"ellmer\")) ) #> Here’s what I see in the images: #>  #> 1. **First Image**:   #>    This is the logo of the **R programming language**. It features a  #> large blue letter “R” overlaid on an elliptical, grey shape that is  #> meant to look like a stylized “R” as well. R is a statistical  #> computing and graphics language widely used among statisticians and  #> data analysts. #>  #> 2. **Second Image**:   #>    This is the hex logo for the **httr2** package, an R package for  #> working with HTTP in R (handling web requests). The design includes a  #> figure of a baseball player swinging a bat, which is a playful nod to  #> the “httr2” name, echoing the action of \"hitting\" (as in hitting a web #> endpoint). The logo also features the \"www\" baseball, reinforcing its  #> web-related purpose. #>  #> Both images relate to R and its ecosystem!  plot(waiting ~ eruptions, data = faithful)  chat <- chat_openai(echo = TRUE) #> Using model = \"gpt-4.1\". chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) #> This is a three-dimensional surface plot with the x-axis and y-axis  #> both ranging from -5 to 5, and the z-axis representing the function  #> value. The surface displays a series of concentric waves or ripples  #> centered at the origin, forming a pattern of alternating peaks and  #> valleys. The plot has radial symmetry, with the amplitude of the  #> ripples decreasing as they move away from the center. The overall  #> shape resembles a damped oscillation or a ripple caused by a  #> disturbance at the center of a calm surface."},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode PDFs content for chat input — content_pdf_file","title":"Encode PDFs content for chat input — content_pdf_file","text":"functions used prepare PDFs input chatbot. content_pdf_url() function used provide URL PDF file, content_pdf_file() used local PDF files. providers support PDF input, check documentation provider using.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode PDFs content for chat input — content_pdf_file","text":"","code":"content_pdf_file(path)  content_pdf_url(url)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode PDFs content for chat input — content_pdf_file","text":"path, url Path URL PDF file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode PDFs content for chat input — content_pdf_file","text":"ContentPDF object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Format contents into a textual representation — contents_text","title":"Format contents into a textual representation — contents_text","text":"generic functions can use convert Turn contents Content objects textual representations. contents_text() minimal includes ContentText objects output. contents_markdown() returns text content (assumes markdown convert ) plus markdown representations images content types. contents_html() returns text content, converted markdown HTML commonmark::markdown_html(), plus HTML representations images content types. content types continue grow change ellmer evolves support providers providers add content types.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format contents into a textual representation — contents_text","text":"","code":"contents_text(content, ...)  contents_html(content, ...)  contents_markdown(content, ...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format contents into a textual representation — contents_text","text":"content Turn Content object converted text. contents_markdown() also accepts Chat instances turn entire conversation history markdown text. ... Additional arguments passed methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format contents into a textual representation — contents_text","text":"string text, markdown HTML.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format contents into a textual representation — contents_text","text":"","code":"turns <- list(   Turn(\"user\", contents = list(     ContentText(\"What's this image?\"),     content_image_url(\"https://placehold.co/200x200\")   )),   Turn(\"assistant\", \"It's a placeholder image.\") )  lapply(turns, contents_text) #> [[1]] #> [1] \"What's this image?\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  lapply(turns, contents_markdown) #> [[1]] #> [1] \"What's this image?\\n\\n![](https://placehold.co/200x200)\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  if (rlang::is_installed(\"commonmark\")) {   contents_html(turns[[1]]) } #> [1] \"<p>What's this image?<\/p>\\n\\n<img src=\\\"https://placehold.co/200x200\\\">\""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right call tool(). function helps documented functions extracting function's R documentation creating tool() call , using LLM. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful Roxygen comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(   topic,   chat = NULL,   model = deprecated(),   echo = interactive(),   verbose = FALSE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. chat Chat object used generate output. NULL (default) uses chat_openai(). model lifecycle::badge(\"deprecated\") Formally used definining model used chat. Now supply chat instead. echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\")   create_tool_def(\"rnorm\", chat = chat_azure_openai()) } # }"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"deprecated-in-v-","dir":"Reference","previous_headings":"","what":"Deprecated in v0.2.0","title":"Deprecated functions — deprecated","text":"chat_azure() renamed chat_azure_openai(). chat_bedrock() renamed chat_aws_bedrock(). chat_claude() renamed chat_anthropic(). chat_gemini() renamed chat_google_gemini().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"deprecated-in-v--1","dir":"Reference","previous_headings":"","what":"Deprecated in v0.1.1","title":"Deprecated functions — deprecated","text":"chat_cortex() renamed v0.1.1 chat_cortex_analyst() distinguish general-purpose Snowflake Cortex chat function, chat_snowflake().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated functions — deprecated","text":"","code":"chat_cortex(...)  chat_azure(...)  chat_bedrock(...)  chat_claude(...)  chat_gemini(...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/deprecated.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated functions — deprecated","text":"... Additional arguments passed deprecated function replacement.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ellmer: Chat with Large Language Models — ellmer-package","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Chat large language models range providers including 'Claude' https://claude.ai, 'OpenAI' https://chatgpt.com, . Supports streaming, asynchronous calls, tool calling, structured data extraction.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co (ORCID) Authors: Joe Cheng Aaron Jacobs Garrick Aden-Buie garrick@posit.co (ORCID) contributors: Posit Software, PBC (03wc8by49) [copyright holder, funder]","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file to gemini — google_upload","title":"Upload a file to gemini — google_upload","text":"function uploads file waits Gemini finish processing can immediately use prompt. experimental currently Gemini specific, expect providers evolve similar feature future. Uploaded files automatically deleted 2 days. file must less 2 GB can upload total 20 GB. ellmer currently provide way delete files early; please file issue useful .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file to gemini — google_upload","text":"","code":"google_upload(   path,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   mime_type = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file to gemini — google_upload","text":"path Path file upload. base_url base URL endpoint; default uses OpenAI. api_key API key use authentication. generally supply directly, instead set GOOGLE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). Gemini, can alternatively set GEMINI_API_KEY. mime_type Optionally, specify mime type file. specified, guesses file extension.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a file to gemini — google_upload","text":"<ContentUploaded> object can passed $chat().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a file to gemini — google_upload","text":"","code":"if (FALSE) { # \\dontrun{ file <- google_upload(\"path/to/file.pdf\")  chat <- chat_google_gemini() chat$chat(file, \"Give me a three paragraph summary of this PDF\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Are credentials avaiable? — has_credentials","title":"Are credentials avaiable? — has_credentials","text":"Used examples/testing.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Are credentials avaiable? — has_credentials","text":"","code":"has_credentials(provider)"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Are credentials avaiable? — has_credentials","text":"provider Provider name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt: interpolate() works string. interpolate_file() works file. interpolate_package() works file insts/prompt directory package. Compared glue, dynamic values wrapped {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())  interpolate_package(package, path, ..., .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md). package Package name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helpers for interpolating data into prompts — interpolate","text":"{glue} string.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a live chat application — live_console","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_anthropic() live_console(chat) live_browser(chat) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in parallel — parallel_chat","title":"Submit multiple chats in parallel — parallel_chat","text":"multiple prompts, can submit parallel. typically considerably faster submitting sequence, especially Gemini OpenAI. using chat_openai() chat_anthropic() willing wait longer, might want use batch_chat() instead, comes 50% discount return taking 24 hours.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"parallel_chat(chat, prompts, max_active = 10, rpm = 500)  parallel_chat_structured(   chat,   prompts,   type,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE,   max_active = 10,   rpm = 500 )"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in parallel — parallel_chat","text":"chat base chat object. prompts vector created interpolate() list character vectors. max_active maximum number simultaneous requests send. chat_anthropic(), note number active connections limited primarily output tokens per minute limit (OTPM) estimated max_tokens parameter, defaults 4096. means usage tier limits 16,000 OTPM, either set max_active = 4 (16,000 / 4096) decrease number active connections use params() chat_anthropic() decrease max_tokens. rpm Maximum number requests per minute. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit multiple chats in parallel — parallel_chat","text":"parallel_chat(), list Chat objects, one prompt. parallel_chat_structured(), single structured data object one element prompt. Typically, type object, data frame one row prompt, one column property.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\".  # Chat ---------------------------------------------------------------------- country <- c(\"Canada\", \"New Zealand\", \"Jamaica\", \"United States\") prompts <- interpolate(\"What's the capital of {{country}}?\") parallel_chat(chat, prompts) #> [[1]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=13/10 $0.00> #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of Canada? #> ── assistant [10] ───────────────────────────────────────────────────── #> The capital of Canada is **Ottawa**. #>  #> [[2]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=14/8 $0.00> #> ── user [14] ────────────────────────────────────────────────────────── #> What's the capital of New Zealand? #> ── assistant [8] ────────────────────────────────────────────────────── #> The capital of New Zealand is Wellington. #>  #> [[3]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=13/10 $0.00> #> ── user [13] ────────────────────────────────────────────────────────── #> What's the capital of Jamaica? #> ── assistant [10] ───────────────────────────────────────────────────── #> The capital of Jamaica is **Kingston**. #>  #> [[4]] #> <Chat OpenAI/gpt-4.1 turns=2 tokens=14/12 $0.00> #> ── user [14] ────────────────────────────────────────────────────────── #> What's the capital of United States? #> ── assistant [12] ───────────────────────────────────────────────────── #> The capital of the United States is Washington, D.C. #>   # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) parallel_chat_structured(chat, prompts, type_person) #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard model parameters — params","title":"Standard model parameters — params","text":"helper function makes easier create list parameters used across many models. parameter names automatically standardised included correctly place API call. Note parameters supported given provider generate warning, error. allows use set parameters across multiple providers.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard model parameters — params","text":"","code":"params(   temperature = NULL,   top_p = NULL,   top_k = NULL,   frequency_penalty = NULL,   presence_penalty = NULL,   seed = NULL,   max_tokens = NULL,   log_probs = NULL,   stop_sequences = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard model parameters — params","text":"temperature Temperature sampling distribution. top_p cumulative probability token selection. top_k number highest probability vocabulary tokens keep. frequency_penalty Frequency penalty generated tokens. presence_penalty Presence penalty generated tokens. seed Seed random number generator. max_tokens Maximum number tokens generate. log_probs Include log probabilities output? stop_sequences character vector tokens stop generation . ... Additional named parameters send provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session. price shown known.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Report on token usage in the current session — token_usage","text":"data frame","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage() #>    provider           model input output price #> 1    OpenAI         gpt-4.1  2394    970 $0.01 #> 2 Anthropic claude-sonnet-4    14    168 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — tool","title":"Define a tool — tool","text":"Define R function use chatbot. function always run current R instance. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — tool","text":"","code":"tool(   .fun,   .description,   ...,   .name = NULL,   .convert = TRUE,   .annotations = list() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — tool","text":".fun function invoked tool called. return value function sent back chatbot. Expert users can customize tool result returning ContentToolResult object. .description detailed description function . Generally, information can provide , better. ... Name-type pairs define arguments accepted function. element created type_*() function. .name name function. .convert JSON inputs automatically convert R data type equivalents? Defaults TRUE. .annotations Additional properties describe tool behavior. Usually created tool_annotations(), can find description annotation properties recommended Model Context Protocol.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a tool — tool","text":"S7 ToolDef object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — tool","text":"","code":"# First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- tool(   rnorm,   \"Drawn numbers from a random normal distribution\",   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\"),   .annotations = tool_annotations(     title = \"Draw Random Normal Numbers\",     read_only_hint = TRUE,     open_world_hint = FALSE   ) ) chat <- chat_openai() #> Using model = \"gpt-4.1\". # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"   Give me five numbers from a random normal distribution. \") #> ◯ [tool call] rnorm(n = 5L, mean = 0L, sd = 1L) #> ● #> [-1.4,0.2553,-2.4373,-0.0056,0.6216] #> Here are five numbers drawn from a random normal distribution (mean =  #> 0, standard deviation = 1): #>  #> -1.4, 0.2553, -2.4373, -0.0056, 0.6216  # Look at the chat history to see how tool calling works: # Assistant sends a tool request which is evaluated locally and # results are send back in a tool result."},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":null,"dir":"Reference","previous_headings":"","what":"Tool annotations — tool_annotations","title":"Tool annotations — tool_annotations","text":"Tool annotations additional properties , passed .annotations argument tool(), provide additional information tool behavior. information can used display users, example Shiny app another user interface. annotations tool_annotations() drawn Model Context Protocol considered hints. Tool authors use annotations communicate tool properties, users note annotations guaranteed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tool annotations — tool_annotations","text":"","code":"tool_annotations(   title = NULL,   read_only_hint = NULL,   open_world_hint = NULL,   idempotent_hint = NULL,   destructive_hint = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tool annotations — tool_annotations","text":"title human-readable title tool. read_only_hint TRUE, tool modify environment. open_world_hint TRUE, tool may interact \"open world\" external entities. FALSE, tool's domain interaction closed. example, world web search tool open, world memory tool . idempotent_hint TRUE, calling tool repeatedly arguments additional effect environment. (meaningful read_only_hint FALSE.) destructive_hint TRUE, tool may perform destructive updates environment, otherwise performs additive updates. (meaningful read_only_hint FALSE.) ... Additional named parameters include tool annotations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tool annotations — tool_annotations","text":"list tool annotations.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tool annotations — tool_annotations","text":"","code":"# See ?tool() for a full example using this function. # We're creating a tool around R's `rnorm()` function to allow the chatbot to # generate random numbers from a normal distribution. tool_rnorm <- tool(   rnorm,   # Describe the tool function to the LLM   .description = \"Drawn numbers from a random normal distribution\",   # Describe the parameters used by the tool function   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\"),   # Tool annotations optionally provide additional context to the LLM   .annotations = tool_annotations(     title = \"Draw Random Normal Numbers\",     read_only_hint = TRUE, # the tool does not modify any state     open_world_hint = FALSE # the tool does not interact with the outside world   ) )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":null,"dir":"Reference","previous_headings":"","what":"Reject a tool call — tool_reject","title":"Reject a tool call — tool_reject","text":"Throws error reject tool call. tool_reject() can used within tool function indicate tool call processed. tool_reject() can also called Chat$on_tool_request() callback. used callback, tool call rejected tool function invoked. example utils::askYesNo() used ask user permission accessing current working directory. happens directly tool function appropriate write tool definition know exactly called.   can achieve similar experience tools written others using tool_request callback. next example, imagine tool provided third-party package. example implements simple menu ask user consent running  tool.","code":"chat <- chat_openai(model = \"gpt-4.1-nano\")  list_files <- function() {   allow_read <- utils::askYesNo(     \"Would you like to allow access to your current directory?\"   )   if (isTRUE(allow_read)) {     dir(pattern = \"[.](r|R|csv)$\")   } else {     tool_reject()   } }  chat$register_tool(tool(   list_files,   \"List files in the user's current directory\" ))  chat$chat(\"What files are available in my current directory?\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) no #> #> Error: Tool call rejected. The user has chosen to disallow the tool #' call. #> It seems I am unable to access the files in your current directory right now. #> If you can tell me what specific files you're looking for or if you can #' provide #> the list, I can assist you further.  chat$chat(\"Try again.\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) yes #> #> app.R #> #> data.csv #> The files available in your current directory are \"app.R\" and \"data.csv\". packaged_list_files_tool <- tool(   function() dir(pattern = \"[.](r|R|csv)$\"),   \"List files in the user's current directory\" )  chat <- chat_openai(model = \"gpt-4.1-nano\") chat$register_tool(packaged_list_files_tool)  always_allowed <- c()  # ContentToolRequest chat$on_tool_request(function(request) {   if (request@name %in% always_allowed) return()    answer <- utils::menu(     title = sprintf(\"Allow tool `%s()` to run?\", request@name),     choices = c(\"Always\", \"Once\", \"No\"),     graphics = FALSE   )    if (answer == 1) {     always_allowed <<- append(always_allowed, request@name)   } else if (answer %in% c(0, 3)) {     tool_reject()   } })  # Try choosing different answers to the menu each time chat$chat(\"What files are available in my current directory?\") chat$chat(\"How about now?\") chat$chat(\"And again now?\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reject a tool call — tool_reject","text":"","code":"tool_reject(reason = \"The user has chosen to disallow the tool call.\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reject a tool call — tool_reject","text":"reason character string describing reason rejecting tool call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reject a tool call — tool_reject","text":"Throws error class ellmer_tool_reject provided reason.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Type specifications — type_boolean","title":"Type specifications — type_boolean","text":"functions specify object types way chatbots understand used tool calling structured data extraction. names based JSON schema, APIs expect behind scenes. translation R concepts types fairly straightforward. type_boolean(), type_integer(), type_number(), type_string() represent scalars. equivalent length-1 logical, integer, double, character vectors (respectively). type_enum() equivalent length-1 factor; string can take specified values. type_array() equivalent vector R. can use represent atomic vector: e.g. type_array(items = type_boolean()) equivalent logical vector type_array(items = type_string()) equivalent character vector). can also use represent list complicated types every element type (R base equivalent ), e.g. type_array(items = type_array(items = type_string())) represents list character vectors. type_object() equivalent named list R, every element must specified type. example, type_object(= type_string(), b = type_array(type_integer())) equivalent list element called string element called b integer vector. type_from_schema() allows specify full schema want get back LLM JSON schema. useful pre-defined schema want use directly without manually creating type using type_*() functions. can point file path argument provide JSON string text. schema must valid JSON schema object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type specifications — type_boolean","text":"","code":"type_boolean(description = NULL, required = TRUE)  type_integer(description = NULL, required = TRUE)  type_number(description = NULL, required = TRUE)  type_string(description = NULL, required = TRUE)  type_enum(description = NULL, values, required = TRUE)  type_array(description = NULL, items, required = TRUE)  type_object(   .description = NULL,   ...,   .required = TRUE,   .additional_properties = FALSE )  type_from_schema(text, path)"},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type specifications — type_boolean","text":"description, .description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required, .required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. values Character vector permitted values. items type array items. Can created type_ function. ... Name-type pairs defineing components object must possess. .additional_properties Can object arbitrary additional properties explicitly listed? supported Claude. text JSON string. path file path JSON file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type specifications — type_boolean","text":"","code":"# An integer vector type_array(items = type_integer()) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"integer\"  # The closest equivalent to a data frame is an array of objects type_array(items = type_object(    x = type_boolean(),    y = type_string(),    z = type_number() )) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeObject> #>  .. @ description          : NULL #>  .. @ required             : logi TRUE #>  .. @ properties           :List of 3 #>  .. .. $ x: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"boolean\" #>  .. .. $ y: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"string\" #>  .. .. $ z: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"number\" #>  .. @ additional_properties: logi FALSE  # There's no specific type for dates, but you use a string with the # requested format in the description (it's not gauranteed that you'll # get this format back, but you should most of the time) type_string(\"The creation date, in YYYY-MM-DD format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The creation date, in YYYY-MM-DD format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\" type_string(\"The update date, in dd/mm/yyyy format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The update date, in dd/mm/yyyy format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-development-version","dir":"Changelog","previous_headings":"","what":"ellmer (development version)","title":"ellmer (development version)","text":"save Chat object disk, API keys automatically redacted. means can longer easily resume chat ’ve saved disk (’ll figure future release) ensures never accidentally save secret key RDS file (#534). chat_anthropic() now defaults Claude Sonnet 4. Add pricing information latest generation Claude models. can now use pre-existing JSON schemas structured chats using type_from_schema() (#133, @hafen) chat_databricks() now picks Databricks workspace URLs set configuration file, improve compatibility Databricks CLI (#521, @atheriel). chat_snowflake() longer streams answers include mysterious list(type = \"text\", text = \"\") trailer (#533, @atheriel). chat_snowflake() chat_databricks() now default Claude Sonnet 3.7, default chat_anthropic() (#539 #546, @atheriel). chat_snowflake() now parses streaming outputs correctly turns (#542, @atheriel). chat_snowflake() now supports structured ouputs standard model parameters (#544 #545, @atheriel). chat_databricks() now works tool calling (#548, @atheriel).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-020","dir":"Changelog","previous_headings":"","what":"ellmer 0.2.0","title":"ellmer 0.2.0","text":"CRAN release: 2025-05-17","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"breaking-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"ellmer 0.2.0","text":"made number refinements way ellmer converts JSON R data structures. breaking changes, although don’t expect affect much code wild. importantly, tools now invoked inputs coerced standard R data structures (#461); opt-setting convert = FALSE tool(). Additionally ellmer now converts NULL NA type_boolean(), type_integer(), type_number(), type_string() (#445), better job arrays required = FALSE (#384). chat_ functions longer turn argument. need set turns, can now use Chat$set_turns() (#427). Additionally, Chat$tokens() renamed Chat$get_tokens() returns data frame tokens, correctly aligned individual turn. print method now uses show many input/output tokens used turn (#354).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.2.0","text":"Two new interfaces help multiple chats single function call: batch_chat() batch_chat_structured() allow submit multiple chats OpenAI Anthropic’s batched interfaces. guarantee response within 24 hours, 50% price regular requests (#143). parallel_chat() parallel_chat_structured() work provider allow submit multiple chats parallel (#143). doesn’t give cost savings, ’s can much, much faster. new family functions experimental ’m 100% sure shape user interface correct, particularly pertains handling errors. google_upload() lets upload files Google Gemini Vertex AI (#310). allows work videos, PDFs, large files Gemini. models_google_gemini(), models_anthropic(), models_openai(), models_aws_bedrock(), models_ollama() models_vllm(), list available models Google Gemini, Anthropic, OpenAI, AWS Bedrock, Ollama, VLLM respectively. Different providers return different metadata guaranteed return data frame least id column (#296). possible (currently Gemini, Anthropic, OpenAI) include known token prices (per million tokens). interpolate() friends now vectorised can generate multiple prompts (e.g.) data frame inputs. also now return specially classed object custom print method (#445). New interpolate_package() makes easier interpolate prompts stored inst/prompts directory inside package (#164). chat_anthropic(), chat_azure(), chat_openai(), chat_gemini() now take params argument, coupled params() helper, makes easy specify common model parameters (like seed temperature) across providers. Support providers grow request (#280). ellmer now tracks cost input output tokens. cost displayed print Chat object, tokens_usage(), Chat$get_cost(). can also request costs parallel_chat_structured(). best accurately compute cost, treat estimate rather exact price. Unfortunately LLM providers currently make difficult figure exactly much queries cost (#203).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"provider-updates-0-2-0","dir":"Changelog","previous_headings":"","what":"Provider updates","title":"ellmer 0.2.0","text":"support three new providers: chat_huggingface() models hosted https://huggingface.co (#359, @s-spavound). chat_mistral() models hosted https://mistral.ai (#319). chat_portkey() models_portkey() models hosted https://portkey.ai (#363, @maciekbanas). also renamed (deprecation) functions make naming scheme consistent (#382, @gadenbuie): chat_azure_openai() replaces chat_azure(). chat_aws_bedrock() replaces chat_bedrock(). chat_anthropic() replaces chat_anthropic(). chat_google_gemini() replaces chat_gemini(). updated default model couple providers: chat_anthropic() uses Sonnet 3.7 (also now displays) (#336). chat_openai() uses GPT-4.1 (#512)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"developer-tooling-0-2-0","dir":"Changelog","previous_headings":"","what":"Developer tooling","title":"ellmer 0.2.0","text":"New Chat$get_provider() lets access underlying provider object (#202). Chat$chat_async() Chat$stream_async() gain tool_mode argument decide \"sequential\" \"concurrent\" tool calling. advanced feature primarily affects asynchronous tools (#488, @gadenbuie). Chat$stream() Chat$stream_async() gain support streaming additional content types generated tool call new stream argument. stream = \"content\" set, streaming response yields Content objects, including ContentToolRequest ContentToolResult objects used request return tool calls (#400, @gadenbuie). New Chat$on_tool_request() $on_tool_result() methods allow register callbacks run tool request tool result. callbacks can used implement custom logging actions tools called, without modifying tool function (#493, @gadenbuie). Chat$chat(echo = \"output\") replaces now-deprecated echo = \"text\" option. using echo = \"output\", additional output, tool requests results, shown occur. echo = \"none\", tool call failures emitted warnings (#366, @gadenbuie). ContentToolResult objects can now returned directly tool() function now includes additional information (#398 #399, @gadenbuie): extra: list additional data associated tool result shown chatbot. request: ContentToolRequest triggered tool call. ContentToolResult longer id property, instead tool call ID can retrieved request@id. also include error condition error property tool call fails (#421, @gadenbuie). ContentToolRequest gains tool property includes tool() definition request matched tool ellmer (#423, @gadenbuie). tool() gains .annotations argument can created tool_annotations() helper. Tool annotations described Model Context Protocol can used describe tool clients. (#402, @gadenbuie) New tool_reject() function can used reject tool request explanation rejection reason. tool_reject() can called within tool function Chat$on_tool_request() callback. latter case, rejecting tool call ensure tool function evaluated (#490, #493, @gadenbuie).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"minor-improvements-and-bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"ellmer 0.2.0","text":"requests now set custom User-Agent identifies requests come ellmer (#341). default timeout increased 5 minutes (#451, #321). chat_anthropic() now supports thinking content type (#396), content_image_url() (#347). gains beta_header argument opt-beta features (#339). (along chat_bedrock()) longer chokes receiving output consists whitespace (#376). Finally, chat_anthropic(max_tokens =) now deprecated favour chat_anthropic(params = ) (#280). chat_google_gemini() chat_google_vertex() gain ways authenticate. can use GEMINI_API_KEY set (@t-kalinowski, #513), authenticate Google default application credentials (including service accounts, etc) (#317, @atheriel) use viewer-based credentials running Posit Connect (#320, @atheriel). Authentication default application credentials requires {gargle} package. now also can now handle responses include citation metadata (#358). chat_ollama() now works tool() definitions optional arguments empty properties (#342, #348, @gadenbuie), now accepts api_key consults OLLAMA_API_KEY environment variable. needed local usage, enables bearer-token authentication Ollama running behind reverse proxy (#501, @gadenbuie). chat_openai(seed =) now deprecated favour chat_openai(params = ) (#280). create_tool_def() can now use Chat instance (#118, @pedrobtz). live_browser() now requires {shinychat} v0.2.0 later provides access app powers live_browser() via shinychat::chat_app(), well Shiny module easily including chat interface ellmer Chat object Shiny apps (#397, @gadenbuie). now initializes UI messages chat turns, rather replaying turns server-side (#381). Provider gains name model fields (#406). now reported print chat object used token_usage().","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-011","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.1","title":"ellmer 0.1.1","text":"CRAN release: 2025-02-06","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"lifecycle-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"ellmer 0.1.1","text":"option(ellmer_verbosity) longer supported; instead use standard httr2 verbosity functions, httr2::with_verbosity(); now support streaming data. chat_cortex() renamed chat_cortex_analyst() better disambiguate chat_snowflake() (also uses “Cortex”) (#275, @atheriel).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.1.1","text":"providers now wait 60s get complete response. can increase , e.g., option(ellmer_timeout_s = 120) (#213, #300). chat_azure(), chat_databricks(), chat_snowflake(), chat_cortex_analyst() now detect viewer-based credentials running Posit Connect (#285, @atheriel). chat_deepseek() provides support DeepSeek models (#242). chat_openrouter() provides support models hosted OpenRouter (#212). chat_snowflake() allows chatting models hosted Snowflake’s Cortex LLM REST API (#258, @atheriel). content_pdf_file() content_pdf_url() allow upload PDFs supported models. Models currently support PDFs Google Gemini Claude Anthropic. help @walkerke @andrie (#265).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.1.1","text":"Chat$get_model() returns model name (#299). chat_azure() greatly improved support Azure Entra ID. API keys now optional can pick ambient credentials Azure service principals attempt use interactive Entra ID authentication possible. broken--design token argument deprecated (handle refreshing tokens properly), new credentials argument can used custom Entra ID support needed instead (instance, ’re trying use tokens generated AzureAuth package) (#248, #263, #273, #257, @atheriel). chat_azure() now reports better error messages underlying HTTP requests fail (#269, @atheriel). now also defaults api_version = \"2024-10-21\" includes data structured data extraction (#271). chat_bedrock() now handles temporary IAM credentials better (#261, @atheriel) chat_bedrock() gains api_args argument (@billsanto, #295). chat_databricks() now handles DATABRICKS_HOST environment variable correctly whether includes HTTPS prefix (#252, @atheriel). also respects SPARK_CONNECT_USER_AGENT environment variable making requests (#254, @atheriel). chat_gemini() now defaults using gemini-2.0-flash model. print(Chat) longer wraps long lines, making easier read code bulleted lists (#246).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-010","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.0","title":"ellmer 0.1.0","text":"CRAN release: 2025-01-09 New chat_vllm() chat models served vLLM (#140). default chat_openai() model now GPT-4o. New Chat$set_turns() set turns. Chat$turns() now Chat$get_turns(). Chat$system_prompt() replaced Chat$set_system_prompt() Chat$get_system_prompt(). Async streaming async chat now event-driven use later::later_fd() wait efficiently curl socket activity (#157). New chat_bedrock() chat AWS bedrock models (#50). New chat$extract_data() uses structured data API available (tool calling otherwise) extract data structured according known type specification. can create specs functions type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object() (#31). general ToolArg() replaced specific type_*() functions. ToolDef() renamed tool. content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprobs (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. can now log low-level JSON request/response info setting options(ellmer_verbosity = 2). chat$register_tool() now takes object created Tool(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56). Databricks now supported via chat_databricks() (#152).","code":""}]
