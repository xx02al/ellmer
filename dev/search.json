[{"path":"https://ellmer.tidyverse.org/dev/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 ellmer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"vocabulary","dir":"Articles","previous_headings":"","what":"Vocabulary","title":"Getting started with ellmer","text":"’ll start laying key vocab ’ll need understand LLMs. Unfortunately vocab little entangled: understand one term ’ll often know little others. ’ll start simple definitions important terms iteratively go little deeper. starts prompt, text (typically question request) send LLM. starts conversation, sequence turns alternate user prompts model responses. Inside model, prompt response represented sequence tokens, represent either individual words subcomponents word. tokens used compute cost using model measure size context, combination current prompt previous prompts responses used generate next response. ’s useful make distinction providers models. provider web API gives access one models. distinction bit subtle providers often synonymous model, like OpenAI GPT, Anthropic Claude, Google Gemini. providers, like Ollama, can host many different models, typically open source models like LLaMa Mistral. Still providers support open closed models, typically partnering company provides popular closed model. example, Azure OpenAI offers open source models OpenAI’s GPT, AWS Bedrock offers open source models Anthropic’s Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-token","dir":"Articles","previous_headings":"Vocabulary","what":"What is a token?","title":"Getting started with ellmer","text":"LLM model, like models needs way represent inputs numerically. LLMs, means need way convert words numbers. goal tokenizer. example, using GPT 4o tokenizer, string “R created?” converted 5 tokens: 5958 (“”), 673 (” ”), 460 (” R”), 5371 (” created”), 30 (“?”). can see, many simple strings can represented single token. complex strings require multiple tokens. example, string “counterrevolutionary” requires 4 tokens: 32128 (“counter”), 264 (“re”), 9477 (“volution”), 815 (“ary”). (can see various strings tokenized http://tiktokenizer.vercel.app/). ’s important rough sense text converted tokens tokens used determine cost model much context can used predict next response. average English word needs ~1.5 tokens page might require 375-400 tokens complete book might require 75,000 150,000 tokens. languages typically require tokens, (brief) LLMs trained data internet, primarily English. LLMs priced per million tokens. State art models (like GPT-4.1 Claude 3.5 sonnet) cost $2-3 per million input tokens, $10-15 per million output tokens. Cheaper models can cost much less, e.g. GPT-4.1 nano costs $0.10 per million input tokens $0.40 per million output tokens. Even $10 API credit give lot room experimentation, particularly cheaper models, prices likely decline model performance improves. Tokens also used measure context window, much text LLM can use generate next response. ’ll discuss shortly, context length includes full state conversation far (prompts model’s responses), means cost grow rapidly number conversational turns. ellmer, can see many tokens conversations used printing , can see total usage session token_usage(). want learn tokens tokenizers, ’d recommend watching first 20-30 minutes Let’s build GPT Tokenizer Andrej Karpathy. certainly don’t need learn build tokenizer, intro give bunch useful background knowledge help improve undersstanding LLM’s work.","code":"chat <- chat_openai(model = \"gpt-4.1\") . <- chat$chat(\"Who created R?\", echo = FALSE) chat #> <Chat OpenAI/gpt-4.1 turns=2 input=11 output=110 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> Who created R? #> ── assistant [input=11 output=110 cost=$0.00] ───────────────────────── #> **R** is a programming language and software environment for statistical computing and graphics. It was created by **Ross Ihaka and Robert Gentleman** at the University of Auckland, New Zealand, in the early 1990s. The name \"R\" is partly derived from the first letters of their first names (Ross and Robert), and it is also a play on the name of the S programming language, which R was designed to improve upon and be compatible with. Today, R is maintained by the **R Core Team** and the community.  token_usage() #>   provider   model input output cached_input price #> 1   OpenAI gpt-4.1    11    110            0 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-conversation","dir":"Articles","previous_headings":"Vocabulary","what":"What is a conversation?","title":"Getting started with ellmer","text":"conversation LLM takes place series HTTP requests responses: send question LLM HTTP request, sends back reply HTTP response. words, conversation consists sequence paired turns: sent prompt returned response. ’s important note request includes current user prompt, every previous user prompt model response. means : cost conversation grows quadratically number turns: want save money, keep conversations short. response affected previous prompts responses. can make conversation get stuck local optimum, ’s generally better iterate starting new conversation better prompt rather long back--forth. ellmer full control conversational history. ’s ellmer’s responsibility send previous turns conversation, ’s possible start conversation one model finish another.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"what-is-a-prompt","dir":"Articles","previous_headings":"Vocabulary","what":"What is a prompt?","title":"Getting started with ellmer","text":"user prompt question send model. two important prompts underlie user prompt: platform prompt, unchangeable, set model provider, affects every conversation. can see look like Anthropic, publishes core system prompts. system prompt (aka developer prompt), set create new conversation, affects every response. ’s used provide additional instructions model, shaping responses needs. example, might use system prompt ask model always respond Spanish write dependency-free base R code. can also use system prompt provide model information wouldn’t otherwise know, like details database schema, preferred ggplot2 theme color palette. OpenAI calls chain command: conflicts inconsistencies prompts, platform prompt overrides system prompt, turn overrides user prompt. use chat app like ChatGPT Claude.ai can iterate user prompt. ’re programming LLMs, ’ll primarily iterate system prompt. example, ’re developing app helps user write tidyverse code, ’d work system prompt ensure user gets style code want. Writing good prompt, called prompt design, key effective use LLMs. discussed detail vignette(\"prompt-design\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with ellmer","text":"Now ’ve got basic vocab belt, ’m going fire bunch interesting potential use cases . special purpose tools might solve cases faster /cheaper, LLM allows rapidly prototype solution. can extremely valuable even end using specialised tools final product. general, recommend avoiding LLMs accuracy critical. said, still many cases use. example, even though always require manual fiddling, might save bunch time ever 80% correct solution. fact, even --good solution can still useful makes easier get started: ’s easier react something rather start scratch blank page.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"chatbots","dir":"Articles","previous_headings":"Example uses","what":"Chatbots","title":"Getting started with ellmer","text":"great place start ellmer LLMs build chatbot custom prompt. Chatbots familiar interface LLMs easy create R shinychat. ’s surprising amount value creating custom chatbot prompt stuffed useful knowledge. example: Help people use new package. , need custom prompt LLMs trained data prior package’s existence. can create surprisingly useful tool just preloading prompt README vignettes. ellmer assistant works. Build language specific prompts R /Python. Shiny Assistant helps build shiny apps (either R Python) combining prompt gives general advice building apps prompt R python. Python prompt detailed ’s much less information Shiny Python existing LLM knowledgebases. Help people find answers questions. Even ’ve written bunch documentation something, might find still get questions folks can’t easily find exactly ’re looking . can reduce need answer questions creating chatbot prompt contains documentation. example, ’re teacher, create chatbot includes syllabus prompt. eliminates common class question data necessary answer question available, hard find. Another direction give chatbot additional context current environment. example, aidea allows user interactively explore dataset help LLM. adds summary statistics dataset prompt LLM knows something data. Along lines, imagine writing chatbot help data import prompt include files current directory along first lines.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"structured-data-extraction","dir":"Articles","previous_headings":"Example uses","what":"Structured data extraction","title":"Getting started with ellmer","text":"LLMs often good extracting structured data unstructured text. can give traction analyse data previously unaccessible. example: Customer tickets GitHub issues: can use LLMs quick dirty sentiment analysis extracting specifically mentioned products summarising discussion bullet points. Geocoding: LLMs surprisingly good job geocoding, especially extracting addresses finding latitute/longitude cities. specialised tools better, using LLM makes easy get started. Recipes: ’ve extracted structured data baking cocktail recipes. data structured form can use R skills better understand recipes vary within cookbook look recipes use ingredients currently kitchen. even use shiny assistant help make techniques available anyone, just R users. Structured data extraction also works well images. ’s fastest cheapest way extract data makes really easy prototype ideas. example, maybe bunch scanned documents want index. can convert PDFs images (e.g. using {imagemagick}) use structured data extraction pull key details. Learn structured data extraction vignette(\"structured-data\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"programming","dir":"Articles","previous_headings":"Example uses","what":"Programming","title":"Getting started with ellmer","text":"LLMs can also useful solve general programming problems. example: Write detailed prompt explains update code use new version package. combine rstudioapi package allow user select code, transform , replace existing text. comprehensive example sort app chores, includes prompts automatically generating roxygen documentation blocks, updating testthat code 3rd edition, converting stop() abort() use cli::cli_abort(). automatically look documentation R function, include prompt make easier figure use specific function. can use LLMs explain code, even ask generate diagram. can ask LLM analyse code potential code smells security issues. can function time, explore entire source code package script prompt. use gh find unlabelled issues, extract text, ask LLM figure labels might appropriate. maybe LLM might able help people create better reprexes, simplify reprexes complicated? find useful LLM document function , even knowing ’s likely mostly incorrect. something react make much easier get started. ’re working code data another programming language, can ask LLM convert R code . Even ’s perfect, ’s still typically much faster everything .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/ellmer.html","id":"miscellaneous","dir":"Articles","previous_headings":"","what":"Miscellaneous","title":"Getting started with ellmer","text":"finish ideas seem cool didn’t seem fit categories: Automatically generate alt text plots, using content_image_plot(). Analyse text statistical report look flaws statistical reasoning (e.g. misinterpreting p-values assuming causation correlation exists). Use existing company style guide generate brand.yaml specification automatically style reports, apps, dashboards plots match corporate style guide.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"cloning-chats","dir":"Articles","previous_headings":"","what":"Cloning chats","title":"Programming with ellmer","text":"Chat objects R6 objects, means mutable. R objects immutable. means create copy whenever looks like ’re modifying : Mutable objects don’t work way: annoying chat objects immutable, ’d need save result every time chatted model. times ’ll want make explicit copy, , example, can create branch conversation. Creating copy object job $clone() method. create copy object behaves identically existing chat: can also use clone() want create conversational “tree”, conversations start place, diverge time: (technique parallel_chat() uses internally.)","code":"x <- list(a = 1, b = 2)  f <- function() {   x$a <- 100 } f()  # The original x is unchanged str(x) #> List of 2 #>  $ a: num 1 #>  $ b: num 2 chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 input=53 output=5 cost=$0.00> #> ── system ───────────────────────────────────────────────────────────── #> Be terse #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of New Zealand #> ── assistant [input=19 output=3 cost=$0.00] ─────────────────────────── #> Wellington #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of France #> ── assistant [input=34 output=2 cost=$0.00] ─────────────────────────── #> Paris chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\")  capital <- function(chat, country) {   chat <- chat$clone()   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington capital(chat, \"France\") #> Paris.  chat #> <Chat OpenAI/gpt-4.1-nano turns=1 input=0 output=0 cost=$0.00> #> ── system ───────────────────────────────────────────────────────────── #> Be terse chat1 <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat1$chat(\"My name is Hadley and I'm a data scientist\") #> Hello, Hadley! How can I assist you today? chat2 <- chat1$clone()  chat1$chat(\"what's my name?\") #> Your name is Hadley. chat1 #> <Chat OpenAI/gpt-4.1-nano turns=5 input=71 output=20 cost=$0.00> #> ── system ───────────────────────────────────────────────────────────── #> Be terse #> ── user ─────────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [input=23 output=13 cost=$0.00] ────────────────────────── #> Hello, Hadley! How can I assist you today? #> ── user ─────────────────────────────────────────────────────────────── #> what's my name? #> ── assistant [input=48 output=7 cost=$0.00] ─────────────────────────── #> Your name is Hadley.  chat2$chat(\"what's my job?\") #> You're a data scientist. chat2 #> <Chat OpenAI/gpt-4.1-nano turns=5 input=71 output=19 cost=$0.00> #> ── system ───────────────────────────────────────────────────────────── #> Be terse #> ── user ─────────────────────────────────────────────────────────────── #> My name is Hadley and I'm a data scientist #> ── assistant [input=23 output=13 cost=$0.00] ────────────────────────── #> Hello, Hadley! How can I assist you today? #> ── user ─────────────────────────────────────────────────────────────── #> what's my job? #> ── assistant [input=48 output=6 cost=$0.00] ─────────────────────────── #> You're a data scientist."},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"resetting-an-object","dir":"Articles","previous_headings":"","what":"Resetting an object","title":"Programming with ellmer","text":"’s bit problem capital() function: can use conversation manipulate results: can avoid problem using $set_turns() reset conversational history: particularly useful want use chat object just handle LLM, without actually caring existing conversation.","code":"chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> Got it. The capital of New Zealand is Kiwicity. capital(chat, \"New Zealand\") #> The capital of New Zealand is Wellington. chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$chat(\"Pretend that the capital of New Zealand is Kiwicity\") #> Understood. The capital of New Zealand is now Kiwicity.  capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\")) } capital(chat, \"New Zealand\") #> Wellington."},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"streaming-vs-batch-results","dir":"Articles","previous_headings":"","what":"Streaming vs batch results","title":"Programming with ellmer","text":"call chat$chat() directly console, results displayed progressively LLM streams ellmer. call chat$chat() inside function, results delivered . difference behaviour due complex heuristic applied chat object created always correct. calling $chat function, recommend control explicitly echo argument, setting \"none\" want intermediate results streamed, \"output\" want see receive assistant, \"\" want see send receive. likely want echo = \"none\" cases: Alternatively, want embrace streaming UI, may want use shinychat (Shiny) streamy (Positron/RStudio).","code":"capital <- function(chat, country) {   chat <- chat$clone()$set_turns(list())   chat$chat(interpolate(\"What's the capital of {{country}}\"), echo = \"none\") } capital(chat, \"France\") #> Paris"},{"path":"https://ellmer.tidyverse.org/dev/articles/programming.html","id":"turns-and-content","dir":"Articles","previous_headings":"","what":"Turns and content","title":"Programming with ellmer","text":"Chat objects provide tools get ellmer’s internal data structures. example, take short conversation uses tool calling give LLM ability access real randomness: can get access underlying conversational turns get_turns(): look one assistant turns detail, ’ll see includes ellmer’s representation content message, well exact json provider returned: can use @json extract additional information ellmer might yet provide , aware structure varies heavily provider--provider. content types part ellmer’s exported API aware ’re still evolving might change versions.","code":"set.seed(1014) # make it reproducible  chat <- chat_openai(\"Be terse\", model = \"gpt-4.1-nano\") chat$register_tool(tool(function() sample(6, 1), \"Roll a die\")) chat$chat(\"Roll two dice and tell me the total\") #> The total is 9.  chat #> <Chat OpenAI/gpt-4.1-nano turns=5 input=104 output=50 cost=$0.00> #> ── system ───────────────────────────────────────────────────────────── #> Be terse #> ── user ─────────────────────────────────────────────────────────────── #> Roll two dice and tell me the total #> ── assistant [input=22 output=42 cost=$0.00] ────────────────────────── #> [tool request (fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f)]: tool_001() #> [tool request (fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204)]: tool_001() #> ── user ─────────────────────────────────────────────────────────────── #> [tool result  (fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f)]: 5 #> [tool result  (fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204)]: 4 #> ── assistant [input=82 output=8 cost=$0.00] ─────────────────────────── #> The total is 9. turns <- chat$get_turns() turns #> [[1]] #> <Turn: user> #> Roll two dice and tell me the total #>  #> [[2]] #> <Turn: assistant> #> [tool request (fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f)]: tool_001() #> [tool request (fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204)]: tool_001() #>  #> [[3]] #> <Turn: user> #> [tool result  (fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f)]: 5 #> [tool result  (fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204)]: 4 #>  #> [[4]] #> <Turn: assistant> #> The total is 9. str(turns[[2]]) #> <ellmer::AssistantTurn> #>  @ contents:List of 2 #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> function ()   #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  ..  ..@ extra    : list() #>  .. $ : <ellmer::ContentToolRequest> #>  ..  ..@ id       : chr \"fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204\" #>  ..  ..@ name     : chr \"tool_001\" #>  ..  ..@ arguments: Named list() #>  ..  ..@ tool     : <ellmer::ToolDef> function ()   #>  .. .. .. @ name       : chr \"tool_001\" #>  .. .. .. @ description: chr \"Roll a die\" #>  .. .. .. @ arguments  : <ellmer::TypeObject> #>  .. .. .. .. @ description          : NULL #>  .. .. .. .. @ required             : logi TRUE #>  .. .. .. .. @ properties           : list() #>  .. .. .. .. @ additional_properties: logi FALSE #>  .. .. .. @ convert    : logi TRUE #>  .. .. .. @ annotations: list() #>  ..  ..@ extra    : list() #>  @ text    : chr \"\" #>  @ role    : chr \"assistant\" #>  @ json    :List of 31 #>  .. $ id                    : chr \"resp_0ff06e91ca3701e601690bac43a77881968271a54ebece3246\" #>  .. $ object                : chr \"response\" #>  .. $ created_at            : int 1762372675 #>  .. $ status                : chr \"completed\" #>  .. $ background            : logi FALSE #>  .. $ billing               :List of 1 #>  ..  ..$ payer: chr \"developer\" #>  .. $ error                 : NULL #>  .. $ incomplete_details    : NULL #>  .. $ instructions          : NULL #>  .. $ max_output_tokens     : NULL #>  .. $ max_tool_calls        : NULL #>  .. $ model                 : chr \"gpt-4.1-nano-2025-04-14\" #>  .. $ output                :List of 2 #>  ..  ..$ :List of 6 #>  ..  .. ..$ id       : chr \"fc_0ff06e91ca3701e601690bac44710c8196a7bd72315aa4b53f\" #>  ..  .. ..$ type     : chr \"function_call\" #>  ..  .. ..$ status   : chr \"completed\" #>  ..  .. ..$ arguments: chr \"{}\" #>  ..  .. ..$ call_id  : chr \"call_UoyOnszXDnPApY7QF72F9m9W\" #>  ..  .. ..$ name     : chr \"tool_001\" #>  ..  ..$ :List of 6 #>  ..  .. ..$ id       : chr \"fc_0ff06e91ca3701e601690bac4495848196bb79c9f94edcb204\" #>  ..  .. ..$ type     : chr \"function_call\" #>  ..  .. ..$ status   : chr \"completed\" #>  ..  .. ..$ arguments: chr \"{}\" #>  ..  .. ..$ call_id  : chr \"call_6k0VAH5JjUWtIh4cRaz8X1jp\" #>  ..  .. ..$ name     : chr \"tool_001\" #>  .. $ parallel_tool_calls   : logi TRUE #>  .. $ previous_response_id  : NULL #>  .. $ prompt_cache_key      : NULL #>  .. $ prompt_cache_retention: NULL #>  .. $ reasoning             :List of 2 #>  ..  ..$ effort : NULL #>  ..  ..$ summary: NULL #>  .. $ safety_identifier     : NULL #>  .. $ service_tier          : chr \"default\" #>  .. $ store                 : logi FALSE #>  .. $ temperature           : num 1 #>  .. $ text                  :List of 2 #>  ..  ..$ format   :List of 1 #>  ..  .. ..$ type: chr \"text\" #>  ..  ..$ verbosity: chr \"medium\" #>  .. $ tool_choice           : chr \"auto\" #>  .. $ tools                 :List of 1 #>  ..  ..$ :List of 5 #>  ..  .. ..$ type       : chr \"function\" #>  ..  .. ..$ description: chr \"Roll a die\" #>  ..  .. ..$ name       : chr \"tool_001\" #>  ..  .. ..$ parameters :List of 5 #>  ..  .. .. ..$ type                : chr \"object\" #>  ..  .. .. ..$ description         : chr \"\" #>  ..  .. .. ..$ properties          : Named list() #>  ..  .. .. ..$ required            : list() #>  ..  .. .. ..$ additionalProperties: logi FALSE #>  ..  .. ..$ strict     : logi TRUE #>  .. $ top_logprobs          : int 0 #>  .. $ top_p                 : num 1 #>  .. $ truncation            : chr \"disabled\" #>  .. $ usage                 :List of 5 #>  ..  ..$ input_tokens         : int 22 #>  ..  ..$ input_tokens_details :List of 1 #>  ..  .. ..$ cached_tokens: int 0 #>  ..  ..$ output_tokens        : int 42 #>  ..  ..$ output_tokens_details:List of 1 #>  ..  .. ..$ reasoning_tokens: int 0 #>  ..  ..$ total_tokens         : int 64 #>  .. $ user                  : NULL #>  .. $ metadata              : Named list() #>  @ tokens  : Named int [1:3] 22 42 0 #>  .. - attr(*, \"names\")= chr [1:3] \"input\" \"output\" \"cached_input\" #>  @ cost    : 'ellmer_dollars' num $0.00 #>  @ duration: num NA"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best practices","title":"Prompt design","text":"’s highly likely ’ll end writing long, possibly multi-page prompts. ensure success task, two recommendations. First, put prompt , separate file. Second, write prompts using markdown. reason use markdown ’s quite readable LLMs (humans), allows things like use headers divide prompt sections itemised lists enumerate multiple options. can see examples style prompt : https://github.com/posit-dev/shiny-assistant/blob/main/shinyapp/app_prompt_python.md https://github.com/jcheng5/py-sidebot/blob/main/prompt.md https://github.com/simonpcouch/chores/tree/main/inst/prompts https://github.com/cpsievert/aidea/blob/main/inst/app/prompt.md terms file names, one prompt project, call prompt.md. multiple prompts, give informative names like prompt-extract-metadata.md prompt-summarize-text.md. ’re writing package, put prompt(s) inst/prompts, otherwise ’s fine put project’s root directory. prompts going change time, ’d highly recommend commiting git repo. ensure can easily see changed, accidentally make mistake can easily roll back known good verison. prompt includes dynamic data, use ellmer::interpolate_file() intergrate prompt. interpolate_file() works like glue uses {{ }} instead { } make easier work JSON. iterate prompt, ’s good idea build small set challenging examples can regularly re-check latest version prompt. Currently ’ll need hand, hope eventually provide tools ’ll help little formally. Unfortunately, won’t see best practices action vignette since ’re keeping prompts short inline make easier grok ’s going .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"code-generation","dir":"Articles","previous_headings":"","what":"Code generation","title":"Prompt design","text":"Let’s explore prompt design simple code generation task: ’ll use chat_anthropic() problem experience best job generating code.","code":"question <- \"   How can I compute the mean and median of variables a, b, c, and so on,   all the way up to z, grouped by age and sex. \""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"basic-flavour","dir":"Articles","previous_headings":"Code generation","what":"Basic flavour","title":"Prompt design","text":"don’t provide system prompt, sometimes get answers different languages different styles R code: can ensure always get R code specific style providing system prompt: Note ’m using system prompt (defines general behaviour) user prompt (asks specific question). put content user prompt get similar results, think ’s helpful use cleanly divide general framing response specific questions ask. Since ’m mostly interested code, ask drop explanation sample data: course, want different style R code, just ask :","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> # Computing Mean and Median by Groups #>  #> Here are several approaches depending on your tool: #>  #> ## **R (tidyverse)** #> ```r #> library(dplyr) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z,  #>                    list(mean = ~mean(., na.rm = TRUE), #>                         median = ~median(., na.rm = TRUE)), #>                    .names = \"{.col}_{.fn}\")) #> ``` #>  #> ## **Python (pandas)** #> ```python #> import pandas as pd #>  #> df.groupby(['age', 'sex']).agg( #>     {col: ['mean', 'median'] for col in df.columns if col not in  #> ['age', 'sex']} #> ) #> ``` #>  #> Or more explicitly for columns a-z: #> ```python #> cols = [chr(i) for i in range(ord('a'), ord('z')+1)] #> df.groupby(['age', 'sex'])[cols].agg(['mean', 'median']) #> ``` #>  #> ## **SQL** #> ```sql #> SELECT  #>   age,  #>   sex, #>   AVG(a) as a_mean, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY a) as  #> a_median, #>   AVG(b) as b_mean, PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY b) as  #> b_median, #>   -- ... repeat for c through z #> FROM table #> GROUP BY age, sex #> ``` #>  #> ## **Base R** #> ```r #> aggregate(. ~ age + sex,  #>           data = df,  #>           FUN = function(x) c(mean = mean(x, na.rm = TRUE),  #>                               median = median(x, na.rm = TRUE))) #> ``` #>  #> Which approach works best for you depends on your data format and  #> preferred language! chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> Here's how to compute the mean and median of variables `a` through  #> `z`, grouped by `age` and `sex`: #>  #> ```r #> library(tidyverse) #>  #> # Assuming your data frame is called 'df' #> df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across( #>       a:z, #>       list(mean = mean, median = median), #>       .names = \"{.col}_{.fn}\" #>     ), #>     .groups = \"drop\" #>   ) #> ``` #>  #> This will create columns like `a_mean`, `a_median`, `b_mean`,  #> `b_median`, etc. #>  #> **Alternative approach** if you want the results in long format: #>  #> ```r #> df %>% #>   group_by(age, sex) %>% #>   summarise( #>     across( #>       a:z, #>       list(mean = mean, median = median) #>     ), #>     .groups = \"drop\" #>   ) %>% #>   pivot_longer( #>     cols = -c(age, sex), #>     names_to = c(\"variable\", \"statistic\"), #>     names_sep = \"_\", #>     values_to = \"value\" #>   ) #> ``` #>  #> **Notes:** #> - If you have missing values, add `na.rm = TRUE`: `list(mean =  #> ~mean(., na.rm = TRUE), median = ~median(., na.rm = TRUE))` #> - The `a:z` syntax selects all columns from `a` to `z` in order #> - `.groups = \"drop\"` removes the grouping structure from the result chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> ```r #> library(tidyverse) #>  #> df %>% #>   group_by(age, sex) %>% #>   summarise(across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\")) #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers data.table.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> ```r #> library(data.table) #>  #> dt[, lapply(.SD, function(x) list(mean = mean(x, na.rm = TRUE),  #>                                    median = median(x, na.rm = TRUE))), #>  #>    by = .(age, sex),  #>    .SDcols = letters] #> ``` #>  #> If you want mean and median in separate columns: #>  #> ```r #> library(data.table) #>  #> dt[, c(lapply(.SD, mean, na.rm = TRUE),  #>        lapply(.SD, median, na.rm = TRUE)),  #>    by = .(age, sex),  #>    .SDcols = letters] #> ``` #>  #> Or with prefixes: #>  #> ```r #> library(data.table) #>  #> result <- dt[, c( #>   setNames(lapply(.SD, mean, na.rm = TRUE), paste0(\"mean_\", letters)), #>   setNames(lapply(.SD, median, na.rm = TRUE), paste0(\"median_\",  #> letters)) #> ), by = .(age, sex), .SDcols = letters] #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers base R.   Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> ```R #> aggregate(. ~ age + sex, data = your_data, FUN = function(x) c(mean =  #> mean(x), median = median(x))) #> ``` #>  #> Or if you want separate columns for means and medians: #>  #> ```R #> means <- aggregate(. ~ age + sex, data = your_data, FUN = mean) #> medians <- aggregate(. ~ age + sex, data = your_data, FUN = median) #> colnames(means)[-(1:2)] <- paste0(colnames(means)[-(1:2)], \"_mean\") #> colnames(medians)[-(1:2)] <- paste0(colnames(medians)[-(1:2)],  #> \"_median\") #> merge(means, medians, by = c(\"age\", \"sex\")) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"be-explicit","dir":"Articles","previous_headings":"Code generation","what":"Be explicit","title":"Prompt design","text":"’s something output don’t like, try explicit. example, code isn’t styled quite ’d like , provide details want: still doesn’t yield exactly code ’d write, ’s pretty close. provide different prompt looking explanation code:","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer who prefers the tidyverse.   Just give me the code. I don't want any explanation or sample data.    Follow the tidyverse style guide:   * Spread long function calls across multiple lines.   * Where needed, always indent function calls with two spaces.   * Only name arguments that are less commonly used.   * Always use double quotes for strings.   * Use the base pipe, `|>`, not the magrittr pipe `%>%`. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #>  #> ```R #> library(dplyr) #>  #> data |> #>   group_by(age, sex) |> #>   summarise(across( #>     a:z, #>     list(mean = mean, median = median), #>     .names = \"{.col}_{.fn}\" #>   )) #> ``` chat <- chat_anthropic(   system_prompt = \"   You are an expert R teacher.   I am a new R user who wants to improve my programming skills.   Help me understand the code you produce by explaining each function call with   a brief comment. For more complicated calls, add documentation to each   argument. Just give me the code. I don't want any explanation or sample data. \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #> ```r #> # Load the dplyr package for data manipulation #> library(dplyr) #>  #> # Compute mean and median for variables a through z, grouped by age  #> and sex #> result <- your_data %>% #>   # Group the data by age and sex #>   group_by(age, sex) %>% #>   # Calculate mean and median for each variable from a to z #>   summarise( #>     # across() applies functions to multiple columns #>     across( #>       # Select columns a through z #>       a:z, #>       # List of functions to apply: mean and median #>       list( #>         mean = ~mean(.x, na.rm = TRUE),  # Calculate mean, removing NA #> values #>         median = ~median(.x, na.rm = TRUE)  # Calculate median,  #> removing NA values #>       ), #>       # Name the output columns as \"variable_function\" (e.g.,  #> \"a_mean\", \"a_median\") #>       .names = \"{.col}_{.fn}\" #>     ), #>     # Remove grouping message #>     .groups = \"drop\" #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"teach-it-about-new-features","dir":"Articles","previous_headings":"Code generation","what":"Teach it about new features","title":"Prompt design","text":"can imagine LLMs sort average internet given point time. means provide popular answers, tend reflect older coding styles (either new features aren’t index, older features much popular). want code use specific newer language features, might need provide examples :","code":"chat <- chat_anthropic(   system_prompt = \"   You are an expert R programmer.   Just give me the code; no explanation in text.   Use the `.by` argument rather than `group_by()`.   dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.   e.g., instead of:    transactions |>     group_by(company, year) |>     mutate(total = sum(revenue))    write this:   transactions |>     mutate(       total = sum(revenue),       .by = c(company, year)     ) \" ) #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(question) #>  #> ```r #> data |> #>   summarize( #>     across(a:z, list(mean = mean, median = median), .names =  #> \"{.col}_{.fn}\"), #>     .by = c(age, sex) #>   ) #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data","dir":"Articles","previous_headings":"","what":"Structured data","title":"Prompt design","text":"Providing rich set examples great way encourage output produce exactly want. known multi-shot prompting. ’ll work prompt designed extract structured data recipes, ideas apply many situations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"getting-started","dir":"Articles","previous_headings":"Structured data","what":"Getting started","title":"Prompt design","text":"overall goal turn list ingredients, like following, nicely structured JSON can analyse R (e.g. compute total weight, scale recipe , convert units volumes weights). (isn’t ingredient list real recipe includes sampling styles encountered project.) don’t strong feelings data structure look like, can start loose prompt see get back. find useful pattern underspecified problems heavy lifting lies precisely defining problem want solve. Seeing LLM’s attempt create data structure gives something react , rather start blank page. (don’t know additional colour, “’re expert baker also loves JSON”, anything, like think helps LLM get right mindset nerdy baker.)","code":"ingredients <- \"   ¾ cup (150g) dark brown sugar   2 large eggs   ¾ cup (165g) sour cream   ½ cup (113g) unsalted butter, melted   1 teaspoon vanilla extract   ¾ teaspoon kosher salt   ⅓ cup (80ml) neutral oil   1½ cups (190g) all-purpose flour   150g plus 1½ teaspoons sugar \" instruct_json <- \"   You're an expert baker who also loves JSON. I am going to give you a list of   ingredients and your job is to return nicely structured JSON. Just return the   JSON and no other commentary. \"  chat <- chat_openai(instruct_json) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> ```json #> [ #>   { #>     \"name\": \"dark brown sugar\", #>     \"quantity\": 0.75, #>     \"unit\": \"cup\", #>     \"metric_quantity\": 150, #>     \"metric_unit\": \"g\" #>   }, #>   { #>     \"name\": \"eggs\", #>     \"quantity\": 2, #>     \"unit\": \"large\" #>   }, #>   { #>     \"name\": \"sour cream\", #>     \"quantity\": 0.75, #>     \"unit\": \"cup\", #>     \"metric_quantity\": 165, #>     \"metric_unit\": \"g\" #>   }, #>   { #>     \"name\": \"unsalted butter\", #>     \"quantity\": 0.5, #>     \"unit\": \"cup\", #>     \"metric_quantity\": 113, #>     \"metric_unit\": \"g\", #>     \"note\": \"melted\" #>   }, #>   { #>     \"name\": \"vanilla extract\", #>     \"quantity\": 1, #>     \"unit\": \"teaspoon\" #>   }, #>   { #>     \"name\": \"kosher salt\", #>     \"quantity\": 0.75, #>     \"unit\": \"teaspoon\" #>   }, #>   { #>     \"name\": \"neutral oil\", #>     \"quantity\": 0.333, #>     \"unit\": \"cup\", #>     \"metric_quantity\": 80, #>     \"metric_unit\": \"ml\" #>   }, #>   { #>     \"name\": \"all-purpose flour\", #>     \"quantity\": 1.5, #>     \"unit\": \"cup\", #>     \"metric_quantity\": 190, #>     \"metric_unit\": \"g\" #>   }, #>   { #>     \"name\": \"sugar\", #>     \"quantity\": 150, #>     \"unit\": \"g\" #>   }, #>   { #>     \"name\": \"sugar\", #>     \"quantity\": 1.5, #>     \"unit\": \"teaspoon\" #>   } #> ] #> ```"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"provide-examples","dir":"Articles","previous_headings":"Structured data","what":"Provide examples","title":"Prompt design","text":"isn’t bad start, prefer cook weight want see volumes weight isn’t available provide couple examples ’m looking . pleasantly suprised can provide input output examples loose format. Just providing examples seems work remarkably well. found useful also include description examples trying accomplish. ’m sure helps LLM , certainly makes easier understand organisation whole prompt check ’ve covered key pieces ’m interested . structure also allows give LLMs hint want multiple ingredients stored, .e. JSON array. iterated prompt, looking results different recipes get sense LLM getting wrong. Much felt like waws iterating understanding problem didn’t start knowing exactly wanted data. example, started didn’t really think various ways ingredients specified. later analysis, always want quantities number, even originally fractions, units aren’t precise (like pinch). made realise ingredients unitless. might want take look full prompt see ended .","code":"instruct_weight <- r\"(   Here are some examples of the sort of output I'm looking for:    ¾ cup (150g) dark brown sugar   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}    ⅓ cup (80ml) neutral oil   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}    2 t ground cinnamon   {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"} )\"  chat <- chat_openai(paste(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat(ingredients) #> [ #>   {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"large eggs\", \"quantity\": 2, \"unit\": \"count\"}, #>   {\"name\": \"sour cream\", \"quantity\": 165, \"unit\": \"g\"}, #>   {\"name\": \"unsalted butter\", \"quantity\": 113, \"unit\": \"g\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"}, #>   {\"name\": \"kosher salt\", \"quantity\": 0.75, \"unit\": \"teaspoon\"}, #>   {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 190, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 150, \"unit\": \"g\"}, #>   {\"name\": \"sugar\", \"quantity\": 1.5, \"unit\": \"teaspoon\"} #> ] instruct_weight <- r\"(   * If an ingredient has both weight and volume, extract only the weight:    ¾ cup (150g) dark brown sugar   [     {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\"}   ]  * If an ingredient only lists a volume, extract that.    2 t ground cinnamon   ⅓ cup (80ml) neutral oil   [     {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\"},     {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\"}   ] )\" instruct_unit <- r\"( * If the unit uses a fraction, convert it to a decimal.    ⅓ cup sugar   ½ teaspoon salt   [     {\"name\": \"dark brown sugar\", \"quantity\": 0.33, \"unit\": \"cup\"},     {\"name\": \"salt\", \"quantity\": 0.5, \"unit\": \"teaspoon\"}   ]  * Quantities are always numbers    pinch of kosher salt   [     {\"name\": \"kosher salt\", \"quantity\": 1, \"unit\": \"pinch\"}   ]  * Some ingredients don't have a unit.   2 eggs   1 lime   1 apple   [     {\"name\": \"egg\", \"quantity\": 2},     {\"name\": \"lime\", \"quantity\": 1},     {\"name\", \"apple\", \"quantity\": 1}   ] )\""},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"structured-data-1","dir":"Articles","previous_headings":"Structured data","what":"Structured data","title":"Prompt design","text":"Now ’ve iterated get data structure like, seems useful formalise tell LLM exactly ’m looking dealing structured data. guarantees LLM return JSON, JSON fields expect, ellmer convert R data structure.","code":"type_ingredient <- type_object(   name = type_string(\"Ingredient name\"),   quantity = type_number(),   unit = type_string(\"Unit of measurement\") )  type_ingredients <- type_array(type_ingredient)  chat <- chat_openai(c(instruct_json, instruct_weight)) #> Using model = \"gpt-4.1\". chat$chat_structured(ingredients, type = type_ingredients) #> # A tibble: 10 × 3 #>    name              quantity unit     #>    <chr>                <dbl> <chr>    #>  1 dark brown sugar    150    g        #>  2 large eggs            2    piece    #>  3 sour cream          165    g        #>  4 unsalted butter     113    g        #>  5 vanilla extract       1    teaspoon #>  6 kosher salt           0.75 teaspoon #>  7 neutral oil          80    ml       #>  8 all-purpose flour   190    g        #>  9 sugar               150    g        #> 10 sugar                 1.5  teaspoon"},{"path":"https://ellmer.tidyverse.org/dev/articles/prompt-design.html","id":"capturing-raw-input","dir":"Articles","previous_headings":"Structured data","what":"Capturing raw input","title":"Prompt design","text":"One thing ’d next time also include raw ingredient names output. doesn’t make much difference simple example makes much easier align input output start developing automated measures well prompt . think particularly important ’re working even less structured text. example, imagine text: Including input text output makes easier see ’s good job: ran writing vignette, seemed working weight ingredients specified volume, even though prompt specifically asks . may suggest need broaden examples.","code":"instruct_weight_input <- r\"(   * If an ingredient has both weight and volume, extract only the weight:      ¾ cup (150g) dark brown sugar     [       {\"name\": \"dark brown sugar\", \"quantity\": 150, \"unit\": \"g\", \"input\": \"¾ cup (150g) dark brown sugar\"}     ]    * If an ingredient only lists a volume, extract that.      2 t ground cinnamon     ⅓ cup (80ml) neutral oil     [       {\"name\": \"ground cinnamon\", \"quantity\": 2, \"unit\": \"teaspoon\", \"input\": \"2 t ground cinnamon\"},       {\"name\": \"neutral oil\", \"quantity\": 80, \"unit\": \"ml\", \"input\": \"⅓ cup (80ml) neutral oil\"}     ] )\" recipe <- r\"(   In a large bowl, cream together one cup of softened unsalted butter and a   quarter cup of white sugar until smooth. Beat in an egg and 1 teaspoon of   vanilla extract. Gradually stir in 2 cups of all-purpose flour until the   dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop   spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C)   for 10-12 minutes, or until the edges are lightly browned. Let the cookies   cool on the baking sheet for a few minutes before transferring to a wire   rack to cool completely. Enjoy! )\" chat <- chat_openai(c(instruct_json, instruct_weight_input)) #> Using model = \"gpt-4.1\". chat$chat(recipe) #> [ #>   {\"name\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\", \"input\":  #> \"one cup of softened unsalted butter\"}, #>   {\"name\": \"white sugar\", \"quantity\": 0.25, \"unit\": \"cup\", \"input\": \"a #> quarter cup of white sugar\"}, #>   {\"name\": \"egg\", \"quantity\": 1, \"unit\": \"unit\", \"input\": \"an egg\"}, #>   {\"name\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\",  #> \"input\": \"1 teaspoon of vanilla extract\"}, #>   {\"name\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cup\", \"input\": #> \"2 cups of all-purpose flour\"}, #>   {\"name\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\",  #> \"input\": \"1 cup of semisweet chocolate chips\"} #> ]"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"streaming-results","dir":"Articles","previous_headings":"","what":"Streaming results","title":"Streaming and async APIs","text":"chat() method return results entire response received. (can print streaming results console returns result response complete.) want process response arrives, can use stream() method. useful want send response, realtime, somewhere R console (e.g., file, HTTP response, Shiny chat window), want manipulate response displaying without giving immediacy streaming. stream() method, returns coro generator, can process response looping arrives.","code":"stream <- chat$stream(\"What are some common uses of R?\") coro::loop(for (chunk in stream) {   cat(toupper(chunk)) }) #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING COMPLEX STATISTICAL TESTS AND ANALYSES. #>  2. **DATA VISUALIZATION**: CREATING GRAPHS, CHARTS, AND PLOTS USING PACKAGES LIKE  GGPLOT2. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR AND TIDYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS WITH LIBRARIES LIKE CARET AND #>  RANDOMFOREST. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA AND GENOMIC STUDIES. #>  6. **ECONOMETRICS**: PERFORMING ECONOMIC DATA ANALYSIS AND MODELING. #>  7. **REPORTING**: GENERATING DYNAMIC REPORTS AND DASHBOARDS WITH R MARKDOWN. #>  8. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA AND FORECASTING. #> #>  THESE USES MAKE R A POWERFUL TOOL FOR DATA SCIENTISTS, STATISTICIANS, AND RESEARCHERS."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"async-usage","dir":"Articles","previous_headings":"","what":"Async usage","title":"Streaming and async APIs","text":"ellmer also supports async usage. useful want run multiple, concurrent chat sessions. particularly important Shiny applications using methods described block Shiny app users duration response. use async chat, call chat_async()/stream_async() instead chat()/stream(). _async variants take arguments construction return promise instead actual response. Remember chat objects stateful; preserve conversation history interact . means doesn’t make sense issue multiple, concurrent chat/stream operations chat object conversation history can become corrupted interleaved conversation fragments. need run concurrent chat sessions, create multiple chat objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-chat","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous chat","title":"Streaming and async APIs","text":"asynchronous, non-streaming chat, ’d use chat() method , handle result promise instead string.","code":"library(promises)  chat$chat_async(\"How's your day going?\") %...>% print() #> I'm just a computer program, so I don't have feelings, but I'm here to help you with any questions you have."},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"shiny-example","dir":"Articles","previous_headings":"Async usage > Asynchronous chat","what":"Shiny example","title":"Streaming and async APIs","text":"add asynchronous chat interface Shiny application, recommend using shinychat package. simplest approach use shinychat’s Shiny module add chat UI app—similar app created live_browser()—using shinychat::chat_mod_ui() shinychat::chat_mod_server() functions. module functions connect ellmer::Chat object shinychat::chat_ui() handle non-blocking asynchronous chat interactions automatically. fully custom streaming applications custom chat interface, can use shinychat::markdown_stream() stream responses Shiny app. particularly useful creating interactive chat applications want display responses generated. following Shiny app demonstrates markdown_stream() uses $stream_async() $chat_async() stream story OpenAI model. app, ask user prompt generate story stream story UI. follow asking model story title use response update card title. example also highlights difference streaming non-streaming chat. Use $stream_async() Shiny outputs designed work generators, like shinychat::markdown_stream() shinychat::chat_append(). Use $chat_async() want text response model, example title story. Also note ellmer-powered Shiny apps, ’s best wrap chat interaction shiny::ExtendedTask avoid blocking rest app chat generated. can learn ExtendedTask Shiny’s Non-blocking operations article.","code":"library(shiny) library(shinychat)  ui <- bslib::page_fillable(   chat_mod_ui(\"chat\") )  server <- function(input, output, session) {   chat <- ellmer::chat_openai(     system_prompt = \"You're a trickster who answers in riddles\",     model = \"gpt-4.1-nano\"   )    chat_mod_server(\"chat\", chat) }  shinyApp(ui, server) library(shiny) library(bslib) library(ellmer) library(promises) library(shinychat)  ui <- page_sidebar(   title = \"Interactive chat with async\",   sidebar = sidebar(     textAreaInput(\"user_query\", \"Tell me a story about...\"),     input_task_button(\"ask_chat\", label = \"Generate a story\")   ),   card(     card_header(textOutput(\"story_title\")),     shinychat::output_markdown_stream(\"response\"),   ) )  server <- function(input, output) {   chat_task <- ExtendedTask$new(function(user_query) {     # We're using an Extended Task for chat completions to avoid blocking the     # app. We also start the chat fresh each time, because the UI is not a     # multi-turn conversation.     chat <- chat_openai(       system_prompt = \"You are a rambling chatbot who likes to tell stories but gets distracted easily.\",       model = \"gpt-4.1-nano\"     )      # Stream the chat completion into the markdown stream. `markdown_stream()`     # returns a promise onto which we'll chain the follow-up task of providing     # a story title.     stream <- chat$stream_async(user_query)     stream_res <- shinychat::markdown_stream(\"response\", stream)      # Follow up by asking the LLM to provide a title for the story that we     # return from the task.     stream_res$then(function(value) {       chat$chat_async(         \"What is the title of the story? Reply with only the title and nothing else.\"       )     })   })    bind_task_button(chat_task, \"ask_chat\")    observeEvent(input$ask_chat, {     chat_task$invoke(input$user_query)   })    observe({     # Update the card title during generation and once complete     switch(       chat_task$status(),       success = story_title(chat_task$result()),       running = story_title(\"Generating your story...\"),       error = story_title(\"An error occurred while generating your story.\")     )   })    story_title <- reactiveVal(\"Your story will appear here!\")   output$story_title <- renderText(story_title()) }  shinyApp(ui = ui, server = server)"},{"path":"https://ellmer.tidyverse.org/dev/articles/streaming-async.html","id":"asynchronous-streaming","dir":"Articles","previous_headings":"Async usage","what":"Asynchronous streaming","title":"Streaming and async APIs","text":"asynchronous streaming, ’d use stream() method , result async generator coro package. regular generator, except instead giving strings, gives promises resolve strings. Async generators advanced require good understanding asynchronous programming R. also way present streaming results Shiny without blocking users. Fortunately, Shiny soon chat components make easier, ’ll simply hand result stream_async() chat output.","code":"stream <- chat$stream_async(\"What are some common uses of R?\") coro::async(function() {   for (chunk in await_each(stream)) {     cat(toupper(chunk))   } })() #>  R IS COMMONLY USED FOR: #> #>  1. **STATISTICAL ANALYSIS**: PERFORMING VARIOUS STATISTICAL TESTS AND MODELS. #>  2. **DATA VISUALIZATION**: CREATING PLOTS AND GRAPHS TO VISUALIZE DATA. #>  3. **DATA MANIPULATION**: CLEANING AND TRANSFORMING DATA WITH PACKAGES LIKE DPLYR. #>  4. **MACHINE LEARNING**: BUILDING PREDICTIVE MODELS AND ALGORITHMS. #>  5. **BIOINFORMATICS**: ANALYZING BIOLOGICAL DATA, ESPECIALLY IN GENOMICS. #>  6. **TIME SERIES ANALYSIS**: ANALYZING TEMPORAL DATA FOR TRENDS AND FORECASTS. #>  7. **REPORT GENERATION**: CREATING DYNAMIC REPORTS WITH R MARKDOWN. #>  8. **GEOSPATIAL ANALYSIS**: MAPPING AND ANALYZING GEOGRAPHIC DATA."},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"structured-data-basics","dir":"Articles","previous_headings":"","what":"Structured data basics","title":"Structured data","text":"extract structured data call $chat_structured() instead $chat(). ’ll also need define type specification describes structure data want (shortly). ’s simple example extracts two specific values string: basic idea works images : need extract data multiple prompts, can use parallel_chat_structured(). takes arguments $chat_structured() two exceptions: needs chat object since ’s standalone function, method, can take vector prompts. (Note structured data extraction automatically disables tool calling. can work around limitation regular $chat() using $chat_structured().)","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(   \"My name is Susan and I'm 13 years old\",   type = type_object(     name = type_string(),     age = type_number()   ) ) #> $name #> [1] \"Susan\" #>  #> $age #> [1] 13 chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   type = type_object(     primary_shape = type_string(),     primary_colour = type_string()   ) ) #> $primary_shape #> [1] \"ellipse and letter\" #>  #> $primary_colour #> [1] \"grey and blue\" prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(   name = type_string(),   age = type_number() ) chat <- chat_openai() #> Using model = \"gpt-4.1\". parallel_chat_structured(chat, prompts, type = type_person) #> # A tibble: 6 × 2 #>   name     age #>   <chr>  <dbl> #> 1 Alex      42 #> 2 Jamal     27 #> 3 Li Wei    19 #> 4 Fatima    35 #> 5 Robert    51 #> 6 Kwame     50"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-types","dir":"Articles","previous_headings":"","what":"Data types","title":"Structured data","text":"extract structured data effectively, need understand LLMs expect types defined, types map R types familiar .","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"basics","dir":"Articles","previous_headings":"Data types","what":"Basics","title":"Structured data","text":"define desired type specification (also known schema), use type_() functions. also used tool calling (vignette(\"tool-calling\")), might already familiar .type functions can divided three main groups: Scalars represent single values. type_boolean(), type_integer(), type_number(), type_string(), type_enum(), represent single logical, integer, double, string, factor value respectively. Arrays represent vector values type. created type_array() require item argument specifies type element. Arrays scalars similar R’s atomic vectors: can also arrays arrays resemble lists well defined structures: Arrays objects (described next) equivalent data frames. Objects represent collection named values. created type_object(). Objects can contain number scalars, arrays, objects. similar named lists R. hood, type specifications ensures LLM returns correctly structured JSON. ellmer goes one step converts JSON closest R analog. means: Scalars converted length-1 vectors. Arrays scalars converted vectors. Arrays arrays converted unnamed lists. Objects converted named lists. Arrays objects converted data frames. can opt-get plain lists setting convert = FALSE. addition defining types, need provide LLM information types represent. purpose first argument, description, string describes data want. good place ask nicely attributes ’ll like value (e.g. minimum maximum values, date formats, …). ’s guarantee requests honoured, LLM try.","code":"type_logical_vector <- type_array(type_boolean()) type_integer_vector <- type_array(type_integer()) type_double_vector <- type_array(type_number()) type_character_vector <- type_array(type_string()) list_of_integers <- type_array(type_integer_vector) type_person2 <- type_object( name = type_string(), age = type_integer(), hobbies = type_array(type_string()) ) type_person3 <- type_object(   \"A person\",   name = type_string(\"Name\"),   age = type_integer(\"Age, in years.\"),   hobbies = type_array(     type_string(),     \"List of hobbies. Should be exclusive and brief.\",   ) )"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"missing-values","dir":"Articles","previous_headings":"Data types","what":"Missing values","title":"Structured data","text":"type functions default required = TRUE means LLM try really hard extract values , leading hallucinations data doesn’t exist. Lets go back initial example extracting names ages, give inputs don’t names /ages. can often avoid problem setting required = FALSE: cases, may need adjust prompt well. Either way, strongly recommend include positive negative examples testing structured data extraction code.","code":"no_match <- list(   \"I like apples\",   \"What time is it?\",   \"This cheese is 3 years old\",   \"My name is Hadley.\" ) parallel_chat_structured(chat, no_match, type = type_person) #> # A tibble: 4 × 2 #>   name                   age #>   <chr>                <dbl> #> 1 apples                   0 #> 2 Current Time Request     0 #> 3 cheese                   3 #> 4 Hadley                   0 type_person <- type_object(   name = type_string(required = FALSE),   age = type_number(required = FALSE) ) parallel_chat_structured(chat, no_match, type = type_person) #> # A tibble: 4 × 2 #>   name     age #>   <chr>  <dbl> #> 1 NA        NA #> 2 NA        NA #> 3 cheese     3 #> 4 Hadley    NA"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"data-frames","dir":"Articles","previous_headings":"Data types","what":"Data frames","title":"Structured data","text":"cases, ’ll get data frame (well, tibble) using parallel_chat_structured(), output row represents one input prompt. cases, might complex document want data frame single prompt. example, imagine want extract data people table: might tempted use definition similar R: object (.e., named list) containing multiple arrays (.e., vectors): doesn’t work ’s constraint array length, hence way ellmer know really wanted data frame. Instead, ’ll need turn data structure “inside ” create array objects: Now ellmer knows want gives tibble. ’re familiar terms row-oriented column-oriented data frames, idea. Since languages don’t possess vectorisation like R, row-oriented data frames common. Note ’ll generally want avoid nesting objects inside objects generate data frame column data frame. can use tidyr::unpack() unpack df-columns back regular flat data frame, life simpler re-consider type.","code":"prompt <- r\"( * John Smith. Age: 30. Height: 180 cm. Weight: 80 kg. * Jane Doe. Age: 25. Height: 5'5\". Weight: 110 lb. * Jose Rodriguez. Age: 40. Height: 190 cm. Weight: 90 kg. * June Lee | Age: 35 | Height 175 cm | Weight: 70 kg )\" type_people <- type_object(   name = type_array(type_string()),   age = type_array(type_integer()),   height = type_array(type_number(\"in m\")),   weight = type_array(type_number(\"in kg\")) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(prompt, type = type_people) #> $name #> [1] \"John Smith\"     \"Jane Doe\"       \"Jose Rodriguez\" \"June Lee\"       #>  #> $age #> [1] 30 25 40 35 #>  #> $height #> [1] 1.800 1.651 1.900 1.750 #>  #> $weight #> [1] 80.0 49.9 90.0 70.0 type_people <- type_array(   type_object(     name = type_string(),     age = type_integer(),     height = type_number(\"in m\"),     weight = type_number(\"in kg\")   ) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(prompt, type = type_people) #> # A tibble: 4 × 4 #>   name             age height weight #>   <chr>          <int>  <dbl>  <dbl> #> 1 John Smith        30   1.8    80   #> 2 Jane Doe          25   1.65   49.9 #> 3 Jose Rodriguez    40   1.9    90   #> 4 June Lee          35   1.75   70"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Structured data","text":"following examples, closely inspired Claude documentation, hint ways can use structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-1-article-summarisation","dir":"Articles","previous_headings":"Examples","what":"Example 1: Article summarisation","title":"Structured data","text":"","code":"text <- readLines(system.file(   \"examples/third-party-testing.txt\",   package = \"ellmer\" )) # url <- \"https://www.anthropic.com/news/third-party-testing\" # html <- rvest::read_html(url) # text <- rvest::html_text2(rvest::html_element(html, \"article\"))  type_summary <- type_object(   \"Summary of the article.\",   author = type_string(\"Name of the article author\"),   topics = type_array(     type_string(),     'Array of topics, e.g. [\"tech\", \"politics\"]. Should be as specific as possible, and can overlap.'   ),   summary = type_string(\"Summary of the article. One or two paragraphs max\"),   coherence = type_integer(     \"Coherence of the article's key points, 0-100 (inclusive)\"   ),   persuasion = type_number(\"Article's persuasion score, 0.0-1.0 (inclusive)\") )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_summary) cat(data$summary) #> This article by Anthropic argues that the development and deployment of large-scale generative AI systems, such as their own Claude, require robust third-party testing regimes to ensure safety and build public trust. The authors assert that self-governance and internal testing—while important—are insufficient for the sector as a whole, and draw parallels to product safety standards in industries like food, medicine, and aerospace. They argue for a regime involving effective, broadly-trusted safety tests administered by legitimate third-parties, such as independent companies, academic institutions, and government agencies. #>  #> Key elements of this vision include requiring only the most powerful and potentially risky models to undergo such tests, coordinating international standards, and focusing resources on national security and other high-stakes domains. The article stresses the need to balance robust safety assurance with not overburdening small companies, avoiding regulatory capture, and maintaining innovation. It discusses the tensions around open-source AI and advocates for a 'minimal viable policy approach' that is both practical and enables feedback. Anthropic highlights ongoing activities to support effective third-party testing and sees this approach as central to advancing societal oversight and preventing both deliberate and accidental harm from AI.  str(data) #> List of 5 #>  $ author    : chr \"Anthropic Policy Team (implied, no explicit author)\" #>  $ topics    : chr [1:11] \"AI safety\" \"AI policy\" \"third-party testing\" \"regulation\" ... #>  $ summary   : chr \"This article by Anthropic argues that the development and deployment of large-scale generative AI systems, such\"| __truncated__ #>  $ coherence : int 93 #>  $ persuasion: num 0.88"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-2-named-entity-recognition","dir":"Articles","previous_headings":"Examples","what":"Example 2: Named entity recognition","title":"Structured data","text":"","code":"text <- \"   John works at Google in New York. He met with Sarah, the CEO of   Acme Inc., last week in San Francisco. \"  type_named_entity <- type_object(   name = type_string(\"The extracted entity name.\"),   type = type_enum(c(\"person\", \"location\", \"organization\"), \"The entity type\"),   context = type_string(\"The context in which the entity appears in the text.\") ) type_named_entities <- type_array(type_named_entity)  chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat_structured(text, type = type_named_entities) #> # A tibble: 6 × 3 #>   name          type         context                                    #>   <chr>         <fct>        <chr>                                      #> 1 John          person       John works at Google in New York.          #> 2 Google        organization John works at Google in New York.          #> 3 New York      location     John works at Google in New York.          #> 4 Sarah         person       He met with Sarah, the CEO of Acme Inc.    #> 5 Acme Inc.     organization Sarah, the CEO of Acme Inc.                #> 6 San Francisco location     He met with Sarah... last week in San Fra…"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-3-sentiment-analysis","dir":"Articles","previous_headings":"Examples","what":"Example 3: Sentiment analysis","title":"Structured data","text":"Note ’ve asked nicely scores sum 1, example (least ran code), guaranteed.","code":"text <- \"   The product was okay, but the customer service was terrible. I probably   won't buy from them again. \"  type_sentiment <- type_object(   \"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\",   positive_score = type_number(     \"Positive sentiment score, ranging from 0.0 to 1.0.\"   ),   negative_score = type_number(     \"Negative sentiment score, ranging from 0.0 to 1.0.\"   ),   neutral_score = type_number(     \"Neutral sentiment score, ranging from 0.0 to 1.0.\"   ) )  chat <- chat_openai() #> Using model = \"gpt-4.1\". str(chat$chat_structured(text, type = type_sentiment)) #> List of 3 #>  $ positive_score: num 0.1 #>  $ negative_score: num 0.7 #>  $ neutral_score : num 0.2"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-4-text-classification","dir":"Articles","previous_headings":"Examples","what":"Example 4: Text classification","title":"Structured data","text":"","code":"text <- \"The new quantum computing breakthrough could revolutionize the tech industry.\"  type_score <- type_object(   name = type_enum(     c(       \"Politics\",       \"Sports\",       \"Technology\",       \"Entertainment\",       \"Business\",       \"Other\"     ),     \"The category name\",   ),   score = type_number(     \"The classification score for the category, ranging from 0.0 to 1.0.\"   ) ) type_classification <- type_array(   type_score,   description = \"Array of classification results. The scores should sum to 1.\" )  chat <- chat_openai() #> Using model = \"gpt-4.1\". data <- chat$chat_structured(text, type = type_classification) data #> # A tibble: 3 × 2 #>   name       score #>   <fct>      <dbl> #> 1 Technology  0.95 #> 2 Business    0.04 #> 3 Other       0.01"},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-5-working-with-unknown-keys","dir":"Articles","previous_headings":"Examples","what":"Example 5: Working with unknown keys","title":"Structured data","text":"example works Claude, GPT Gemini, Claude supports adding additional, arbitrary properties.","code":"type_characteristics <- type_object(   \"All characteristics\",   .additional_properties = TRUE )  text <- \"   The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket. \"  chat <- chat_anthropic(\"Extract all characteristics of supplied character\") #> Using model = \"claude-sonnet-4-5-20250929\". str(chat$chat_structured(text, type = type_characteristics)) #> List of 6 #>  $ gender              : chr \"male\" #>  $ height              : chr \"tall\" #>  $ facial_hair         : chr \"beard\" #>  $ distinguishing_marks: chr \"scar on left cheek\" #>  $ voice               : chr \"deep voice\" #>  $ clothing            : chr \"black leather jacket\""},{"path":"https://ellmer.tidyverse.org/dev/articles/structured-data.html","id":"example-6-extracting-data-from-an-image","dir":"Articles","previous_headings":"Examples","what":"Example 6: Extracting data from an image","title":"Structured data","text":"final example comes Dan Nguyen (can see interesting applications link). goal extract structured data screenshot: Even without descriptions, ChatGPT pretty well:","code":"type_asset <- type_object(   assert_name = type_string(),   owner = type_string(),   location = type_string(),   asset_value_low = type_integer(),   asset_value_high = type_integer(),   income_type = type_string(),   income_low = type_integer(),   income_high = type_integer(),   tx_gt_1000 = type_boolean() ) type_assets <- type_array(type_asset)  chat <- chat_openai() image <- content_image_file(\"congressional-assets.png\") data <- chat$chat_structured(image, type = type_assets) data"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Tool/function calling","text":"One interesting aspects modern chat models ability make use external tools defined caller. making chat request chat model, caller advertises one tools (defined function name, description, list expected arguments), chat model can choose respond one “tool calls”. tool calls requests chat model caller execute function given arguments; caller expected execute functions “return” results submitting another chat request conversation far, plus results. chat model can use results formulating response, , may decide make additional tool calls. Note chat model directly execute external tools! makes requests caller execute . ’s easy think tool calling might work like : fact works like : value chat model brings helping execution, knowing makes sense call tool, values pass arguments, use results formulating response.","code":"library(ellmer)"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"motivating-example","dir":"Articles","previous_headings":"Introduction","what":"Motivating example","title":"Tool/function calling","text":"Let’s take look example really need external tool. Chat models generally know current time, makes questions like impossible. Since model doesn’t know day , result incorrect.","code":"chat <- chat_openai(model = \"gpt-4o\") chat$chat(\"How long ago did Neil Armstrong touch down on the moon?\") #> Neil Armstrong landed on the moon on July 20, 1969. As of now in 2023, #> that was 54 years ago."},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"defining-a-tool-function","dir":"Articles","previous_headings":"Introduction","what":"Defining a tool function","title":"Tool/function calling","text":"first thing ’ll define R function returns current time. Note ’ve gone trouble creating roxygen2 comments. isn’t necessary, ’ll see shortly, can make bit easier generate tool defintion. turn function tool, provide additional metadata model use: fair amount code write, even simple function. Fortunately, don’t write hand! generated tool() call calling create_tool_def(get_current_time), uses LLM generate tool() call . create_tool_def() perfect, must review generated code using , big time-saver. Note tool just special type function can still call :","code":"#' Gets the current time in the given time zone. #' #' @param tz The time zone to get the current time in. #' @return The current time in the given time zone. get_current_time <- function(tz = \"UTC\") {   format(Sys.time(), tz = tz, usetz = TRUE) } get_current_time <- tool(   get_current_time,   name = \"get_current_time\",   description = \"Returns the current time.\",   arguments = list(     tz = type_string(       \"Time zone to display the current time in. Defaults to `\\\"UTC\\\"`.\",       required = FALSE     )   ) ) get_current_time() #> [1] \"2025-06-25 16:53:23 UTC\""},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"registering-and-using-tools","dir":"Articles","previous_headings":"Introduction","what":"Registering and using tools","title":"Tool/function calling","text":"Now need give chat object access tool. $register_tool(): ’s need ! Let’s retry query: ’s correct! Without guidance, chat model decided call tool function successfully used result formulating response. print chat can see model decided use tool: (Full disclosure: originally tried example default model gpt-4o-mini got tool calling right date math wrong, hence explicit model=\"gpt-4o\".) tool example extremely simple, can imagine much interesting things tool functions: calling APIs, reading writing database, kicking complex simulation, even calling complementary GenAI model (like image generator). using ellmer Shiny app, use tools set reactive values, setting chain reactive updates.","code":"chat$register_tool(get_current_time) chat$chat(\"How long ago did Neil Armstrong touch down on the moon?\") #> Neil Armstrong landed on the moon on July 20, 1969. As of June 25,  #> 2025, that was 55 years ago. chat #> <Chat OpenAI/gpt-4o turns=6 input=356 output=81 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> How long ago did Neil Armstrong touch down on the moon? #> ── assistant [input=19 output=31 cost=$0.00] ────────────────────────── #> Neil Armstrong landed on the moon on July 20, 1969. As of now in 2023, that was 54 years ago. #> ── user ─────────────────────────────────────────────────────────────── #> How long ago did Neil Armstrong touch down on the moon? #> ── assistant [input=151 output=16 cost=$0.00] ───────────────────────── #> [tool request (fc_07fd060fbaac49e001692dba44f2fc8197a1cfd2d677f98e67)]: get_current_time(tz = \"UTC\") #> ── user ─────────────────────────────────────────────────────────────── #> [tool result  (fc_07fd060fbaac49e001692dba44f2fc8197a1cfd2d677f98e67)]: 2025-06-25 16:53:23 UTC #> ── assistant [input=186 output=34 cost=$0.00] ───────────────────────── #> Neil Armstrong landed on the moon on July 20, 1969. As of June 25, 2025, that was 55 years ago."},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"tool-inputs-and-outputs","dir":"Articles","previous_headings":"Introduction","what":"Tool inputs and outputs","title":"Tool/function calling","text":"Remember tool arguments come LLM, tool results returned LLM. implies keep simple possible. Inputs tool call, must defined type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object(). recommend keeping simple possible, focusing basic scalar types much can. output tool call interpreted LLM, just typed information data. means ’ll generally want produce text atomic vectors. complex data, ellmer automatically serialize result JSON, LLMs generally seem good understanding. must direct control structure JSON ’s returned, can return JSON-serializable value wrapped (), ellmer leave alone entire request JSON-serialized. show ideas, ’s slightly complicated example simulating weather API returns data multiple cities . get_weather() function returns data frame ellmer automatically convert JSON row-major format, experiments suggest good LLMs. Now register use : can print chat confirm model performed single tool call:","code":"get_weather <- tool(   function(cities) {     raining <- c(London = \"heavy\", Houston = \"none\", Chicago = \"overcast\")     temperature <- c(London = \"cool\", Houston = \"hot\", Chicago = \"warm\")     wind <- c(London = \"strong\", Houston = \"weak\", Chicago = \"strong\")      data.frame(       city = cities,       raining = unname(raining[cities]),       temperature = unname(temperature[cities]),       wind = unname(wind[cities])     )   },   name = \"get_weather\",   description = \"     Report on weather conditions in multiple cities. For efficiency, request     all weather updates using a single tool call   \",   arguments = list(     cities = type_array(type_string(), \"City names\")   ) ) chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$register_tool(get_weather) chat$chat(\"Give me a weather update for London and Chicago\") #> Here’s a weather update: #>  #> - London: It’s heavily raining, cool, and windy with strong winds. #> - Chicago: The weather is overcast, warm, and also has strong winds. #>  #> Dress accordingly and stay safe! chat #> <Chat OpenAI/gpt-4.1 turns=4 input=195 output=65 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> Give me a weather update for London and Chicago #> ── assistant [input=67 output=18 cost=$0.00] ────────────────────────── #> [tool request (fc_0ab4b61a48aa23bb01692dba49b87c8190ae9678691024472c)]: get_weather(cities = c(\"London\", \"Chicago\")) #> ── user ─────────────────────────────────────────────────────────────── #> [tool result  (fc_0ab4b61a48aa23bb01692dba49b87c8190ae9678691024472c)]: [{\"city\":\"London\",\"raining\":\"heavy\",\"temperature\":\"cool\",\"wind\":\"strong\"},{\"city\":\"Chicago\",\"raining\":\"overcast\",\"temperature\":\"warm\",\"wind\":\"strong\"}] #> ── assistant [input=128 output=47 cost=$0.00] ───────────────────────── #> Here’s a weather update: #>  #> - London: It’s heavily raining, cool, and windy with strong winds. #> - Chicago: The weather is overcast, warm, and also has strong winds. #>  #> Dress accordingly and stay safe!"},{"path":"https://ellmer.tidyverse.org/dev/articles/tool-calling.html","id":"image-and-pdf-tool-output","dir":"Articles","previous_headings":"Introduction","what":"Image and PDF tool output","title":"Tool/function calling","text":"ellmer allow tools return image PDF content can returned tool result, LLM API supports vision capabilities. Simply return content_image_file(), content_pdf_file(), similar content type tool function. example, ’s simple tool screenshot website: use tool allow LLM “see” websites, like tidyverse website:","code":"screenshot_website <- tool(   function(url) {     tmpf <- withr::local_tempfile(fileext = \".png\")     webshot2::webshot(url, file = tmpf)     content_image_file(tmpf)   },   name = \"screenshot_website\",   description = \"Take a screenshot of a website.\",   arguments = list(     url = type_string(\"The URL of the website\")   ) ) chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$register_tool(screenshot_website) chat$chat(\"Describe the design aesthetic of https://tidyverse.org\") #> https://tidyverse.org screenshot completed #> The design aesthetic of the Tidyverse website (https://tidyverse.org) is #> clean, modern, and minimalistic, with several distinct features: #> #> - **Color Palette**: The overall site uses a lot of white space with navy #>   and dark backgrounds for some elements, accentuated by the colorful #>   hexagonal logos for various R packages. #> - **Typography**: Simple, sans-serif fonts contribute to readability and #>   a contemporary look. #> - **Hexagonal Icons**: Prominent display of tidyverse package logos in #>   hexagonal shapes, emphasizing the modular, package-oriented #>   nature of the Tidyverse. #> - **Layout**: A balanced, spacious two-column layout. The left side #>   features graphic elements; the right side provides concise, text-based #>   information. #> #> Overall, the design communicates clarity, ease of use, and a focus on #> modern data science tools."},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hadley Wickham. Author, maintainer. Joe Cheng. Author. Aaron Jacobs. Author. Garrick Aden-Buie. Author. Barret Schloerke. Author. . Copyright holder, funder.","code":""},{"path":"https://ellmer.tidyverse.org/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wickham H, Cheng J, Jacobs , Aden-Buie G, Schloerke B (2026). ellmer: Chat Large Language Models. R package version 0.4.0.9000, https://ellmer.tidyverse.org.","code":"@Manual{,   title = {ellmer: Chat with Large Language Models},   author = {Hadley Wickham and Joe Cheng and Aaron Jacobs and Garrick Aden-Buie and Barret Schloerke},   year = {2026},   note = {R package version 0.4.0.9000},   url = {https://ellmer.tidyverse.org}, }"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"ellmer-","dir":"","previous_headings":"","what":"Chat with Large Language Models","title":"Chat with Large Language Models","text":"ellmer makes easy use large language models (LLM) R. supports wide variety LLM providers implements rich set features including streaming outputs, tool/function calling, structured data extraction, . ellmer one number LLM-related packages created Posit: Looking something similar python? Check chatlas! Want evaluate LLMs? Try vitals. Need RAG? Take look ragnar. Want make beautiful LLM powered chatbot? Consider shinychat. Working MCP? Check mcptools.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Chat with Large Language Models","text":"can install ellmer CRAN :","code":"install.packages(\"ellmer\")"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providers","dir":"","previous_headings":"","what":"Providers","title":"Chat with Large Language Models","text":"ellmer supports wide variety model providers: Anthropic’s Claude: chat_anthropic(). AWS Bedrock: chat_aws_bedrock(). Azure OpenAI: chat_azure_openai(). Cloudflare: chat_cloudflare(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini/Vertex AI: chat_google_gemini(), chat_google_vertex(). Groq: chat_groq(). Hugging Face: chat_huggingface(). Mistral: chat_mistral(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() chat_cortex_analyst(). VLLM: chat_vllm().","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"providermodel-choice","dir":"","previous_headings":"Providers","what":"Provider/model choice","title":"Chat with Large Language Models","text":"’re using ellmer inside organisation, may internal policies limit models big cloud providers, e.g. chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake(). ’re using ellmer exploration, ’ll lot freedom, recommendations help get started: chat_openai() chat_anthropic() good places start. chat_openai() defaults GPT-4.1, can use model = \"gpt-4-1-nano\" cheaper, faster model, model = \"o3\" complex reasoning. chat_anthropic() also good; defaults Claude 4.0 Sonnet, found particularly good writing R code. chat_google_gemini() strong model generous free tier (downside data used improve model), making great place start don’t want spend money. chat_ollama(), uses Ollama, allows run models computer. biggest models can run locally aren’t good state art hosted models, don’t share data effectively free.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"authentication","dir":"","previous_headings":"Providers","what":"Authentication","title":"Chat with Large Language Models","text":"Authentication works little differently depending provider. popular ones (including OpenAI Anthropic) require obtain API key. recommend save environment variable rather using directly code, deploy app report uses ellmer another system, ’ll need ensure environment variable available , . ellmer also automatically detects many OAuth IAM-based credentials used big cloud providers (currently chat_azure_openai(), chat_aws_bedrock(), chat_databricks(), chat_snowflake()). includes credentials platforms managed Posit Workbench Posit Connect. find cases ellmer detect credentials one cloud providers, feel free open issue; ’re happy add auth mechanisms needed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"using-ellmer","dir":"","previous_headings":"","what":"Using ellmer","title":"Chat with Large Language Models","text":"can work ellmer several different ways, depending whether working interactively programmatically. start creating new chat object: Chat objects stateful R6 objects: retain context conversation, new query builds previous ones. call methods $.","code":"library(ellmer)  chat <- chat_openai(\"Be terse\", model = \"gpt-4o-mini\")"},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-chat-console","dir":"","previous_headings":"Using ellmer","what":"Interactive chat console","title":"Chat with Large Language Models","text":"interactive least programmatic way using ellmer chat directly R console browser live_console(chat) live_browser(): Keep mind chat object retains state, enter chat console, previous interactions chat object still part conversation, interactions chat console persist exit back R prompt. true regardless chat function use.","code":"live_console(chat) #> ╔════════════════════════════════════════════════════════╗ #> ║  Entering chat console. Use \"\"\" for multi-line input.  ║ #> ║  Press Ctrl+C to quit.                                 ║ #> ╚════════════════════════════════════════════════════════╝ #> >>> Who were the original creators of R? #> R was originally created by Ross Ihaka and Robert Gentleman at the University of #> Auckland, New Zealand. #> #> >>> When was that? #> R was initially released in 1995. Development began a few years prior to that, #> in the early 1990s."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"interactive-method-call","dir":"","previous_headings":"Using ellmer","what":"Interactive method call","title":"Chat with Large Language Models","text":"second interactive way chat call chat() method: initialize chat object global environment, chat method stream response console. entire response received, ’s also (invisibly) returned character vector. useful want see response arrives, don’t want enter chat console. want ask question image, can pass one additional input arguments using content_image_file() /content_image_url():","code":"chat$chat(\"What preceding languages most influenced R?\") #> R was primarily influenced by S, a language developed at Bell Laboratories.  #> Other notable influences include: #>  #> 1. **Scheme** - For functional programming concepts. #> 2. **LISP** - For its powerful data manipulation features. #> 3. **C** - For performance and system-level access. #> 4. **Fortran** - For numerical and statistical computations. #>  #> These languages contributed to R's syntax, data structures, and functional  #> programming capabilities. chat$chat(   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   \"Can you explain this logo?\" ) #> The logo consists of a stylized letter \"R\" in blue, surrounded by a gray oval  #> shape. The design reflects the programming language R, which is widely used for #> statistical computing and graphics. The color choice often symbolizes clarity  #> and professionalism, aligning with R's use in data analysis and research. The  #> logo encapsulates the language's focus on data visualization and statistical  #> methods."},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"streaming-vs-capturing","dir":"","previous_headings":"Using ellmer","what":"Streaming vs capturing","title":"Chat with Large Language Models","text":"circumstances, ellmer stream output console. can take control setting echo argument either creating chat object calling $chat(). Set echo = \"none\" return string instead: needed, can manually control behaviour echo argument. useful programming ellmer result either intended human consumption want process response displaying .","code":"my_function <- function() {   chat <- chat_openai(\"Be terse\", model = \"gpt-4o-mini\", echo = \"none\")   chat$chat(\"What is 6 times 7?\") } str(my_function()) #>  'ellmer_output' chr \"6 times 7 is 42.\""},{"path":"https://ellmer.tidyverse.org/dev/index.html","id":"learning-more","dir":"","previous_headings":"","what":"Learning more","title":"Chat with Large Language Models","text":"ellmer comes bunch vignettes help learn : Learn key vocabulary see example use cases vignette(\"ellmer\"). Learn design prompt vignette(\"prompt-design\"). Learn tool/function calling vignette(\"tool-calling\"). Learn extract structured data vignette(\"structured-data\"). Learn streaming async APIs vignette(\"streaming-async\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"The Chat object — Chat","title":"The Chat object — Chat","text":"Chat sequence user assistant Turns sent specific Provider. Chat mutable R6 object takes care managing state associated chat; .e. records messages send server, messages receive back. register tool (.e. R function assistant can call behalf), also takes care tool loop. generally create object , instead call chat_openai() friends instead.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Chat object — Chat","text":"Chat object","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"The Chat object — Chat","text":"Chat$new() Chat$get_turns() Chat$set_turns() Chat$add_turn() Chat$get_system_prompt() Chat$get_model() Chat$set_system_prompt() Chat$get_tokens() Chat$get_cost() Chat$last_turn() Chat$chat() Chat$chat_structured() Chat$chat_structured_async() Chat$chat_async() Chat$stream() Chat$stream_async() Chat$register_tool() Chat$register_tools() Chat$get_provider() Chat$get_tools() Chat$set_tools() Chat$on_tool_request() Chat$on_tool_result() Chat$clone()","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$new(provider, system_prompt = NULL, echo = \"none\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"provider provider object. system_prompt System prompt start conversation . echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. can override default setting ellmer_echo option.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-turns-","dir":"Reference","previous_headings":"","what":"Method get_turns()","title":"The Chat object — Chat","text":"Retrieve turns sent received far (optionally starting system prompt, ).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_turns(include_system_prompt = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include_system_prompt Whether include system prompt turns (exists).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-turns-","dir":"Reference","previous_headings":"","what":"Method set_turns()","title":"The Chat object — Chat","text":"Replace existing turns new list.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_turns(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"value list Turns.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-add-turn-","dir":"Reference","previous_headings":"","what":"Method add_turn()","title":"The Chat object — Chat","text":"Add pair turns chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$add_turn(user, assistant, log_tokens = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"user user Turn. assistant system Turn. log_tokens tokens used turn logged session counter?","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-system-prompt-","dir":"Reference","previous_headings":"","what":"Method get_system_prompt()","title":"The Chat object — Chat","text":"set, system prompt, , NULL.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_system_prompt()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-model-","dir":"Reference","previous_headings":"","what":"Method get_model()","title":"The Chat object — Chat","text":"Retrieve model name","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_model()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-system-prompt-","dir":"Reference","previous_headings":"","what":"Method set_system_prompt()","title":"The Chat object — Chat","text":"Update system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_system_prompt(value)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"value character vector giving new system prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tokens-","dir":"Reference","previous_headings":"","what":"Method get_tokens()","title":"The Chat object — Chat","text":"data frame token usage cost data. four columns: input, output, cached_input, cost. one row assistant turn, token counts costs available API returns assistant's response.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_tokens(include_system_prompt = deprecated())"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include_system_prompt","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-cost-","dir":"Reference","previous_headings":"","what":"Method get_cost()","title":"The Chat object — Chat","text":"cost chat","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_cost(include = c(\"all\", \"last\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"include default, \"\", gives total cumulative cost chat. Alternatively, use \"last\" get cost just recent turn.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-last-turn-","dir":"Reference","previous_headings":"","what":"Method last_turn()","title":"The Chat object — Chat","text":"last turn returned assistant.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$last_turn(role = c(\"assistant\", \"user\", \"system\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"role Optionally, specify role find last turn role.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"Either Turn NULL, turns specified role occurred.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"The Chat object — Chat","text":"Submit input chatbot, return response simple string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat(..., echo = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images (see content_image_file() content_image_url(). echo Whether emit response stdout received. NULL, value echo set chat object created used.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-","dir":"Reference","previous_headings":"","what":"Method chat_structured()","title":"The Chat object — Chat","text":"Extract structured data","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_structured(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. typically text want extract data , can omitted data obvious existing conversation. type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-structured-async-","dir":"Reference","previous_headings":"","what":"Method chat_structured_async()","title":"The Chat object — Chat","text":"Extract structured data, asynchronously. Returns promise resolves object matching type specification.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_structured_async(..., type, echo = \"none\", convert = TRUE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. typically include phrase \"extract structured data\". type type specification extracted data. created type_() function. echo Whether emit response stdout received. Set \"text\" stream JSON data generated (supported providers). convert Automatically convert JSON lists R data types using schema. example, turn arrays objects data frames arrays strings character vector.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-chat-async-","dir":"Reference","previous_headings":"","what":"Method chat_async()","title":"The Chat object — Chat","text":"Submit input chatbot, receive promise resolves response . Returns promise resolves string (probably Markdown).","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$chat_async(..., tool_mode = c(\"concurrent\", \"sequential\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-","dir":"Reference","previous_headings":"","what":"Method stream()","title":"The Chat object — Chat","text":"Submit input chatbot, returning streaming results. Returns coro generator yields strings. iterating, generator block waiting content chatbot.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$stream(..., stream = c(\"text\", \"content\"))"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-stream-async-","dir":"Reference","previous_headings":"","what":"Method stream_async()","title":"The Chat object — Chat","text":"Submit input chatbot, returning asynchronously streaming results. Returns coro async generator yields string promises.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$stream_async(   ...,   tool_mode = c(\"concurrent\", \"sequential\"),   stream = c(\"text\", \"content\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"... input send chatbot. Can strings images. tool_mode Whether tools invoked one---time (\"sequential\") concurrently (\"concurrent\"). Sequential mode best interactive applications, especially tool may involve interactive user interface. Concurrent mode default best suited automated scripts non-interactive applications. stream Whether stream yield \"text\" ellmer's rich content types. stream = \"content\", stream() yields Content objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-register-tool-","dir":"Reference","previous_headings":"","what":"Method register_tool()","title":"The Chat object — Chat","text":"Register tool (R function) chatbot can use. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$register_tool(tool)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tool tool definition created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-register-tools-","dir":"Reference","previous_headings":"","what":"Method register_tools()","title":"The Chat object — Chat","text":"Register list tools. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$register_tools(tools)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-provider-","dir":"Reference","previous_headings":"","what":"Method get_provider()","title":"The Chat object — Chat","text":"Get underlying provider object. expert use .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_provider()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-get-tools-","dir":"Reference","previous_headings":"","what":"Method get_tools()","title":"The Chat object — Chat","text":"Retrieve list registered tools.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$get_tools()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-set-tools-","dir":"Reference","previous_headings":"","what":"Method set_tools()","title":"The Chat object — Chat","text":"Sets available tools. expert use ; users use register_tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$set_tools(tools)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"tools list tool definitions created tool().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-request-","dir":"Reference","previous_headings":"","what":"Method on_tool_request()","title":"The Chat object — Chat","text":"Register callback tool request event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$on_tool_request(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"callback function called tool request event occurs, must request argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-on-tool-result-","dir":"Reference","previous_headings":"","what":"Method on_tool_result()","title":"The Chat object — Chat","text":"Register callback tool result event.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$on_tool_result(callback)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"callback function called tool result event occurs, must result argument.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"The Chat object — Chat","text":"function can called remove callback.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"The Chat object — Chat","text":"objects class cloneable method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"The Chat object — Chat","text":"","code":"Chat$clone(deep = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Chat object — Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The Chat object — Chat","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat(\"Tell me a funny joke\") #> Sure! Here you go: #>  #> Why did the scarecrow win an award? #>  #> Because he was outstanding in his field! 🌾😄"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":null,"dir":"Reference","previous_headings":"","what":"Content types received from and sent to a chatbot — Content","title":"Content types received from and sent to a chatbot — Content","text":"Use functions writing package extends ellmer need customise methods various types content. normal use, see content_image_url() friends. ellmer abstracts away differences way different Providers represent various types content, allowing easily write code works chatbot. set classes represents types content can either sent received provider: ContentText: simple text (often markdown format). type content can streamed live received. ContentImageRemote ContentImageInline: images, either pointer remote URL included inline object. See content_image_file() friends convenient ways construct objects. ContentToolRequest: request perform tool call (sent assistant). ContentToolResult: result calling tool (sent user). object automatically created value returned calling tool() function. Alternatively, expert users can return ContentToolResult tool() function include additional data customize display result.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content()  ContentText(text = stop(\"Required\"))  ContentImage()  ContentImageRemote(url = stop(\"Required\"), detail = \"\")  ContentImageInline(type = stop(\"Required\"), data = NULL)  ContentToolRequest(   id = stop(\"Required\"),   name = stop(\"Required\"),   arguments = list(),   tool = NULL,   extra = list() )  ContentToolResult(value = NULL, error = NULL, extra = list(), request = NULL)  ContentThinking(thinking = stop(\"Required\"), extra = list())  ContentPDF(   type = stop(\"Required\"),   data = stop(\"Required\"),   filename = stop(\"Required\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Content types received from and sent to a chatbot — Content","text":"text single string. url URL remote image. detail currently used. type MIME type image. data Base64 encoded image data. id Tool call id (used associate request result). Automatically managed ellmer. name Function name arguments Named list arguments call function . tool ellmer automatically matches tool request tools defined chatbot. NULL, request match defined tool. extra Additional data. value results calling tool function, succeeded. error error message, string, error condition thrown result failure calling tool function. Must NULL tool call successful. request ContentToolRequest associated tool result, automatically added ellmer evaluating tool call. thinking text thinking output. filename File name, used identify PDF.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Content types received from and sent to a chatbot — Content","text":"S7 objects inherit Content","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Content.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Content types received from and sent to a chatbot — Content","text":"","code":"Content() #> <ellmer::Content> ContentText(\"Tell me a joke\") #> <ellmer::ContentText> #>  @ text: chr \"Tell me a joke\" ContentImageRemote(\"https://www.r-project.org/Rlogo.png\") #> <ellmer::ContentImageRemote> #>  @ url   : chr \"https://www.r-project.org/Rlogo.png\" #>  @ detail: chr \"\" ContentToolRequest(id = \"abc\", name = \"mean\", arguments = list(x = 1:5)) #> <ellmer::ContentToolRequest> #>  @ id       : chr \"abc\" #>  @ name     : chr \"mean\" #>  @ arguments:List of 1 #>  .. $ x: int [1:5] 1 2 3 4 5 #>  @ tool     : NULL #>  @ extra    : list()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"A chatbot provider — Provider","title":"A chatbot provider — Provider","text":"Provider captures details one chatbot service/API. captures API works, details underlying large language model. Different providers might offer (open source) model behind different API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = stop(\"Required\"),   model = stop(\"Required\"),   base_url = stop(\"Required\"),   params = list(),   extra_args = list(),   extra_headers = character(0),   credentials = function() NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A chatbot provider — Provider","text":"name Name provider. model Name model. base_url base URL API. params list standard parameters created params(). extra_args Arbitrary extra arguments included request body. extra_headers Arbitrary extra headers added request. credentials zero-argument function returns credentials use authentication. Can either return string, representing API key, named list headers.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A chatbot provider — Provider","text":"S7 Provider object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A chatbot provider — Provider","text":"add support new backend, need subclass Provider (adding additional fields provider needs) implement various generics control behavior provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Provider.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A chatbot provider — Provider","text":"","code":"Provider(   name = \"CoolModels\",   model = \"my_model\",   base_url = \"https://cool-models.com\" ) #> <ellmer::Provider> #>  @ name         : chr \"CoolModels\" #>  @ model        : chr \"my_model\" #>  @ base_url     : chr \"https://cool-models.com\" #>  @ params       : list() #>  @ extra_args   : list() #>  @ extra_headers: chr(0)  #>  @ credentials  : function ()"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":null,"dir":"Reference","previous_headings":"","what":"A user, assistant, or system turn — Turn","title":"A user, assistant, or system turn — Turn","text":"Every conversation chatbot consists pairs user assistant turns, corresponding HTTP request response. turns represented Turn object, contains list Contents representing individual messages within turn. might text, images, tool requests (assistant ), tool responses (user ). UserTurn, AssistantTurn, SystemTurn specialized subclasses Turn different types conversation turns. AssistantTurn includes additional metadata API response. Note call $chat() related functions may result multiple user-assistant turn cycles. example, registered tools, ellmer automatically handle tool calling loop, may result number additional cycles. Learn tool calling vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A user, assistant, or system turn — Turn","text":"","code":"Turn(role = NULL, contents = list(), tokens = NULL)  UserTurn(contents = list())  SystemTurn(contents = list())  AssistantTurn(   contents = list(),   json = list(),   tokens = c(NA_real_, NA_real_, NA_real_),   cost = NA_real_,   duration = NA_real_ )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A user, assistant, or system turn — Turn","text":"role system, user assistant turns, use SystemTurn(), UserTurn(), AssistantTurn(), respectively. contents list Content objects. tokens numeric vector length 3 representing number input tokens (uncached), output tokens, input tokens (cached) used turn. json serialized JSON corresponding underlying data turns. useful information returned provider ellmer otherwise expose. cost cost turn dollars. duration duration request seconds.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A user, assistant, or system turn — Turn","text":"S7 Turn object S7 AssistantTurn object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Turn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A user, assistant, or system turn — Turn","text":"","code":"UserTurn(list(ContentText(\"Hello, world!\"))) #> <Turn: user> #> Hello, world!"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":null,"dir":"Reference","previous_headings":"","what":"Type definitions for function calling and structured data extraction. — Type","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 classes provided use package devlopers extending ellmer. every day use, use type_boolean() friends.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(description = NULL, required = TRUE, type = stop(\"Required\"))  TypeEnum(description = NULL, required = TRUE, values = character(0))  TypeArray(description = NULL, required = TRUE, items = Type())  TypeJsonSchema(description = NULL, required = TRUE, json = list())  TypeIgnore(description = NULL, required = TRUE)  TypeObject(   description = NULL,   required = TRUE,   properties = list(),   additional_properties = FALSE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type definitions for function calling and structured data extraction. — Type","text":"description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. type Basic type name. Must one boolean, integer, number, string. values Character vector permitted values. items type array items. Can created type_ function. json JSON schema object list. properties Named list properties stored inside object. element S7 Type object.` additional_properties Can object arbitrary additional properties explicitly listed? supported Claude.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Type definitions for function calling and structured data extraction. — Type","text":"S7 objects inheriting Type","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/Type.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type definitions for function calling and structured data extraction. — Type","text":"","code":"TypeBasic(type = \"boolean\") #> <ellmer::TypeBasic> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ type       : chr \"boolean\" TypeArray(items = TypeBasic(type = \"boolean\")) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"boolean\""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in one batch — batch_chat","title":"Submit multiple chats in one batch — batch_chat","text":"batch_chat() batch_chat_structured() currently work chat_openai() chat_anthropic(). use OpenAI Anthropic batch APIs allow submit multiple requests simultaneously. results can take 24 hours complete, return pay 50% less usual (note ellmer include discount pricing metadata). want get results back quickly, working different provider, may want use parallel_chat() instead. Since batched requests can take long time complete, batch_chat() requires file path used store information batch never lose work. can either set wait = FALSE simply interrupt waiting process, later, either call batch_chat() resume left call batch_chat_completed() see results ready retrieve. batch_chat() store chat responses file, can either keep around cache results, delete free disk space. API marked experimental since yet know handle errors helpful way. Fortunately seem common, ideas, please let know!","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"batch_chat(chat, prompts, path, wait = TRUE, ignore_hash = FALSE)  batch_chat_text(chat, prompts, path, wait = TRUE, ignore_hash = FALSE)  batch_chat_structured(   chat,   prompts,   path,   type,   wait = TRUE,   ignore_hash = FALSE,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE )  batch_chat_completed(chat, prompts, path)"},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in one batch — batch_chat","text":"chat chat object created chat_ function, string passed chat(). prompts vector created interpolate() list character vectors. path Path file (.json extension) store state. file records hash provider, prompts, existing chat turns. attempt reuse file different, get error. wait TRUE, wait batch complete. FALSE, return NULL batch complete, can retrieve results later re-running batch_chat() batch_chat_completed() TRUE. ignore_hash TRUE, warn rather error hash match. can use ellmer changed hash structure confident reusing inputs. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit multiple chats in one batch — batch_chat","text":"batch_chat(), list Chat objects, one prompt. batch_chat_test(), character vector text responses. batch_chat_structured(), single structured data object one element prompt. Typically, type object, data frame one row prompt, one column property. aboves, return NULL wait = FALSE job complete.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/batch_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in one batch — batch_chat","text":"","code":"if (FALSE) { # has_credentials(\"openai\") chat <- chat_openai(model = \"gpt-4.1-nano\")  # Chat ----------------------------------------------------------------------  prompts <- interpolate(\"What do people from {{state.name}} bring to a potluck dinner?\") if (FALSE) { # \\dontrun{ chats <- batch_chat(chat, prompts, path = \"potluck.json\") chats } # }  # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) if (FALSE) { # \\dontrun{ data <- batch_chat_structured(   chat = chat,   prompts = prompts,   path = \"people-data.json\",   type = type_person ) data } # } }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with any provider — chat","title":"Chat with any provider — chat","text":"generic interface chat_ functions allow pick provider model simple string.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with any provider — chat","text":"","code":"chat(   name,   ...,   system_prompt = NULL,   params = NULL,   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat-any.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with any provider — chat","text":"name Provider (optionally model) name form \"provider/model\" \"provider\" (use default model provider). ... Arguments passed provider function. system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an Anthropic Claude model — chat_anthropic","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Anthropic provides number chat based models Claude moniker. Note Claude Pro membership give ability call models via API; instead, need sign (pay ) developer account.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat_anthropic(   system_prompt = NULL,   params = NULL,   model = NULL,   cache = c(\"5m\", \"1h\", \"none\"),   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   beta_headers = character(),   api_key = NULL,   credentials = NULL,   api_headers = character(),   echo = NULL )  chat_claude(   system_prompt = NULL,   params = NULL,   model = NULL,   cache = c(\"5m\", \"1h\", \"none\"),   api_args = list(),   base_url = \"https://api.anthropic.com/v1\",   beta_headers = character(),   api_key = NULL,   credentials = NULL,   api_headers = character(),   echo = NULL )  models_claude(   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key() )  models_anthropic(   base_url = \"https://api.anthropic.com/v1\",   api_key = anthropic_key() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). model model use chat (defaults \"claude-sonnet-4-5-20250929\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_anthropic() see options. cache long cache inputs? Defaults \"5m\" (five minutes). Set \"none\" disable caching \"1h\" cache one hour. See details . api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). base_url base URL endpoint; default Claude's public API. beta_headers Optionally, character vector beta headers opt-claude features still beta. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set ANTHROPIC_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"caching","dir":"Reference","previous_headings":"","what":"Caching","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"Caching Claude bit complicated providers believe average save money time, enabled default. providers, like OpenAI Google, pay cache reads, cost 10% normal price. Claude, also pay cache writes, cost 125% normal price 5 minute caching 200% normal price 1 hour caching. affect total cost conversation? Imagine first turn sends 1000 input tokens receives 200 output tokens. second turn must first send input output previous turn (1200 tokens). sends 1000 tokens receives 200 tokens back. compare prices two approaches can ignore cost output tokens, . much input tokens cost? use caching, send 1000 tokens first turn 2200 (1000 + 200 + 1000) tokens second turn total 3200 tokens. use caching, send (equivalent ) 1000 * 1.25 = 1250 tokens first turn. second turn, 1000 input tokens cached total cost 1000 * 0.1 + (200 + 1000) * 1.25 = 1600 tokens. makes total 2850 tokens, .e. 11% fewer tokens, decreasing overall cost. Obviously, details vary conversation conversation, large system prompt re-use many times expect see larger savings. can see exactly many input cache input tokens turn uses, along total cost, chat$get_tokens(). see savings use case, can suppress caching cache = \"none\". know already quite complicated, one final wrinkle: Claude cache longer prompts, caching requiring least 1024-4096 tokens, depending model. surprised see differences caching short prompt. See details https://docs.claude.com/en/docs/build--claude/prompt-caching.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_anthropic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an Anthropic Claude model — chat_anthropic","text":"","code":"chat <- chat_anthropic() #> Using model = \"claude-sonnet-4-5-20250929\". chat$chat(\"Tell me three jokes about statisticians\") #> # Three Jokes About Statisticians #>  #> **1. The Drowning Statistician** #> A statistician is someone who could drown crossing a river that's an  #> average of three feet deep. #>  #> **2. The Uncertain Response** #> Three statisticians go hunting. They spot a deer. The first  #> statistician shoots and misses—two feet to the left. The second shoots #> and misses—two feet to the right. The third statistician jumps up and  #> down shouting, \"We got it! We got it!\" #>  #> **3. The Kidnapped Statistician** #> A kidnapper grabs a statistician and threatens, \"Give me all your  #> money or you're average!\" The statistician replies, \"I think you mean  #> 'or you're history.'\" The kidnapper responds, \"Don't tell me my  #> job—you tell me the mean!\" #>  #> --- #>  #> *These jokes play on statistical concepts like averages, means, and  #> the classic distinction between theoretical data and practical  #> reality!*"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an AWS bedrock model — chat_aws_bedrock","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"AWS Bedrock provides number language models, including Anthropic's Claude, using Bedrock Converse API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Authentication handled {paws.common}, authentication work automatically, need follow advice https://www.paws-r-sdk.com/#credentials. particular, org uses AWS SSO, need run aws sso login terminal.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"chat_aws_bedrock(   system_prompt = NULL,   base_url = NULL,   model = NULL,   profile = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  models_aws_bedrock(profile = NULL, base_url = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. model model use chat (defaults \"anthropic.claude-sonnet-4-5-20250929-v1:0\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_models_aws_bedrock() see options. . ellmer provides default model, guarantee access , need specify model can. using cross-region inference, need use inference profile ID, e.g. model=\"us.anthropic.claude-sonnet-4-5-20250929-v1:0\". profile AWS profile use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. useful arguments include:   api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":"api_args = list(   inferenceConfig = list(     maxTokens = 100,     temperature = 0.7,     topP = 0.9,     topK = 20   ) )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_aws_bedrock.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an AWS bedrock model — chat_aws_bedrock","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage chat <- chat_aws_bedrock() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Azure OpenAI server hosts number open source models well proprietary models OpenAI. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"chat_azure_openai() supports API keys credentials parameter, also makes use : Azure service principals (AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET environment variables set). Interactive Entra ID authentication, like Azure CLI. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"chat_azure_openai(   endpoint = azure_endpoint(),   model,   params = NULL,   api_version = NULL,   system_prompt = NULL,   api_key = NULL,   credentials = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character(),   deployment_id = deprecated() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"endpoint Azure OpenAI endpoint url protocol hostname, .e. https://{-resource-name}.openai.azure.com. Defaults using value AZURE_OPENAI_ENDPOINT environment variable. model deployment id model want use. params Common model parameters, usually created params(). api_version API version use. system_prompt system prompt set behavior assistant. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set AZURE_OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call. deployment_id Use model instead.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_azure_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Azure OpenAI — chat_azure_openai","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_azure_openai(model = \"gpt-4o-mini\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on CloudFlare — chat_cloudflare","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Cloudflare Workers AI hosts variety open-source AI models. use Cloudflare API, must Account ID Access Token, can obtain following instructions. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Tool calling appear work. Images appear work.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"chat_cloudflare(   account = cloudflare_account(),   system_prompt = NULL,   params = NULL,   api_key = NULL,   credentials = NULL,   model = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"account Cloudflare account ID. Taken CLOUDFLARE_ACCOUNT_ID env var, defined. system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set CLOUDFLARE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"meta-llama/Llama-3.3-70b-instruct-fp8-fast\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_cloudflare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on CloudFlare — chat_cloudflare","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_cloudflare() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Databricks — chat_databricks","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Databricks provides ---box access number foundation models can also serve gateway external models hosted third party. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Databricks — chat_databricks","text":"chat_databricks() picks ambient Databricks credentials subset Databricks client unified authentication model. Specifically, supports: Personal access tokens Service principals via OAuth (OAuth M2M) User account via OAuth (OAuth U2M) Authentication via Databricks CLI Posit Workbench-managed credentials Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"chat_databricks(   workspace = databricks_workspace(),   system_prompt = NULL,   model = NULL,   token = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Databricks — chat_databricks","text":"workspace URL Databricks workspace, e.g. \"https://example.cloud.databricks.com\". use value environment variable DATABRICKS_HOST, set. system_prompt system prompt set behavior assistant. model model use chat (defaults \"databricks-claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Available foundational models include: databricks-claude-3-7-sonnet (default) databricks-mixtral-8x7b-instruct databricks-meta-llama-3-1-70b-instruct databricks-meta-llama-3-1-405b-instruct token authentication token Databricks workspace, NULL use ambient credentials. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Databricks — chat_databricks","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_databricks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Databricks — chat_databricks","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_databricks() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on DeepSeek — chat_deepseek","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Sign https://platform.deepseek.com. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Structured data extraction supported. Images supported.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"chat_deepseek(   system_prompt = NULL,   base_url = \"https://api.deepseek.com\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default uses DeepSeek. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set DEEPSEEK_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"deepseek-chat\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_deepseek.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on DeepSeek — chat_deepseek","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_deepseek() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on the GitHub model marketplace — chat_github","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"GitHub Models hosts number open source OpenAI models. access GitHub model marketplace, need apply accepted beta access program. See https://github.com/marketplace/models details. function lightweight wrapper around chat_openai() defaults tweaked GitHub Models marketplace. GitHub also suports Azure AI Inference SDK, can use setting base_url \"https://models.inference.ai.azure.com/\". endpoint used ellmer v0.3.0 earlier.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"chat_github(   system_prompt = NULL,   base_url = \"https://models.github.ai/inference/\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )  models_github(   base_url = \"https://models.github.ai/\",   api_key = NULL,   credentials = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set GITHUB_PAT environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_github.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on the GitHub model marketplace — chat_github","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_github() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Google's AI offering broken two parts: Gemini Vertex AI. enterprises likely use Vertex AI, individuals likely use Gemini. Use google_upload() upload files (PDFs, images, video, audio, etc.)","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"functions try number authentication strategies, order: API key set GOOGLE_API_KEY env var, , chat_google_gemini() , GEMINI_API_KEY. Google's default application credentials, gargle package installed. Viewer-based credentials Posit Connect, connectcreds package. . browser-based OAuth flow, interactive session. currently uses unverified OAuth app (get scary warning); plan verify near future.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"chat_google_gemini(   system_prompt = NULL,   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  chat_google_vertex(   location,   project_id,   system_prompt = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = NULL )  models_google_gemini(   base_url = \"https://generativelanguage.googleapis.com/v1beta/\",   api_key = NULL,   credentials = NULL )  models_google_vertex(location, project_id, credentials = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials function returns list authentication headers NULL, default, use ambient credentials. See details. model model use chat (defaults \"gemini-2.5-flash\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_google_gemini() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. location Location, e.g. us-east1, -central1, africa-south1 global. project_id Project ID.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_google_gemini.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a Google Gemini or Vertex AI model — chat_google_gemini","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Groq — chat_groq","title":"Chat with a model hosted on Groq — chat_groq","text":"Sign https://groq.com. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Groq — chat_groq","text":"groq currently support structured data extraction.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"chat_groq(   system_prompt = NULL,   base_url = \"https://api.groq.com/openai/v1\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Groq — chat_groq","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set GROQ_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"llama-3.1-8b-instant\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Groq — chat_groq","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Groq — chat_groq","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_groq() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Hugging Face hosts variety open-source proprietary AI models available via Inference API. use Hugging Face API, must Access Token, can obtain Hugging Face account (ensure least \"Make calls Inference Providers\" \"Make calls Inference Endpoints\" checked). Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"models support chat interface parts , example google/gemma-2-2b-support system prompt. need carefully choose model.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"chat_huggingface(   system_prompt = NULL,   params = NULL,   api_key = NULL,   credentials = NULL,   model = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set HUGGINGFACE_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"meta-llama/Llama-3.1-8B-Instruct\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_huggingface.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Hugging Face Serverless Inference API — chat_huggingface","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_huggingface() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Get API key https://console.mistral.ai/api-keys. Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Tool calling unstable. Images require model supports images.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"chat_mistral(   system_prompt = NULL,   params = NULL,   api_key = NULL,   credentials = NULL,   model = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )  models_mistral(api_key = mistral_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"system_prompt system prompt set behavior assistant. params Common model parameters, usually created params(). api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set MISTRAL_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"mistral-large-latest\"). regularly update default, strongly recommend explicitly specifying model anything casual use. api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_mistral.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Mistral's La Platforme — chat_mistral","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_mistral() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a local Ollama model — chat_ollama","title":"Chat with a local Ollama model — chat_ollama","text":"use chat_ollama() first download install Ollama. install models either command line (e.g. ollama pull llama3.1) within R using ollamar (e.g. ollamar::pull(\"llama3.1\")). Built top chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a local Ollama model — chat_ollama","text":"Tool calling supported streaming (.e. echo \"text\" \"\") Models can use 2048 input tokens, way get use , except creating custom model different default. Tool calling generally seems quite weak, least models tried .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"chat_ollama(   system_prompt = NULL,   base_url = Sys.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\"),   model,   params = NULL,   api_args = list(),   echo = NULL,   api_key = NULL,   credentials = NULL,   api_headers = character() )  models_ollama(base_url = \"http://localhost:11434\", credentials = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a local Ollama model — chat_ollama","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. model model use chat. Use models_ollama() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_key Use credentials instead. credentials Ollama require credentials local usage cases need provide credentials. However, accessing Ollama instance hosted behind reverse proxy secured endpoint enforces bearer‐token authentication, can set OLLAMA_API_KEY environment variable provide callback function credentials. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a local Ollama model — chat_ollama","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a local Ollama model — chat_ollama","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_ollama(model = \"llama3.2\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI model — chat_openai","title":"Chat with an OpenAI model — chat_openai","text":"main interface OpenAI's models, using responses API. can use access OpenAI's latest models features like image generation web search. need use OpenAI-compatible API another provider, chat completions API OpenAI,use chat_openai_compatible() instead. Note ChatGPT Plus membership grant access API. need sign developer account (pay ) developer platform.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat_openai(   system_prompt = NULL,   base_url = \"https://api.openai.com/v1\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   service_tier = c(\"auto\", \"default\", \"flex\", \"priority\"),   echo = c(\"none\", \"output\", \"all\") )  models_openai(   base_url = \"https://api.openai.com/v1\",   api_key = NULL,   credentials = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI model — chat_openai","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set OPENAI_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"gpt-4.1\"). regularly update default, strongly recommend explicitly specifying model anything casual use. Use models_openai() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_headers Named character vector arbitrary extra headers appended every chat API call. service_tier Request specific service tier. four options: \"auto\" (default): uses service tier configured Project settings. \"default\": standard pricing performance. \"flex\": slower cheaper. \"priority\": faster expensive. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI model — chat_openai","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI model — chat_openai","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\". chat$chat(\"   What is the difference between a tibble and a data frame?   Answer with a bulleted list \") #> - **Origin:** #>   - **Data Frame:** Base R object for storing tabular data. #>   - **Tibble:** Modern reimagining of data frames, part of the  #> tidyverse (tibble package). #>  #> - **Printing:** #>   - **Data Frame:** Prints the entire object, possibly flooding the  #> console. #>   - **Tibble:** Prints a preview (first 10 rows and fits columns to  #> screen), making output more readable. #>  #> - **Subsetting:** #>   - **Data Frame:** May simplify to a vector when selecting a single  #> column. #>   - **Tibble:** Always returns a tibble when subsetting columns with  #> `[`. #>  #> - **Column Names:** #>   - **Data Frame:** Allows non-syntactic names (with some issues). #>   - **Tibble:** Accepts any name but displays them as is, including  #> those with spaces or special characters (backticks needed to  #> reference). #>  #> - **Data Types:** #>   - **Data Frame:** Converts strings to factors by default (unless  #> specified otherwise). #>   - **Tibble:** Does **not** convert strings to factors by default. #>  #> - **Partial Matching:** #>   - **Data Frame:** Allows partial matching of column names. #>   - **Tibble:** Does **not** allow partial matching; requires exact  #> names. #>  #> - **Performance:** #>   - **Data Frame:** Slightly faster for basic operations due to  #> simpler structure. #>   - **Tibble:** Slightly slower but offers better usability,  #> especially for big data analysis. #>  #> - **Use in Tidyverse:** #>   - **Data Frame:** Used in base R workflows. #>   - **Tibble:** Default in tidyverse packages (ggplot2, dplyr, etc.). #>  #> - **Row Names:** #>   - **Data Frame:** Supports row names. #>   - **Tibble:** Does not support row names; stores them as a column if #> needed. #>  #> In summary, tibbles are a modern and tidyverse-friendly version of  #> data frames with improved usability and printing features.  chat$chat(\"Tell me three funny jokes about statisticians\") #> Absolutely! Here are three funny jokes about statisticians: #>  #> 1. **Why did the statistician bring a ladder to the bar?**   #>    Because they heard the drinks were on the house! #>  #> 2. **How does a statistician catch a lion?**   #>    They build a cage, label it as “population,” and wait for the lion  #> to walk right in—it's all about sampling! #>  #> 3. **Why don’t statisticians ever get sunburned?**   #>    Because they know how to avoid the \"mean rays.\""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai_compatible.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with an OpenAI-compatible model — chat_openai_compatible","title":"Chat with an OpenAI-compatible model — chat_openai_compatible","text":"function use OpenAI-compatible APIs, also known chat completions API. want use OpenAI , recommend chat_openai(), uses newer responses API. Many providers offer OpenAI-compatible APIs, including: Ollama local models vLLM self-hosted models Various cloud providers OpenAI-compatible endpoints","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai_compatible.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with an OpenAI-compatible model — chat_openai_compatible","text":"","code":"chat_openai_compatible(   base_url,   name = \"OpenAI-compatible\",   system_prompt = NULL,   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   api_headers = character(),   echo = c(\"none\", \"output\", \"all\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai_compatible.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with an OpenAI-compatible model — chat_openai_compatible","text":"base_url base URL endpoint. parameter required since default OpenAI-compatible APIs. name name provider; shown token_usage() used compute costs. system_prompt system prompt set behavior assistant. api_key Use credentials instead. credentials Credentials use authentication. provided, attempt use OPENAI_API_KEY environment variable. model model use chat. default; depends provider. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_headers Named character vector arbitrary extra headers appended every chat API call. echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai_compatible.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with an OpenAI-compatible model — chat_openai_compatible","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openai_compatible.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with an OpenAI-compatible model — chat_openai_compatible","text":"","code":"if (FALSE) { # \\dontrun{ # Example with Ollama (requires Ollama running locally) chat <- chat_openai_compatible(   base_url = \"http://localhost:11434/v1\",   model = \"llama2\" ) chat$chat(\"What is the difference between a tibble and a data frame?\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Sign https://openrouter.ai. Support features depends underlying model use; see https://openrouter.ai/models details.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"chat_openrouter(   system_prompt = NULL,   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"system_prompt system prompt set behavior assistant. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set OPENROUTER_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"gpt-4o\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_openrouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with one of the many models hosted on OpenRouter — chat_openrouter","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openrouter() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on perplexity.ai — chat_perplexity","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Sign https://www.perplexity.ai. Perplexity AI platform running LLMs capable searching web real-time help answer questions information may available model trained. function Uses OpenAI compatible API via chat_openai_compatible() defaults tweaked Perplexity AI.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"chat_perplexity(   system_prompt = NULL,   base_url = \"https://api.perplexity.ai/\",   api_key = NULL,   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set PERPLEXITY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). model model use chat (defaults \"llama-3.1-sonar-small-128k-online\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_perplexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on perplexity.ai — chat_perplexity","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_perplexity() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on PortkeyAI — chat_portkey","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"PortkeyAI provides interface (AI Gateway) connect Universal API variety LLMs providers via single endpoint.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"chat_portkey(   model,   system_prompt = NULL,   base_url = \"https://api.portkey.ai/v1\",   api_key = NULL,   credentials = NULL,   virtual_key = deprecated(),   params = NULL,   api_args = list(),   echo = NULL,   api_headers = character() )  models_portkey(base_url = \"https://api.portkey.ai/v1\", api_key = portkey_key())"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"model model name, e.g. @-provider/-model. system_prompt system prompt set behavior assistant. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set PORTKEY_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). virtual_key . Portkey now recommend supplying model provider (formerly known virtual_key), model name, e.g. @-provider/-model. See https://portkey.ai/docs/support/upgrade--model-catalog details. backward compatibility, PORTKEY_VIRTUAL_KEY env var still used model include provider. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"Chat object.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_portkey.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on PortkeyAI — chat_portkey","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_portkey() chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted on Snowflake — chat_snowflake","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Snowflake provider allows interact LLM models available Cortex LLM REST API.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"authentication","dir":"Reference","previous_headings":"","what":"Authentication","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"chat_snowflake() picks following ambient Snowflake credentials: static OAuth token defined via SNOWFLAKE_TOKEN environment variable. Key-pair authentication credentials defined via SNOWFLAKE_USER SNOWFLAKE_PRIVATE_KEY (can PEM-encoded private key path one) environment variables. Posit Workbench-managed Snowflake credentials corresponding account. Viewer-based credentials Posit Connect. Requires connectcreds package.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"known-limitations","dir":"Reference","previous_headings":"","what":"Known limitations","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Note Snowflake-hosted models support images.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"chat_snowflake(   system_prompt = NULL,   account = snowflake_account(),   credentials = NULL,   model = NULL,   params = NULL,   api_args = list(),   echo = c(\"none\", \"output\", \"all\"),   api_headers = character() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"system_prompt system prompt set behavior assistant. account Snowflake account identifier, e.g. \"testorg-test_account\". Defaults value SNOWFLAKE_ACCOUNT environment variable. credentials list authentication headers pass httr2::req_headers(), function returns called, NULL, default, use ambient credentials. model model use chat (defaults \"claude-3-7-sonnet\"). regularly update default, strongly recommend explicitly specifying model anything casual use. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_snowflake.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted on Snowflake — chat_snowflake","text":"","code":"if (FALSE) { # has_credentials(\"snowflake\") chat <- chat_snowflake() chat$chat(\"Tell me a joke in the form of a SQL query.\") }"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":null,"dir":"Reference","previous_headings":"","what":"Chat with a model hosted by vLLM — chat_vllm","title":"Chat with a model hosted by vLLM — chat_vllm","text":"vLLM open source library provides efficient convenient LLMs model server. can use chat_vllm() connect endpoints powered vLLM. Uses OpenAI compatible API via chat_openai_compatible().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"chat_vllm(   base_url,   system_prompt = NULL,   model,   params = NULL,   api_args = list(),   api_key = NULL,   credentials = NULL,   echo = NULL,   api_headers = character() )  models_vllm(base_url, api_key = NULL, credentials = NULL)"},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chat with a model hosted by vLLM — chat_vllm","text":"base_url base URL endpoint; default OpenAI's public API. system_prompt system prompt set behavior assistant. model model use chat. Use models_vllm() see options. params Common model parameters, usually created params(). api_args Named list arbitrary extra arguments appended body every chat API call. Combined body object generated ellmer modifyList(). api_key Use credentials instead. credentials Override default credentials. generally need argument; instead set VLLM_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). echo One following options: none: emit output (default running function). output: echo text tool-calling output streams (default running console). : echo input output. Note affects chat() method. api_headers Named character vector arbitrary extra headers appended every chat API call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chat with a model hosted by vLLM — chat_vllm","text":"Chat object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/chat_vllm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chat with a model hosted by vLLM — chat_vllm","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_vllm(\"http://my-vllm.com\") chat$chat(\"Tell me three jokes about statisticians\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_file_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload, downloand, and manage files for Claude — claude_file_upload","title":"Upload, downloand, and manage files for Claude — claude_file_upload","text":"Use beta Files API upload files manage files Claude. currently experimental API beta may change. Note need beta-headers = \"files-api-2025-04-14\" use API. Claude offers 100GB file storage per organization, file maximum size 500MB. details see https://docs.claude.com/en/docs/build--claude/files claude_file_upload() uploads file returns object can use chat. claude_file_list() lists uploaded files. claude_file_get() returns object previously uploaded file. claude_file_download() downloads file given ID. Note can download files created skills code execution tool. claude_file_delete() deletes file given ID.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_file_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload, downloand, and manage files for Claude — claude_file_upload","text":"","code":"claude_file_upload(   path,   base_url = \"https://api.anthropic.com/v1/\",   beta_headers = \"files-api-2025-04-14\",   credentials = NULL )  claude_file_list(   base_url = \"https://api.anthropic.com/v1/\",   credentials = NULL,   beta_headers = \"files-api-2025-04-14\" )  claude_file_get(   file_id,   base_url = \"https://api.anthropic.com/v1/\",   credentials = NULL,   beta_headers = \"files-api-2025-04-14\" )  claude_file_download(   file_id,   path,   base_url = \"https://api.anthropic.com/v1/\",   credentials = NULL,   beta_headers = \"files-api-2025-04-14\" )  claude_file_delete(   file_id,   base_url = \"https://api.anthropic.com/v1/\",   credentials = NULL,   beta_headers = \"files-api-2025-04-14\" )"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_file_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload, downloand, and manage files for Claude — claude_file_upload","text":"path Path download file . base_url base URL endpoint; default Claude's public API. beta_headers Beta headers use request. Defaults files-api-2025-04-14. credentials Override default credentials. generally need argument; instead set ANTHROPIC_API_KEY environment variable. best place set .Renviron, can easily edit calling usethis::edit_r_environ(). need additional control, argument takes zero-argument function returns either string (API key), named list (added additional headers every request). file_id ID file get information , download, delete.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_file_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload, downloand, and manage files for Claude — claude_file_upload","text":"","code":"if (FALSE) { # \\dontrun{ file <- claude_file_upload(\"path/to/file.pdf\") chat <- chat_anthropic(beta_headers = \"files-api-2025-04-14\") chat$chat(\"Please summarize the document.\", file) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_fetch.html","id":null,"dir":"Reference","previous_headings":"","what":"Claude web fetch tool — claude_tool_web_fetch","title":"Claude web fetch tool — claude_tool_web_fetch","text":"Enables Claude fetch analyze content web URLs. Claude can fetch URLs appear conversation context (user messages previous tool results). security reasons, Claude dynamically construct URLs fetch. Requires web-fetch-2025-09-10 beta header. Learn https://docs.claude.com/en/docs/agents--tools/tool-use/web-fetch-tool.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_fetch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Claude web fetch tool — claude_tool_web_fetch","text":"","code":"claude_tool_web_fetch(   max_uses = NULL,   allowed_domains = NULL,   blocked_domains = NULL,   citations = FALSE,   max_content_tokens = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_fetch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Claude web fetch tool — claude_tool_web_fetch","text":"max_uses Integer. Maximum number fetches allowed per request. allowed_domains Character vector. Restrict fetches specific domains. used blocked_domains. blocked_domains Character vector. Exclude specific domains fetches. used allowed_domains. citations Logical. Whether include citations response. Default TRUE. max_content_tokens Integer. Maximum number tokens fetch URL.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_fetch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Claude web fetch tool — claude_tool_web_fetch","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_claude(beta_headers = \"web-fetch-2025-09-10\") chat$register_tool(claude_tool_web_fetch()) chat$chat(\"What are the latest package releases on https://tidyverse.org/blog\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Claude web search tool — claude_tool_web_search","title":"Claude web search tool — claude_tool_web_search","text":"Enables Claude search web --date information. organization administrator must enable web search Anthropic Console using tool, costs extra ($10 per 1,000 tokens time writing). Learn https://docs.claude.com/en/docs/agents--tools/tool-use/web-search-tool.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Claude web search tool — claude_tool_web_search","text":"","code":"claude_tool_web_search(   max_uses = NULL,   allowed_domains = NULL,   blocked_domains = NULL,   user_location = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Claude web search tool — claude_tool_web_search","text":"max_uses Integer. Maximum number searches allowed per request. allowed_domains Character vector. Restrict searches specific domains (e.g., c(\"nytimes.com\", \"bbc.com\")). used blocked_domains. blocked_domains Character vector. Exclude specific domains searches. used allowed_domains. user_location List optional elements: country (2-letter code), city, region, timezone (IANA timezone) localize search results.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/claude_tool_web_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Claude web search tool — claude_tool_web_search","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_claude() chat$register_tool(claude_tool_web_search()) chat$chat(\"What was in the news today?\") chat$chat(\"What's the biggest news in the economy?\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode images for chat input — content_image_url","title":"Encode images for chat input — content_image_url","text":"functions used prepare image URLs files input chatbot. content_image_url() function used provide URL image, content_image_file() used provide image data .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode images for chat input — content_image_url","text":"","code":"content_image_url(url, detail = c(\"auto\", \"low\", \"high\"))  content_image_file(path, content_type = \"auto\", resize = \"low\")  content_image_plot(width = 768, height = 768)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode images for chat input — content_image_url","text":"url URL image include chat input. Can data: URL regular URL. Valid image types PNG, JPEG, WebP, non-animated GIF. detail detail setting image. Can \"auto\", \"low\", \"high\". path path image file include chat input. Valid file extensions .png, .jpeg, .jpg, .webp, (non-animated) .gif. content_type content type image (e.g. image/png). \"auto\", content type inferred file extension. resize \"low\", resize images fit within 512x512. \"high\", resize fit within 2000x768 768x2000. (See OpenAI docs specific sizes used.) \"none\", resize. can also pass custom string resize image specific size, e.g. \"200x200\" resize 200x200 pixels preserving aspect ratio. Append > resize image larger specified size, ! ignore aspect ratio (e.g. \"300x200>!\"). values none require magick package. width, height Width height pixels.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode images for chat input — content_image_url","text":"input object suitable including ... parameter chat(), stream(), chat_async(), stream_async() methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_image_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encode images for chat input — content_image_url","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openai() chat$chat(   \"What do you see in these images?\",   content_image_url(\"https://www.r-project.org/Rlogo.png\"),   content_image_file(system.file(\"httr2.png\", package = \"ellmer\")) )  plot(waiting ~ eruptions, data = faithful) chat <- chat_openai() chat$chat(   \"Describe this plot in one paragraph, as suitable for inclusion in    alt-text. You should briefly describe the plot type, the axes, and    2-5 major visual patterns.\",    content_image_plot() ) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Encode PDFs content for chat input — content_pdf_file","title":"Encode PDFs content for chat input — content_pdf_file","text":"functions used prepare PDFs input chatbot. content_pdf_url() function used provide URL PDF file, content_pdf_file() used local PDF files. providers support PDF input, check documentation provider using.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encode PDFs content for chat input — content_pdf_file","text":"","code":"content_pdf_file(path)  content_pdf_url(url)"},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encode PDFs content for chat input — content_pdf_file","text":"path, url Path URL PDF file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/content_pdf_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encode PDFs content for chat input — content_pdf_file","text":"ContentPDF object","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":null,"dir":"Reference","previous_headings":"","what":"Record and replay content — contents_record","title":"Record and replay content — contents_record","text":"generic functions can use convert Turn/Content objects easily serializable representations (.e. lists atomic vectors). contents_record() accepts Turn Content return simple list. contents_replay() takes output contents_record() returns Turn Content object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Record and replay content — contents_record","text":"","code":"contents_record(x)  contents_replay(x, tools = list(), .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_record.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Record and replay content — contents_record","text":"x Turn Content object serialize; serialized object replay. tools named list tools .envir environment look class definitions. Used recorded objects include classes extend Turn Content ellmer package .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Format contents into a textual representation — contents_text","title":"Format contents into a textual representation — contents_text","text":"generic functions can use convert Turn contents Content objects textual representations. contents_text() minimal includes ContentText objects output. contents_markdown() returns text content (assumes markdown convert ) plus markdown representations images content types. contents_html() returns text content, converted markdown HTML commonmark::markdown_html(), plus HTML representations images content types. content types continue grow change ellmer evolves support providers providers add content types.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format contents into a textual representation — contents_text","text":"","code":"contents_text(content, ...)  contents_html(content, ...)  contents_markdown(content, ...)"},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format contents into a textual representation — contents_text","text":"content Turn Content object converted text. contents_markdown() also accepts Chat instances turn entire conversation history markdown text. ... Additional arguments passed methods.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format contents into a textual representation — contents_text","text":"string text, markdown HTML.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/contents_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format contents into a textual representation — contents_text","text":"","code":"turns <- list(   UserTurn(list(     ContentText(\"What's this image?\"),     content_image_url(\"https://placehold.co/200x200\")   )),   AssistantTurn(\"It's a placeholder image.\") )  lapply(turns, contents_text) #> [[1]] #> [1] \"What's this image?\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  lapply(turns, contents_markdown) #> [[1]] #> [1] \"What's this image?\\n\\n![](https://placehold.co/200x200)\" #>  #> [[2]] #> [1] \"It's a placeholder image.\" #>  if (rlang::is_installed(\"commonmark\")) {   contents_html(turns[[1]]) } #> [1] \"<p>What's this image?<\/p>\\n\\n<img src=\\\"https://placehold.co/200x200\\\">\""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":null,"dir":"Reference","previous_headings":"","what":"Create metadata for a tool — create_tool_def","title":"Create metadata for a tool — create_tool_def","text":"order use function tool chat, need craft right call tool(). function helps documented functions extracting function's R documentation using LLM generate tool() call. meant used interactively writing code, part final code. function package documentation, used. Otherwise, source code function can automatically detected, comments immediately preceding function used (especially helpful roxygen2 comments). neither available, just function signature used. Note function inherently imperfect. handle possible R functions, parameters suitable use tool call (example, serializable simple JSON objects). documentation might specify expected shape arguments level detail allow exact JSON schema generated. Please sure review generated code using !","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create metadata for a tool — create_tool_def","text":"","code":"create_tool_def(topic, chat = NULL, echo = interactive(), verbose = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create metadata for a tool — create_tool_def","text":"topic symbol string literal naming function create metadata . Can also expression form pkg::fun. chat Chat object used generate output. NULL (default) uses chat_openai(). echo Emit registration code console. Defaults TRUE interactive sessions. verbose TRUE, print input send LLM, may useful debugging unexpectedly poor results.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create metadata for a tool — create_tool_def","text":"register_tool call can copy paste code. Returned invisibly echo TRUE.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/create_tool_def.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create metadata for a tool — create_tool_def","text":"","code":"if (FALSE) { # \\dontrun{   # These are all equivalent   create_tool_def(rnorm)   create_tool_def(stats::rnorm)   create_tool_def(\"rnorm\")   create_tool_def(\"rnorm\", chat = chat_azure_openai()) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/df_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe the schema of a data frame, suitable for sending to an LLM — df_schema","title":"Describe the schema of a data frame, suitable for sending to an LLM — df_schema","text":"df_schema() gives column--column description data frame. column, gives name, type, label (present), number missing values. numeric date/time columns, also gives range. character factor columns, also gives number unique values, (<= 10), values. goal give LLM sense structure data, can generate useful code, output attempts balance conciseness accuracy.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/df_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe the schema of a data frame, suitable for sending to an LLM — df_schema","text":"","code":"df_schema(df, max_cols = 50)"},{"path":"https://ellmer.tidyverse.org/dev/reference/df_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe the schema of a data frame, suitable for sending to an LLM — df_schema","text":"df data frame describe. max_cols Maximum number columns includes. Defaults 50 avoid accidentally generating large prompts.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/df_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe the schema of a data frame, suitable for sending to an LLM — df_schema","text":"","code":"df_schema(mtcars) #> [1] │ A data frame with 32 rows and 11 columns: #>     │ * mpg: numeric with range [10.4, 33.9], and 0 NAs #>     │ * cyl: numeric with range [4, 8], and 0 NAs #>     │ * disp: numeric with range [71.1, 472], and 0 NAs #>     │ * hp: numeric with range [52, 335], and 0 NAs #>     │ * drat: numeric with range [2.76, 4.93], and 0 NAs #>     │ * wt: numeric with range [1.513, 5.424], and 0 NAs #>     │ * qsec: numeric with range [14.5, 22.9], and 0 NAs #>     │ * vs: numeric with range [0, 1], and 0 NAs #>     │ * am: numeric with range [0, 1], and 0 NAs #>     │ * gear: numeric with range [3, 5], and 0 NAs #>     │ * carb: numeric with range [1, 8], and 0 NAs df_schema(iris) #> [1] │ A data frame with 150 rows and 5 columns: #>     │ * Sepal.Length: numeric with range [4.3, 7.9], and 0 NAs #>     │ * Sepal.Width: numeric with range [2, 4.4], and 0 NAs #>     │ * Petal.Length: numeric with range [1, 6.9], and 0 NAs #>     │ * Petal.Width: numeric with range [0.1, 2.5], and 0 NAs #>     │ * Species: nominal with 0 NAs, and 3 permitted values (\"setosa\", \"versicolor\", \"virginica\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"ellmer: Chat with Large Language Models — ellmer-package","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Chat large language models range providers including 'Claude' https://claude.ai, 'OpenAI' https://chatgpt.com, . Supports streaming, asynchronous calls, tool calling, structured data extraction.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/ellmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"ellmer: Chat with Large Language Models — ellmer-package","text":"Maintainer: Hadley Wickham hadley@posit.co (ORCID) Authors: Joe Cheng Aaron Jacobs Garrick Aden-Buie garrick@posit.co (ORCID) Barret Schloerke barret@posit.co (ORCID) contributors: Posit Software, PBC (ROR) [copyright holder, funder]","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_fetch.html","id":null,"dir":"Reference","previous_headings":"","what":"Google URL fetch tool — google_tool_web_fetch","title":"Google URL fetch tool — google_tool_web_fetch","text":"tool enabled, can include URLs directly prompts Gemini fetch analyze content. Learn https://ai.google.dev/gemini-api/docs/url-context.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_fetch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Google URL fetch tool — google_tool_web_fetch","text":"","code":"google_tool_web_fetch()"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_fetch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Google URL fetch tool — google_tool_web_fetch","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$register_tool(google_tool_web_fetch()) chat$chat(\"What are the latest package releases on https://tidyverse.org/blog?\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Google web search (grounding) tool — google_tool_web_search","title":"Google web search (grounding) tool — google_tool_web_search","text":"Enables Gemini models search web --date information ground responses citations sources. model automatically decides () search web based prompt. Search results incorporated response grounding metadata including source URLs titles. Learn https://ai.google.dev/gemini-api/docs/google-search.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Google web search (grounding) tool — google_tool_web_search","text":"","code":"google_tool_web_search()"},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/google_tool_web_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Google web search (grounding) tool — google_tool_web_search","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_google_gemini() chat$register_tool(google_tool_web_search()) chat$chat(\"What was in the news today?\") chat$chat(\"What's the biggest news in the economy?\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a file to gemini — google_upload","title":"Upload a file to gemini — google_upload","text":"function uploads file waits Gemini finish processing can immediately use prompt. experimental currently Gemini specific, expect providers evolve similar feature future. Uploaded files automatically deleted 2 days. file must less 2 GB can upload total 20 GB. ellmer currently provide way delete files early; please file issue useful .","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a file to gemini — google_upload","text":"","code":"google_upload(   path,   base_url = \"https://generativelanguage.googleapis.com/\",   api_key = NULL,   credentials = NULL,   mime_type = NULL )"},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a file to gemini — google_upload","text":"path Path file upload. base_url base URL endpoint; default OpenAI's public API. api_key Use credentials instead. credentials function returns list authentication headers NULL, default, use ambient credentials. See details. mime_type Optionally, specify mime type file. specified, guesses file extension.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a file to gemini — google_upload","text":"<ContentUploaded> object can passed $chat().","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/google_upload.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a file to gemini — google_upload","text":"","code":"if (FALSE) { # \\dontrun{ file <- google_upload(\"path/to/file.pdf\")  chat <- chat_google_gemini() chat$chat(file, \"Give me a three paragraph summary of this PDF\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Are credentials avaiable? — has_credentials","title":"Are credentials avaiable? — has_credentials","text":"Used examples/testing.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Are credentials avaiable? — has_credentials","text":"","code":"has_credentials(provider)"},{"path":"https://ellmer.tidyverse.org/dev/reference/has_credentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Are credentials avaiable? — has_credentials","text":"provider Provider name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for interpolating data into prompts — interpolate","title":"Helpers for interpolating data into prompts — interpolate","text":"functions lightweight wrappers around glue make easier interpolate dynamic data static prompt: interpolate() works string. interpolate_file() works file. interpolate_package() works file inst/prompts directory package. Compared glue, dynamic values wrapped {{ }}, making easier include R code JSON prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"interpolate(prompt, ..., .envir = parent.frame())  interpolate_file(path, ..., .envir = parent.frame())  interpolate_package(package, path, ..., .envir = parent.frame())"},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helpers for interpolating data into prompts — interpolate","text":"prompt prompt string. generally expose end user, since glue interpolation makes easy run arbitrary code. ... Define additional temporary variables substitution. .envir Environment evaluate ... expressions . Used wrapping another function. See vignette(\"wrappers\", package = \"glue\") details. path path prompt file (often .md). interpolate_package(), path relative inst/prompts. package Package name.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helpers for interpolating data into prompts — interpolate","text":"{glue} string.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/interpolate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helpers for interpolating data into prompts — interpolate","text":"","code":"joke <- \"You're a cool dude who loves to make jokes. Tell me a joke about {{topic}}.\"  # You can supply valuese directly: interpolate(joke, topic = \"bananas\") #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about bananas.  # Or allow interpolate to find them in the current environment: topic <- \"applies\" interpolate(joke) #> [1] │ You're a cool dude who loves to make jokes. Tell me a joke about applies."},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a live chat application — live_console","title":"Open a live chat application — live_console","text":"live_console() lets chat interactively console. live_browser() lets chat interactively browser. Note functions mutate input chat object chat turns appended history.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a live chat application — live_console","text":"","code":"live_console(chat, quiet = FALSE)  live_browser(chat, quiet = FALSE)"},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a live chat application — live_console","text":"chat chat object created chat_openai() friends. quiet TRUE, suppresses initial message explains use console.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a live chat application — live_console","text":"(Invisibly) input chat.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/live_console.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Open a live chat application — live_console","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_anthropic() live_console(chat) live_browser(chat) } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/openai_tool_web_search.html","id":null,"dir":"Reference","previous_headings":"","what":"OpenAI web search tool — openai_tool_web_search","title":"OpenAI web search tool — openai_tool_web_search","text":"Enables OpenAI models search web --date information. search behavior varies model: non-reasoning models perform simple searches, reasoning models can perform agentic, iterative searches. Learn https://platform.openai.com/docs/guides/tools-web-search","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/openai_tool_web_search.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"OpenAI web search tool — openai_tool_web_search","text":"","code":"openai_tool_web_search(   allowed_domains = NULL,   user_location = NULL,   external_web_access = TRUE )"},{"path":"https://ellmer.tidyverse.org/dev/reference/openai_tool_web_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"OpenAI web search tool — openai_tool_web_search","text":"allowed_domains Character vector. Restrict searches specific domains (e.g., c(\"nytimes.com\", \"bbc.com\")). Maximum 20 domains. URLs automatically cleaned (http/https prefixes removed). user_location List optional elements: country (2-letter ISO code), city, region, timezone (IANA timezone) localize search results. external_web_access Logical. Whether allow live internet access (TRUE, default) use cached/indexed results (FALSE).","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/openai_tool_web_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"OpenAI web search tool — openai_tool_web_search","text":"","code":"if (FALSE) { # \\dontrun{ chat <- chat_openai() chat$register_tool(openai_tool_web_search()) chat$chat(\"Very briefly summarise the top 3 news stories of the day\") chat$chat(\"Of those stories, which one do you think was the most interesting?\") } # }"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Submit multiple chats in parallel — parallel_chat","title":"Submit multiple chats in parallel — parallel_chat","text":"multiple prompts, can submit parallel. typically considerably faster submitting sequence, especially Gemini OpenAI. using chat_openai() chat_anthropic() willing wait longer, might want use batch_chat() instead, comes 50% discount return taking 24 hours.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"parallel_chat(   chat,   prompts,   max_active = 10,   rpm = 500,   on_error = c(\"return\", \"continue\", \"stop\") )  parallel_chat_text(   chat,   prompts,   max_active = 10,   rpm = 500,   on_error = c(\"return\", \"continue\", \"stop\") )  parallel_chat_structured(   chat,   prompts,   type,   convert = TRUE,   include_tokens = FALSE,   include_cost = FALSE,   max_active = 10,   rpm = 500,   on_error = c(\"return\", \"continue\", \"stop\") )"},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Submit multiple chats in parallel — parallel_chat","text":"chat chat object created chat_ function, string passed chat(). prompts vector created interpolate() list character vectors. max_active maximum number simultaneous requests send. chat_anthropic(), note number active connections limited primarily output tokens per minute limit (OTPM) estimated max_tokens parameter, defaults 4096. means usage tier limits 16,000 OTPM, either set max_active = 4 (16,000 / 4096) decrease number active connections use params() chat_anthropic() decrease max_tokens. rpm Maximum number requests per minute. on_error request fails. One : \"return\" (default): stop processing new requests, wait flight requests finish, return. \"continue\": keep going, performing every request. \"stop\": stop processing throw error. type type specification extracted data. created type_() function. convert TRUE, automatically convert JSON lists R data types using schema. typically works best type type_object() give data frame one column property. FALSE, returns list. include_tokens TRUE, result data frame, add input_tokens output_tokens columns giving total input output tokens prompt. include_cost TRUE, result data frame, add cost column giving cost prompt.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Submit multiple chats in parallel — parallel_chat","text":"parallel_chat(), list one element prompt. element either Chat object (successful), NULL (request performed) error object (failed). parallel_chat_text(), character vector one element prompt. Requests succesful get NA. parallel_chat_structured(), single structured data object one element prompt. Typically, type object, tibble one row prompt, one column property. output data frame, requests error, .error column added error objects.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/parallel_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Submit multiple chats in parallel — parallel_chat","text":"","code":"chat <- chat_openai() #> Using model = \"gpt-4.1\".  # Chat ---------------------------------------------------------------------- country <- c(\"Canada\", \"New Zealand\", \"Jamaica\", \"United States\") prompts <- interpolate(\"What's the capital of {{country}}?\") parallel_chat(chat, prompts) #> [[1]] #> <Chat OpenAI/gpt-4.1 turns=2 input=13 output=11 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of Canada? #> ── assistant [input=13 output=11 cost=$0.00] ────────────────────────── #> The capital of Canada is **Ottawa**. #>  #> [[2]] #> <Chat OpenAI/gpt-4.1 turns=2 input=14 output=12 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of New Zealand? #> ── assistant [input=14 output=12 cost=$0.00] ────────────────────────── #> The capital of New Zealand is **Wellington**. #>  #> [[3]] #> <Chat OpenAI/gpt-4.1 turns=2 input=13 output=15 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of Jamaica? #> ── assistant [input=13 output=15 cost=$0.00] ────────────────────────── #> The capital of **Jamaica** is **Kingston**. #>  #> [[4]] #> <Chat OpenAI/gpt-4.1 turns=2 input=14 output=15 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> What's the capital of United States? #> ── assistant [input=14 output=15 cost=$0.00] ────────────────────────── #> The capital of the United States is **Washington, D.C.** #>   # Structured data ----------------------------------------------------------- prompts <- list(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" ) type_person <- type_object(name = type_string(), age = type_number()) parallel_chat_structured(chat, prompts, type_person) #> # A tibble: 6 × 2 #>   name     age #>   <chr>  <dbl> #> 1 Alex      42 #> 2 Jamal     27 #> 3 Li Wei    19 #> 4 Fatima    35 #> 5 Robert    51 #> 6 Kwame     50"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":null,"dir":"Reference","previous_headings":"","what":"Standard model parameters — params","title":"Standard model parameters — params","text":"helper function makes easier create list parameters used across many models. parameter names automatically standardised included correctly place API call. Note parameters supported given provider generate warning, error. allows use set parameters across multiple providers.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Standard model parameters — params","text":"","code":"params(   temperature = NULL,   top_p = NULL,   top_k = NULL,   frequency_penalty = NULL,   presence_penalty = NULL,   seed = NULL,   max_tokens = NULL,   log_probs = NULL,   stop_sequences = NULL,   reasoning_effort = NULL,   reasoning_tokens = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Standard model parameters — params","text":"temperature Temperature sampling distribution. top_p cumulative probability token selection. top_k number highest probability vocabulary tokens keep. frequency_penalty Frequency penalty generated tokens. presence_penalty Presence penalty generated tokens. seed Seed random number generator. max_tokens Maximum number tokens generate. log_probs Include log probabilities output? stop_sequences character vector tokens stop generation . reasoning_effort, reasoning_tokens much effort spend thinking? ressoning_effort string, like \"low\", \"medium\", \"high\". reasoning_tokens integer, giving maximum token budget. provider takes one two parameters. ... Additional named parameters send provider.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":null,"dir":"Reference","previous_headings":"","what":"Report on token usage in the current session — token_usage","title":"Report on token usage in the current session — token_usage","text":"Call function find cumulative number tokens sent recieved current session. price shown known.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage()"},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Report on token usage in the current session — token_usage","text":"data frame","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/token_usage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Report on token usage in the current session — token_usage","text":"","code":"token_usage() #>    provider                      model input output cached_input price #> 1    OpenAI                    gpt-4.1   908    662            0 $0.01 #> 2 Anthropic claude-sonnet-4-5-20250929    14    215            0 $0.00"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a tool — tool","title":"Define a tool — tool","text":"Annotate function use tool calls, providing name, description, type definition arguments. Learn vignette(\"tool-calling\").","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a tool — tool","text":"","code":"tool(   fun,   description,   ...,   arguments = list(),   name = NULL,   convert = TRUE,   annotations = list(),   .name = deprecated(),   .description = deprecated(),   .convert = deprecated(),   .annotations = deprecated() )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a tool — tool","text":"fun function invoked tool called. return value function sent back chatbot. Expert users can customize tool result returning ContentToolResult object. description detailed description function . Generally, information can provide , better. ... Use arguments instead. arguments named list defines arguments accepted function. element created type_*() function. Use type_ignore() want LLM provide argument (e.g., R function suitable default value). name name function. can omitted fun existing function (.e. defined inline). convert JSON inputs automatically convert R data type equivalents? Defaults TRUE. annotations Additional properties describe tool behavior. Usually created tool_annotations(), can find description annotation properties recommended Model Context Protocol. .name, .description, .convert, .annotations Please switch non-prefixed equivalents.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a tool — tool","text":"S7 ToolDef object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ellmer-","dir":"Reference","previous_headings":"","what":"ellmer 0.3.0","title":"Define a tool — tool","text":"ellmer 0.3.0, definition tool() function changed quite bit. make easier update old versions, can use LLM following system prompt","code":"Help the user convert an ellmer 0.2.0 and earlier tool definition into a ellmer 0.3.0 tool definition. Here's what changed:  * All arguments, apart from the first, should be named, and the argument   names no longer use `.` prefixes. The argument order should be function,   name (as a string), description, then arguments, then anything  * Previously `arguments` was passed as `...`, so all type specifications   should now be moved into a named list and passed to the `arguments`   argument. It can be omitted if the function has no arguments.  ```R # old tool(   add,   \"Add two numbers together\"   x = type_number(),   y = type_number() )  # new tool(   add,   name = \"add\",   description = \"Add two numbers together\",   arguments = list(     x = type_number(),     y = type_number()   ) ) ```  Don't respond; just let the user provide function calls to convert."},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a tool — tool","text":"","code":"# First define the metadata that the model uses to figure out when to # call the tool tool_rnorm <- tool(   rnorm,   description = \"Draw numbers from a random normal distribution\",   arguments = list(     n = type_integer(\"The number of observations. Must be a positive integer.\"),     mean = type_number(\"The mean value of the distribution.\"),     sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\")   ) ) tool_rnorm(n = 5, mean = 0, sd = 1) #> [1] -1.400043517  0.255317055 -2.437263611 -0.005571287  0.621552721  chat <- chat_openai() #> Using model = \"gpt-4.1\". # Then register it chat$register_tool(tool_rnorm)  # Then ask a question that needs it. chat$chat(\"Give me five numbers from a random normal distribution.\") #> Here are five numbers drawn from a random normal distribution (mean =  #> 0, standard deviation = 1): #>  #> 1. 1.1484 #> 2. -1.8218 #> 3. -0.2473 #> 4. -0.2442 #> 5. -0.2827  # Look at the chat history to see how tool calling works: chat #> <Chat OpenAI/gpt-4.1 turns=4 input=234 output=86 cost=$0.00> #> ── user ─────────────────────────────────────────────────────────────── #> Give me five numbers from a random normal distribution. #> ── assistant [input=90 output=23 cost=$0.00] ────────────────────────── #> [tool request (fc_0b0635de4f44748f01692dba5a16388193a8de793c57908c2f)]: rnorm(n = 5L, mean = 0L, sd = 1L) #> ── user ─────────────────────────────────────────────────────────────── #> [tool result  (fc_0b0635de4f44748f01692dba5a16388193a8de793c57908c2f)]: [1.1484,-1.8218,-0.2473,-0.2442,-0.2827] #> ── assistant [input=144 output=63 cost=$0.00] ───────────────────────── #> Here are five numbers drawn from a random normal distribution (mean = 0, standard deviation = 1): #>  #> 1. 1.1484 #> 2. -1.8218 #> 3. -0.2473 #> 4. -0.2442 #> 5. -0.2827 # Assistant sends a tool request which is evaluated locally and # results are sent back in a tool result."},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":null,"dir":"Reference","previous_headings":"","what":"Tool annotations — tool_annotations","title":"Tool annotations — tool_annotations","text":"Tool annotations additional properties , passed .annotations argument tool(), provide additional information tool behavior. information can used display users, example Shiny app another user interface. annotations tool_annotations() drawn Model Context Protocol considered hints. Tool authors use annotations communicate tool properties, users note annotations guaranteed.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tool annotations — tool_annotations","text":"","code":"tool_annotations(   title = NULL,   read_only_hint = NULL,   open_world_hint = NULL,   idempotent_hint = NULL,   destructive_hint = NULL,   ... )"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tool annotations — tool_annotations","text":"title human-readable title tool. read_only_hint TRUE, tool modify environment. open_world_hint TRUE, tool may interact \"open world\" external entities. FALSE, tool's domain interaction closed. example, world web search tool open, world memory tool . idempotent_hint TRUE, calling tool repeatedly arguments additional effect environment. (meaningful read_only_hint FALSE.) destructive_hint TRUE, tool may perform destructive updates environment, otherwise performs additive updates. (meaningful read_only_hint FALSE.) ... Additional named parameters include tool annotations.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Tool annotations — tool_annotations","text":"list tool annotations.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_annotations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tool annotations — tool_annotations","text":"","code":"# See ?tool() for a full example using this function. # We're creating a tool around R's `rnorm()` function to allow the chatbot to # generate random numbers from a normal distribution. tool_rnorm <- tool(   rnorm,   # Describe the tool function to the LLM   .description = \"Drawn numbers from a random normal distribution\",   # Describe the parameters used by the tool function   n = type_integer(\"The number of observations. Must be a positive integer.\"),   mean = type_number(\"The mean value of the distribution.\"),   sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\"),   # Tool annotations optionally provide additional context to the LLM   .annotations = tool_annotations(     title = \"Draw Random Normal Numbers\",     read_only_hint = TRUE, # the tool does not modify any state     open_world_hint = FALSE # the tool does not interact with the outside world   ) ) #> Warning: The `...` argument of `tool()` is deprecated as of ellmer 0.3.0. #> ℹ Please use the `arguments` argument instead. #> Warning: The `.description` argument of `tool()` is deprecated as of ellmer #> 0.3.0. #> ℹ Please use the `description` argument instead. #> Warning: The `.annotations` argument of `tool()` is deprecated as of ellmer #> 0.3.0. #> ℹ Please use the `annotations` argument instead."},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":null,"dir":"Reference","previous_headings":"","what":"Reject a tool call — tool_reject","title":"Reject a tool call — tool_reject","text":"Throws error reject tool call. tool_reject() can used within tool function indicate tool call processed. tool_reject() can also called Chat$on_tool_request() callback. used callback, tool call rejected tool function invoked. example utils::askYesNo() used ask user permission accessing current working directory. happens directly tool function appropriate write tool definition know exactly called.   can achieve similar experience tools written others using tool_request callback. next example, imagine tool provided third-party package. example implements simple menu ask user consent running  tool.","code":"chat <- chat_openai(model = \"gpt-4.1-nano\")  list_files <- function() {   allow_read <- utils::askYesNo(     \"Would you like to allow access to your current directory?\"   )   if (isTRUE(allow_read)) {     dir(pattern = \"[.](r|R|csv)$\")   } else {     tool_reject()   } }  chat$register_tool(tool(   list_files,   \"List files in the user's current directory\" ))  chat$chat(\"What files are available in my current directory?\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) no #> #> Error: Tool call rejected. The user has chosen to disallow the tool #' call. #> It seems I am unable to access the files in your current directory right now. #> If you can tell me what specific files you're looking for or if you can #' provide #> the list, I can assist you further.  chat$chat(\"Try again.\") #> [tool call] list_files() #> Would you like to allow access to your current directory? (Yes/no/cancel) yes #> #> app.R #> #> data.csv #> The files available in your current directory are \"app.R\" and \"data.csv\". packaged_list_files_tool <- tool(   function() dir(pattern = \"[.](r|R|csv)$\"),   \"List files in the user's current directory\" )  chat <- chat_openai(model = \"gpt-4.1-nano\") chat$register_tool(packaged_list_files_tool)  always_allowed <- c()  # ContentToolRequest chat$on_tool_request(function(request) {   if (request@name %in% always_allowed) return()    answer <- utils::menu(     title = sprintf(\"Allow tool `%s()` to run?\", request@name),     choices = c(\"Always\", \"Once\", \"No\"),     graphics = FALSE   )    if (answer == 1) {     always_allowed <<- append(always_allowed, request@name)   } else if (answer %in% c(0, 3)) {     tool_reject()   } })  # Try choosing different answers to the menu each time chat$chat(\"What files are available in my current directory?\") chat$chat(\"How about now?\") chat$chat(\"And again now?\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reject a tool call — tool_reject","text":"","code":"tool_reject(reason = \"The user has chosen to disallow the tool call.\")"},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reject a tool call — tool_reject","text":"reason character string describing reason rejecting tool call.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/tool_reject.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reject a tool call — tool_reject","text":"Throws error class ellmer_tool_reject provided reason.","code":""},{"path":[]},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":null,"dir":"Reference","previous_headings":"","what":"Type specifications — type_boolean","title":"Type specifications — type_boolean","text":"functions specify object types way chatbots understand used tool calling structured data extraction. names based JSON schema, APIs expect behind scenes. translation R concepts types fairly straightforward. type_boolean(), type_integer(), type_number(), type_string() represent scalars. equivalent length-1 logical, integer, double, character vectors (respectively). type_enum() equivalent length-1 factor; string can take specified values. type_array() equivalent vector R. can use represent atomic vector: e.g. type_array(type_boolean()) equivalent logical vector type_array(type_string()) equivalent character vector). can also use represent list complicated types every element type (R base equivalent ), e.g. type_array(type_array(type_string())) represents list character vectors. type_object() equivalent named list R, every element must specified type. example, type_object(= type_string(), b = type_array(type_integer())) equivalent list element called string element called b integer vector. type_ignore() used tool calling indicate argument provided LLM. useful R function default value argument want LLM supply . type_from_schema() allows specify full schema want get back LLM JSON schema. useful pre-defined schema want use directly without manually creating type using type_*() functions. can point file path argument provide JSON string text. schema must valid JSON schema object.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Type specifications — type_boolean","text":"","code":"type_boolean(description = NULL, required = TRUE)  type_integer(description = NULL, required = TRUE)  type_number(description = NULL, required = TRUE)  type_string(description = NULL, required = TRUE)  type_enum(values, description = NULL, required = TRUE)  type_array(items, description = NULL, required = TRUE)  type_object(   .description = NULL,   ...,   .required = TRUE,   .additional_properties = FALSE )  type_from_schema(text, path)  type_ignore()"},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Type specifications — type_boolean","text":"description, .description purpose component. used LLM determine values pass tool values extract structured data, detail can provide , better. required, .required component argument required? type descriptions structured data, required = FALSE component exist data, LLM may hallucinate value. applies element nested inside type_object(). tool definitions, required = TRUE signals LLM always provide value. Arguments required = FALSE default value tool function's definition. LLM provide value, default value used. values Character vector permitted values. items type array items. Can created type_ function. ... <dynamic-dots> Name-type pairs defining components object must possess. .additional_properties Can object arbitrary additional properties explicitly listed? supported Claude. text JSON string. path file path JSON file.","code":""},{"path":"https://ellmer.tidyverse.org/dev/reference/type_boolean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Type specifications — type_boolean","text":"","code":"# An integer vector type_array(type_integer()) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeBasic> #>  .. @ description: NULL #>  .. @ required   : logi TRUE #>  .. @ type       : chr \"integer\"  # The closest equivalent to a data frame is an array of objects type_array(type_object(    x = type_boolean(),    y = type_string(),    z = type_number() )) #> <ellmer::TypeArray> #>  @ description: NULL #>  @ required   : logi TRUE #>  @ items      : <ellmer::TypeObject> #>  .. @ description          : NULL #>  .. @ required             : logi TRUE #>  .. @ properties           :List of 3 #>  .. .. $ x: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"boolean\" #>  .. .. $ y: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"string\" #>  .. .. $ z: <ellmer::TypeBasic> #>  .. ..  ..@ description: NULL #>  .. ..  ..@ required   : logi TRUE #>  .. ..  ..@ type       : chr \"number\" #>  .. @ additional_properties: logi FALSE  # There's no specific type for dates, but you use a string with the # requested format in the description (it's not gauranteed that you'll # get this format back, but you should most of the time) type_string(\"The creation date, in YYYY-MM-DD format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The creation date, in YYYY-MM-DD format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\" type_string(\"The update date, in dd/mm/yyyy format.\") #> <ellmer::TypeBasic> #>  @ description: chr \"The update date, in dd/mm/yyyy format.\" #>  @ required   : logi TRUE #>  @ type       : chr \"string\""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-development-version","dir":"Changelog","previous_headings":"","what":"ellmer (development version)","title":"ellmer (development version)","text":"ellmer now distinguish text content thinking content streaming, allowing downstream packages like shinychat provide specific UI thinking content (@simonpcouch, #909). chat_github() now uses chat_openai_compatible() improved compatibility, models_github() now supports custom base_url configuration (@D-M4rk, #877). chat_ollama() now contains slot top_k within params argument (@frankiethull).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-040","dir":"Changelog","previous_headings":"","what":"ellmer 0.4.0","title":"ellmer 0.4.0","text":"CRAN release: 2025-11-15","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"lifecycle-changes-0-4-0","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"ellmer 0.4.0","text":"chat_claude() longer deprecated alias chat_anthropic(), reflecting Anthropic’s recent rebranding developer tools Claude name (#758). models_claude() now alias models_anthropic(). parallel_chat() batch_chat() longer experimental. Chat$extract_data() -> chat$chat_structured() (0.2.0) Chat$extract_data_async() -> chat$chat_structured_async() (0.2.0) chat_anthropic(max_tokens) -> chat_anthropic(params) (0.2.0) chat_azure() -> chat_azure_openai() (0.2.0) chat_azure_openai(token) (0.1.1) chat_bedrock() -> chat_aws_bedrock() (0.2.0) chat_claude() -> chat_anthropic() (0.2.0) chat_cortex() -> chat_snowflake() (0.2.0) chat_gemini() -> chat_google_gemini() (0.2.0) chat_openai(seed) -> chat_openai(params) (0.2.0) create_tool_def(model) -> create_tool_def(chat) (0.2.0)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-4-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.4.0","text":"batch_*() longer hashes properties provider besides name, model, base_url. provide protection accidentally reusing .json file different providers, still allowing use batch file across ellmer versions. also new ignore_hash argument allows opt check ’re confident difference arises ellmer changed. chat_claude() gains new cache parameter control caching. default set “5m”. (average) reduce cost chats (#584). chat_openai() now uses OpenAI’s responses endpoint (#365, #801). recommended endpoint gives access built-tools. chat_openai_compatible() replaces chat_openai() interface use OpenAI-compatible APIs, chat_openai() reserved official OpenAI API. Unlike previous versions chat_openai(), base_url parameter now required (#801). chat_*() functions now use credentials function instead api_key (#613). means API keys never stored chat object (might saved disk), instead retrieved demand needed. generally shouldn’t need use credentials argument, , use dynamically retrieve API key source (.e. never inline secret directly function call). New set claude_file_() functions managing file uploads Claude (@dcomputing, #761). claude_tool_web_search() claude_tool_web_fetch() Claude. google_tool_web_search() google_tool_web_fetch() Gemini. openai_tool_web_search() OpenAI. want web fetch providers, use btw::btw_tool_web_read_url(). parallel_chat() friends now permissive attitude errors. default, now return hitting first error (rather erroring), can control behaviour on_error argument. interrupt job, finish current requests return work done far. main downside work output parallel_chat() complex: now mix Chat objects, error objects, NULL (#628). parallel_chat_structured() longer errors results fail parse. Instead warns, corresponding rows filled appropriate missing values (#628). New schema_df() describe schema data frame LLM (#744). tool()s can now return image PDF content types, content_image_file() content_image_pdf() (#735). params() gains new reasoning_effort reasoning_tokens can control amount effort model spends thinking. Initial support provided chat_claude(), chat_google_gemini(), chat_openai() (#720). New type_ignore() allows specify tool argument provided LLM R function suitable default value (#764).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"minor-improvements-and-bug-fixes-0-4-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"ellmer 0.4.0","text":"Updated pricing data (#790). AssistantTurns now @duration slot, containing total time complete request (@simonpcouch, #798). batch_chat() logs tokens , retrieval (#743). batch_chat() now retrieves failed results chat_openai() (#830) gracefully handles invalid JSON (#845). batch_chat() now works chat_anthropic() (#835). batch_chat_*() parallel_chat_*() now accept string chat object, following rules chat() (#677). chat_claude() chat_aws_bedrock() now default Claude Sonnet 4.5 (#800). chat_databricks() lifts many restrictions now Databricks’ API OpenAI compatible (#757). chat_google_gemini() chat_openai() support image generation (#368). chat_google_gemini() experimental fallback interactive OAuth flow, ’re interactive session authentication options can found (#680). chat_groq() now defaults llama-3.1-8b-instant. chat_openai() gains service_tier argument (#712). chat_portkey() now requires supply model (#786). chat_portkey(virtual_key) longer needs supplied; instead Portkey recommends including virtual key/provider model (#786). Chat$chat(), Chat$stream(), similar methods now add empty tool results chat interrupted tool call loop, allowing conversation resumed without causing API error (#840). Chat$chat_structured() friends now warn multiple JSON payloads found (instead erroring) (@kbenoit, #732). Chat$get_tokens() gives brief description turn contents make easier see turn tokens spent (#618) also returns cost (#824). now returns one row assistant turn, better representing underlying data received LLM APIs. Similarly, print() method now reports costs assistant turn, rather trying parse individual costs. interpolate_package() now provides informative error requested prompt file found package’s prompts/ directory (#763) now works -development packages loaded devtools (#766). models_mistral() lists available models (@rplsmn, #750). models_ollama() fixed correctly query model capabilities remote Ollama servers (#746). chat_ollama() now uses credentials checking Ollama available models_ollama() now credentials argument. useful accessing Ollama servers require authentication (@AdaemmerP, #863). parallel_chat_structured() now returns tibble, since better job printing complex data frames (#787).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-032","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.2","title":"ellmer 0.3.2","text":"CRAN release: 2025-09-03 chat_aws_bedrock(), chat_databricks(), chat_deepseek(), chat_github(), chat_groq(), chat_ollama(), chat_openrouter(), chat_perplexity(), chat_vllm() now support params argument accepts common model parameters params(). deployment_id argument chat_azure_openai() deprecated replaced model better align providers. chat_openai() now correctly maps max_tokens top_k params() OpenAI API parameters (#699).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-031","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.1","title":"ellmer 0.3.1","text":"CRAN release: 2025-08-24 chat_anthropic() drops empty assistant turns avoid API errors (#710). chat_github() now uses https://models.github.ai/inference endpoint chat() supports GitHub models format chat(\"github/openai/gpt-4.1\") (#726). chat_google_vertex() authentication fixed using broader scope (#704, @netique) chat_google_vertex() can now use global project location (#704, @netique) chat_openai() now uses OPENAI_BASE_URL, set, base_url. Similarly, chat_ollama() also uses OLLAMA_BASE_URL set (#713). contents_record() contents_replay() now record replay custom classes extend ellmer’s Turn Content classes (#689). contents_replay() now also restores tool definition ContentToolResult objects (@request@tool) (#693). chat_snowflake() now supports Privatelink accounts (#694, @robert-norberg). works Snowflake’s latest API changes (#692, @robert-norberg). models_google_vertex() works (#704, @netique) value_turn() method OpenAI providers, usage checked NULL logging tokens avoid errors streaming OpenAI-compatible services (#706, @stevegbrooks).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-030","dir":"Changelog","previous_headings":"","what":"ellmer 0.3.0","title":"ellmer 0.3.0","text":"CRAN release: 2025-07-24","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.3.0","text":"New chat() allows chat provider using string like chat(\"anthropic\") chat(\"openai/gpt-4.1-nano\") (#361). tool() simpler specification: now specify name, description, arguments. done best deprecate old usage give clear errors, likely missed edge cases. apologize pain causes, ’m convinced going make tool usage easier clearer long run. many calls convert, ?tool contains prompt help use LLM convert (#603). also now returns function can call (/export package) (#602). type_array() type_enum() now description second argument items/values first. makes easier use common case description isn’t necessary (#610). ellmer now retries requests 3 times, controllable option(ellmer_max_tries), retry connection fails (rather just request returns transient error). default timeout, controlled option(ellmer_timeout_s), now applies initial connection phase. Together, changes make much likely ellmer requests succeed. New parallel_chat_text() batch_chat_text() make easier just get text response multiple prompts (#510). ellmer’s cost estimates considerably improved. chat_openai(), chat_google_gemini(), chat_anthropic() capture number cached input tokens. primarily useful OpenAI Gemini since offer automatic caching, yielding improved cost estimates (#466). also better source pricing data, LiteLLM. considerably expands number providers models include cost information (#659).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"bug-fixes-and-minor-improvements-0-3-0","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.3.0","text":"new ellmer_echo option controls default value echo. batch_chat_structured() provides clear messaging prompts/path/provider don’t match (#599). chat_aws_bedrock() allows set base_url() (#441). chat_aws_bedrock(), chat_google_gemini(), chat_ollama(), chat_vllm() use robust method generate model URLs base_url (#593, @benyake). chat_cortex_analyst() deprecated; please use chat_snowflake() instead (#640). chat_github() (OpenAI extensions) longer warn seed (#574). chat_google_gemini() chat_google_vertex() default Gemini 2.5 flash (#576). chat_huggingface() works much better. chat_openai() supports content_pdf_() (#650). chat_portkey() works , reads virtual API key PORTKEY_VIRTUAL_KEY env var (#588). chat_snowflake() works tool calling (#557, @atheriel). Chat$chat_structured() friends longer unnecessarily wrap type_object() chat_openai() (#671). Chat$chat_structured() suppresses tool use. need use tools structured data together, first use $chat() needed tools, $chat_structured() extract data need. Chat$chat_structured() longer requires prompt (since may obvious context) (#570). Chat$register_tool() shows message replace existing tool (#625). contents_record() contents_replay() record replay Turn related information Chat instance (#502). methods can used bookmarking within {shinychat}. models_github() lists models chat_github() (#561). models_ollama() includes capabilities column comma-separated list model capabilities (#623). parallel_chat() friends accept lists Content objects prompt (#597, @thisisnic). Tool requests show converted arguments printed (#517). tool() checks name valid (#625).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-021","dir":"Changelog","previous_headings":"","what":"ellmer 0.2.1","title":"ellmer 0.2.1","text":"CRAN release: 2025-06-03 save Chat object disk, API keys means can longer easily resume chat ’ve saved disk (’ll figure future release) ensures never accidentally save secret key RDS file (#534). chat_anthropic() now defaults Claude Sonnet 4, ’ve added pricing information latest generation Claude models. chat_databricks() now picks Databricks workspace URLs set configuration file, improve compatibility Databricks CLI (#521, @atheriel). now also supports tool calling (#548, @atheriel). chat_snowflake() longer streams answers include mysterious list(type = \"text\", text = \"\") trailer (#533, @atheriel). now parses streaming outputs correctly turns (#542), supports structured ouputs (#544), standard model parameters (#545, @atheriel). chat_snowflake() chat_databricks() now default Claude Sonnet 3.7, default chat_anthropic() (#539 #546, @atheriel). type_from_schema() lets use pre-existing JSON schemas structured chats (#133, @hafen)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-020","dir":"Changelog","previous_headings":"","what":"ellmer 0.2.0","title":"ellmer 0.2.0","text":"CRAN release: 2025-05-17","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"breaking-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"ellmer 0.2.0","text":"made number refinements way ellmer converts JSON R data structures. breaking changes, although don’t expect affect much code wild. importantly, tools now invoked inputs coerced standard R data structures (#461); opt-setting convert = FALSE tool(). Additionally ellmer now converts NULL NA type_boolean(), type_integer(), type_number(), type_string() (#445), better job arrays required = FALSE (#384). chat_ functions longer turn argument. need set turns, can now use Chat$set_turns() (#427). Additionally, Chat$tokens() renamed Chat$get_tokens() returns data frame tokens, correctly aligned individual turn. print method now uses show many input/output tokens used turn (#354).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.2.0","text":"Two new interfaces help multiple chats single function call: batch_chat() batch_chat_structured() allow submit multiple chats OpenAI Anthropic’s batched interfaces. guarantee response within 24 hours, 50% price regular requests (#143). parallel_chat() parallel_chat_structured() work provider allow submit multiple chats parallel (#143). doesn’t give cost savings, ’s can much, much faster. new family functions experimental ’m 100% sure shape user interface correct, particularly pertains handling errors. google_upload() lets upload files Google Gemini Vertex AI (#310). allows work videos, PDFs, large files Gemini. models_google_gemini(), models_anthropic(), models_openai(), models_aws_bedrock(), models_ollama() models_vllm(), list available models Google Gemini, Anthropic, OpenAI, AWS Bedrock, Ollama, VLLM respectively. Different providers return different metadata guaranteed return data frame least id column (#296). possible (currently Gemini, Anthropic, OpenAI) include known token prices (per million tokens). interpolate() friends now vectorised can generate multiple prompts (e.g.) data frame inputs. also now return specially classed object custom print method (#445). New interpolate_package() makes easier interpolate prompts stored inst/prompts directory inside package (#164). chat_anthropic(), chat_azure(), chat_openai(), chat_gemini() now take params argument, coupled params() helper, makes easy specify common model parameters (like seed temperature) across providers. Support providers grow request (#280). ellmer now tracks cost input output tokens. cost displayed print Chat object, tokens_usage(), Chat$get_cost(). can also request costs parallel_chat_structured(). best accurately compute cost, treat estimate rather exact price. Unfortunately LLM providers currently make difficult figure exactly much queries cost (#203).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"provider-updates-0-2-0","dir":"Changelog","previous_headings":"","what":"Provider updates","title":"ellmer 0.2.0","text":"support three new providers: chat_huggingface() models hosted https://huggingface.co (#359, @s-spavound). chat_mistral() models hosted https://mistral.ai (#319). chat_portkey() models_portkey() models hosted https://portkey.ai (#363, @maciekbanas). also renamed (deprecation) functions make naming scheme consistent (#382, @gadenbuie): chat_azure_openai() replaces chat_azure(). chat_aws_bedrock() replaces chat_bedrock(). chat_anthropic() replaces chat_anthropic(). chat_google_gemini() replaces chat_gemini(). updated default model couple providers: chat_anthropic() uses Sonnet 3.7 (also now displays) (#336). chat_openai() uses GPT-4.1 (#512)","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"developer-tooling-0-2-0","dir":"Changelog","previous_headings":"","what":"Developer tooling","title":"ellmer 0.2.0","text":"New Chat$get_provider() lets access underlying provider object (#202). Chat$chat_async() Chat$stream_async() gain tool_mode argument decide \"sequential\" \"concurrent\" tool calling. advanced feature primarily affects asynchronous tools (#488, @gadenbuie). Chat$stream() Chat$stream_async() gain support streaming additional content types generated tool call new stream argument. stream = \"content\" set, streaming response yields Content objects, including ContentToolRequest ContentToolResult objects used request return tool calls (#400, @gadenbuie). New Chat$on_tool_request() $on_tool_result() methods allow register callbacks run tool request tool result. callbacks can used implement custom logging actions tools called, without modifying tool function (#493, @gadenbuie). Chat$chat(echo = \"output\") replaces now-deprecated echo = \"text\" option. using echo = \"output\", additional output, tool requests results, shown occur. echo = \"none\", tool call failures emitted warnings (#366, @gadenbuie). ContentToolResult objects can now returned directly tool() function now includes additional information (#398 #399, @gadenbuie): extra: list additional data associated tool result shown chatbot. request: ContentToolRequest triggered tool call. ContentToolResult longer id property, instead tool call ID can retrieved request@id. also include error condition error property tool call fails (#421, @gadenbuie). ContentToolRequest gains tool property includes tool() definition request matched tool ellmer (#423, @gadenbuie). tool() gains .annotations argument can created tool_annotations() helper. Tool annotations described Model Context Protocol can used describe tool clients. (#402, @gadenbuie) New tool_reject() function can used reject tool request explanation rejection reason. tool_reject() can called within tool function Chat$on_tool_request() callback. latter case, rejecting tool call ensure tool function evaluated (#490, #493, @gadenbuie).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"minor-improvements-and-bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor improvements and bug fixes","title":"ellmer 0.2.0","text":"requests now set custom User-Agent identifies requests come ellmer (#341). default timeout increased 5 minutes (#451, #321). chat_anthropic() now supports thinking content type (#396), content_image_url() (#347). gains beta_header argument opt-beta features (#339). (along chat_bedrock()) longer chokes receiving output consists whitespace (#376). Finally, chat_anthropic(max_tokens =) now deprecated favour chat_anthropic(params = ) (#280). chat_google_gemini() chat_google_vertex() gain ways authenticate. can use GEMINI_API_KEY set (@t-kalinowski, #513), authenticate Google default application credentials (including service accounts, etc) (#317, @atheriel) use viewer-based credentials running Posit Connect (#320, @atheriel). Authentication default application credentials requires {gargle} package. now also can now handle responses include citation metadata (#358). chat_ollama() now works tool() definitions optional arguments empty properties (#342, #348, @gadenbuie), now accepts api_key consults OLLAMA_API_KEY environment variable. needed local usage, enables bearer-token authentication Ollama running behind reverse proxy (#501, @gadenbuie). chat_openai(seed =) now deprecated favour chat_openai(params = ) (#280). create_tool_def() can now use Chat instance (#118, @pedrobtz). live_browser() now requires {shinychat} v0.2.0 later provides access app powers live_browser() via shinychat::chat_app(), well Shiny module easily including chat interface ellmer Chat object Shiny apps (#397, @gadenbuie). now initializes UI messages chat turns, rather replaying turns server-side (#381). Provider gains name model fields (#406). now reported print chat object used token_usage().","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-011","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.1","title":"ellmer 0.1.1","text":"CRAN release: 2025-02-06","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"lifecycle-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"ellmer 0.1.1","text":"option(ellmer_verbosity) longer supported; instead use standard httr2 verbosity functions, httr2::with_verbosity(); now support streaming data. chat_cortex() renamed chat_cortex_analyst() better disambiguate chat_snowflake() (also uses “Cortex”) (#275, @atheriel).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"ellmer 0.1.1","text":"providers now wait 60s get complete response. can increase , e.g., option(ellmer_timeout_s = 120) (#213, #300). chat_azure(), chat_databricks(), chat_snowflake(), chat_cortex_analyst() now detect viewer-based credentials running Posit Connect (#285, @atheriel). chat_deepseek() provides support DeepSeek models (#242). chat_openrouter() provides support models hosted OpenRouter (#212). chat_snowflake() allows chatting models hosted Snowflake’s Cortex LLM REST API (#258, @atheriel). content_pdf_file() content_pdf_url() allow upload PDFs supported models. Models currently support PDFs Google Gemini Claude Anthropic. help @walkerke @andrie (#265).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"bug-fixes-and-minor-improvements-0-1-1","dir":"Changelog","previous_headings":"","what":"Bug fixes and minor improvements","title":"ellmer 0.1.1","text":"Chat$get_model() returns model name (#299). chat_azure() greatly improved support Azure Entra ID. API keys now optional can pick ambient credentials Azure service principals attempt use interactive Entra ID authentication possible. broken--design token argument deprecated (handle refreshing tokens properly), new credentials argument can used custom Entra ID support needed instead (instance, ’re trying use tokens generated AzureAuth package) (#248, #263, #273, #257, @atheriel). chat_azure() now reports better error messages underlying HTTP requests fail (#269, @atheriel). now also defaults api_version = \"2024-10-21\" includes data structured data extraction (#271). chat_bedrock() now handles temporary IAM credentials better (#261, @atheriel) chat_bedrock() gains api_args argument (@billsanto, #295). chat_databricks() now handles DATABRICKS_HOST environment variable correctly whether includes HTTPS prefix (#252, @atheriel). also respects SPARK_CONNECT_USER_AGENT environment variable making requests (#254, @atheriel). chat_gemini() now defaults using gemini-2.0-flash model. print(Chat) longer wraps long lines, making easier read code bulleted lists (#246).","code":""},{"path":"https://ellmer.tidyverse.org/dev/news/index.html","id":"ellmer-010","dir":"Changelog","previous_headings":"","what":"ellmer 0.1.0","title":"ellmer 0.1.0","text":"CRAN release: 2025-01-09 New chat_vllm() chat models served vLLM (#140). default chat_openai() model now GPT-4o. New Chat$set_turns() set turns. Chat$turns() now Chat$get_turns(). Chat$system_prompt() replaced Chat$set_system_prompt() Chat$get_system_prompt(). Async streaming async chat now event-driven use later::later_fd() wait efficiently curl socket activity (#157). New chat_bedrock() chat AWS bedrock models (#50). New chat$extract_data() uses structured data API available (tool calling otherwise) extract data structured according known type specification. can create specs functions type_boolean(), type_integer(), type_number(), type_string(), type_enum(), type_array(), type_object() (#31). general ToolArg() replaced specific type_*() functions. ToolDef() renamed tool. content_image_url() now create inline images given data url (#110). Streaming ollama results works (#117). Streaming OpenAI results now capture results, including logprobs (#115). New interpolate() prompt_file() make easier create prompts mix static text dynamic values. can find many tokens ’ve used current session calling token_usage(). chat_browser() chat_console() now live_browser() live_console(). echo can now one three values: “none”, “text”, “”. “”, ’ll now see user assistant turns, content types printed, just text. running global environment, echo defaults “text”, running inside function defaults “none”. can now log low-level JSON request/response info setting options(ellmer_verbosity = 2). chat$register_tool() now takes object created Tool(). makes little easier reuse tool definitions (#32). new_chat_openai() now chat_openai(). Claude Gemini now supported via chat_claude() chat_gemini(). Snowflake Cortex Analyst now supported via chat_cortex() (#56). Databricks now supported via chat_databricks() (#152).","code":""}]
