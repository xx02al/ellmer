<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Submit multiple chats in one batch — batch_chat • ellmer</title><!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png"><link rel="icon" type="”image/svg+xml”" href="../favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" sizes="any" href="../favicon.ico"><link rel="manifest" href="../site.webmanifest"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Source_Sans_Pro-0.4.10/font.css" rel="stylesheet"><link href="../deps/Source_Code_Pro-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Submit multiple chats in one batch — batch_chat"><meta name="description" content="
batch_chat() and batch_chat_structured() currently only work with
chat_openai() and chat_anthropic(). They use the
OpenAI and
Anthropic
batch APIs which allow you to submit multiple requests simultaneously.
The results can take up to 24 hours to complete, but in return you pay 50%
less than usual (but note that ellmer doesn't include this discount in
its pricing metadata). If you want to get results back more quickly, or
you're working with a different provider, you may want to use
parallel_chat() instead.
Since batched requests can take a long time to complete, batch_chat()
requires a file path that is used to store information about the batch so
you never lose any work. You can either set wait = FALSE or simply
interrupt the waiting process, then later, either call batch_chat() to
resume where you left off or call batch_chat_completed() to see if the
results are ready to retrieve. batch_chat() will store the chat responses
in this file, so you can either keep it around to cache the results,
or delete it to free up disk space.
This API is marked as experimental since I don't yet know how to handle
errors in the most helpful way. Fortunately they don't seem to be common,
but if you have ideas, please let me know!"><meta property="og:description" content="
batch_chat() and batch_chat_structured() currently only work with
chat_openai() and chat_anthropic(). They use the
OpenAI and
Anthropic
batch APIs which allow you to submit multiple requests simultaneously.
The results can take up to 24 hours to complete, but in return you pay 50%
less than usual (but note that ellmer doesn't include this discount in
its pricing metadata). If you want to get results back more quickly, or
you're working with a different provider, you may want to use
parallel_chat() instead.
Since batched requests can take a long time to complete, batch_chat()
requires a file path that is used to store information about the batch so
you never lose any work. You can either set wait = FALSE or simply
interrupt the waiting process, then later, either call batch_chat() to
resume where you left off or call batch_chat_completed() to see if the
results are ready to retrieve. batch_chat() will store the chat responses
in this file, so you can either keep it around to cache the results,
or delete it to free up disk space.
This API is marked as experimental since I don't yet know how to handle
errors in the most helpful way. Fortunately they don't seem to be common,
but if you have ideas, please let me know!"><meta property="og:image" content="https://ellmer.tidyverse.org/logo.png"><meta name="robots" content="noindex"><script defer data-domain="ellmer.tidyverse.org,all.tidyverse.org" src="https://plausible.io/js/plausible.js"></script></head><body>
    <a href="#container" class="visually-hidden-focusable">Skip to content</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-none" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">ellmer</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.3.2.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/ellmer.html">Get started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/programming.html">Programming with ellmer</a></li>
    <li><a class="dropdown-item" href="../articles/prompt-design.html">Prompt design</a></li>
    <li><a class="dropdown-item" href="../articles/streaming-async.html">Streaming and async APIs</a></li>
    <li><a class="dropdown-item" href="../articles/structured-data.html">Structured data</a></li>
    <li><a class="dropdown-item" href="../articles/tool-calling.html">Tool/function calling</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news"><li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="external-link dropdown-item" href="https://www.tidyverse.org/blog/2025/07/ellmer-0-3-0/">Version 0.3.0</a></li>
    <li><a class="external-link dropdown-item" href="https://www.tidyverse.org/blog/2025/05/ellmer-0-2-0/">Version 0.2.0</a></li>
    <li><a class="external-link dropdown-item" href="https://posit.co/blog/announcing-ellmer/">Version 0.1.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tidyverse/ellmer/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic" id="container">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Submit multiple chats in one batch</h1>
      <small class="dont-index">Source: <a href="https://github.com/tidyverse/ellmer/blob/main/R/batch-chat.R" class="external-link"><code>R/batch-chat.R</code></a></small>
      <div class="d-none name"><code>batch_chat.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p><a href="https://lifecycle.r-lib.org/articles/stages.html#experimental" class="external-link"><img src="figures/lifecycle-experimental.svg" alt="[Experimental]"></a></p>
<p><code>batch_chat()</code> and <code>batch_chat_structured()</code> currently only work with
<code><a href="chat_openai.html">chat_openai()</a></code> and <code><a href="chat_anthropic.html">chat_anthropic()</a></code>. They use the
<a href="https://platform.openai.com/docs/guides/batch" class="external-link">OpenAI</a> and
<a href="https://docs.anthropic.com/en/docs/build-with-claude/batch-processing" class="external-link">Anthropic</a>
batch APIs which allow you to submit multiple requests simultaneously.
The results can take up to 24 hours to complete, but in return you pay 50%
less than usual (but note that ellmer doesn't include this discount in
its pricing metadata). If you want to get results back more quickly, or
you're working with a different provider, you may want to use
<code><a href="parallel_chat.html">parallel_chat()</a></code> instead.</p>
<p>Since batched requests can take a long time to complete, <code>batch_chat()</code>
requires a file path that is used to store information about the batch so
you never lose any work. You can either set <code>wait = FALSE</code> or simply
interrupt the waiting process, then later, either call <code>batch_chat()</code> to
resume where you left off or call <code>batch_chat_completed()</code> to see if the
results are ready to retrieve. <code>batch_chat()</code> will store the chat responses
in this file, so you can either keep it around to cache the results,
or delete it to free up disk space.</p>
<p>This API is marked as experimental since I don't yet know how to handle
errors in the most helpful way. Fortunately they don't seem to be common,
but if you have ideas, please let me know!</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">batch_chat</span><span class="op">(</span><span class="va">chat</span>, <span class="va">prompts</span>, <span class="va">path</span>, wait <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">batch_chat_text</span><span class="op">(</span><span class="va">chat</span>, <span class="va">prompts</span>, <span class="va">path</span>, wait <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">batch_chat_structured</span><span class="op">(</span></span>
<span>  <span class="va">chat</span>,</span>
<span>  <span class="va">prompts</span>,</span>
<span>  <span class="va">path</span>,</span>
<span>  <span class="va">type</span>,</span>
<span>  wait <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  convert <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  include_tokens <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  include_cost <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">batch_chat_completed</span><span class="op">(</span><span class="va">chat</span>, <span class="va">prompts</span>, <span class="va">path</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-chat">chat<a class="anchor" aria-label="anchor" href="#arg-chat"></a></dt>
<dd><p>A base chat object.</p></dd>


<dt id="arg-prompts">prompts<a class="anchor" aria-label="anchor" href="#arg-prompts"></a></dt>
<dd><p>A vector created by <code><a href="interpolate.html">interpolate()</a></code> or a list
of character vectors.</p></dd>


<dt id="arg-path">path<a class="anchor" aria-label="anchor" href="#arg-path"></a></dt>
<dd><p>Path to file (with <code>.json</code> extension) to store state.</p>
<p>The file records a hash of the provider, the prompts, and the existing
chat turns. If you attempt to reuse the same file with any of these being
different, you'll get an error.</p></dd>


<dt id="arg-wait">wait<a class="anchor" aria-label="anchor" href="#arg-wait"></a></dt>
<dd><p>If <code>TRUE</code>, will wait for batch to complete. If <code>FALSE</code>,
it will return <code>NULL</code> if the batch is not complete, and you can retrieve
the results later by re-running <code>batch_chat()</code> when
<code>batch_chat_completed()</code> is <code>TRUE</code>.</p></dd>


<dt id="arg-type">type<a class="anchor" aria-label="anchor" href="#arg-type"></a></dt>
<dd><p>A type specification for the extracted data. Should be
created with a <code><a href="type_boolean.html">type_()</a></code> function.</p></dd>


<dt id="arg-convert">convert<a class="anchor" aria-label="anchor" href="#arg-convert"></a></dt>
<dd><p>If <code>TRUE</code>, automatically convert from JSON lists to R
data types using the schema. This typically works best when <code>type</code> is
<code><a href="type_boolean.html">type_object()</a></code> as this will give you a data frame with one column for
each property. If <code>FALSE</code>, returns a list.</p></dd>


<dt id="arg-include-tokens">include_tokens<a class="anchor" aria-label="anchor" href="#arg-include-tokens"></a></dt>
<dd><p>If <code>TRUE</code>, and the result is a data frame, will
add <code>input_tokens</code> and <code>output_tokens</code> columns giving the total input
and output tokens for each prompt.</p></dd>


<dt id="arg-include-cost">include_cost<a class="anchor" aria-label="anchor" href="#arg-include-cost"></a></dt>
<dd><p>If <code>TRUE</code>, and the result is a data frame, will
add <code>cost</code> column giving the cost of each prompt.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>For <code>batch_chat()</code>, a list of <a href="Chat.html">Chat</a> objects, one for each prompt.
For <code>batch_chat_test()</code>, a character vector of text responses.
For <code>batch_chat_structured()</code>, a single structured data object with one
element for each prompt. Typically, when <code>type</code> is an object, this will
will be a data frame with one row for each prompt, and one column for each
property.</p>
<p>For any of the aboves, will return <code>NULL</code> if <code>wait = FALSE</code> and the job
is not complete.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># has_credentials("openai")</span></span></span>
<span class="r-in"><span><span class="va">chat</span> <span class="op">&lt;-</span> <span class="fu"><a href="chat_openai.html">chat_openai</a></span><span class="op">(</span>model <span class="op">=</span> <span class="st">"gpt-4.1-nano"</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Chat ----------------------------------------------------------------------</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="fu"><a href="interpolate.html">interpolate</a></span><span class="op">(</span><span class="st">"What do people from {{state.name}} bring to a potluck dinner?"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="va">chats</span> <span class="op">&lt;-</span> <span class="fu">batch_chat</span><span class="op">(</span><span class="va">chat</span>, <span class="va">prompts</span>, path <span class="op">=</span> <span class="st">"potluck.json"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">chats</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Structured data -----------------------------------------------------------</span></span></span>
<span class="r-in"><span><span class="va">prompts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  <span class="st">"I go by Alex. 42 years on this planet and counting."</span>,</span></span>
<span class="r-in"><span>  <span class="st">"Pleased to meet you! I'm Jamal, age 27."</span>,</span></span>
<span class="r-in"><span>  <span class="st">"They call me Li Wei. Nineteen years young."</span>,</span></span>
<span class="r-in"><span>  <span class="st">"Fatima here. Just celebrated my 35th birthday last week."</span>,</span></span>
<span class="r-in"><span>  <span class="st">"The name's Robert - 51 years old and proud of it."</span>,</span></span>
<span class="r-in"><span>  <span class="st">"Kwame here - just hit the big 5-0 this year."</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">type_person</span> <span class="op">&lt;-</span> <span class="fu"><a href="type_boolean.html">type_object</a></span><span class="op">(</span>name <span class="op">=</span> <span class="fu"><a href="type_boolean.html">type_string</a></span><span class="op">(</span><span class="op">)</span>, age <span class="op">=</span> <span class="fu"><a href="type_boolean.html">type_number</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">batch_chat_structured</span><span class="op">(</span></span></span>
<span class="r-in"><span>  chat <span class="op">=</span> <span class="va">chat</span>,</span></span>
<span class="r-in"><span>  prompts <span class="op">=</span> <span class="va">prompts</span>,</span></span>
<span class="r-in"><span>  path <span class="op">=</span> <span class="st">"people-data.json"</span>,</span></span>
<span class="r-in"><span>  type <span class="op">=</span> <span class="va">type_person</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">data</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
<span class="r-in"><span><span class="op">}</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


   </div>
  <footer><div class="container">
  <div class="pkgdown-footer-left">
  <p>Developed by <a href="https://hadley.nz" class="external-link">Hadley Wickham</a>, Joe Cheng, Aaron Jacobs, <a href="https://garrickadenbuie.com" class="external-link">Garrick Aden-Buie</a>, <a href="http://schloerke.com" class="external-link">Barret Schloerke</a>, <a href="https://www.posit.co" class="external-link"><img src="https://www.tidyverse.org/posit-logo.svg" alt="Posit" height="16" width="62" style="margin-bottom: 3px;"></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

  </div></footer></body></html>

